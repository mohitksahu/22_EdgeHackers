# ğŸª Pluto - Multimodal RAG System

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11+-blue?logo=python" alt="Python">
  <img src="https://img.shields.io/badge/FastAPI-0.104+-green?logo=fastapi" alt="FastAPI">
  <img src="https://img.shields.io/badge/React-18.2+-61DAFB?logo=react" alt="React">
  <img src="https://img.shields.io/badge/LangGraph-0.0.40+-orange" alt="LangGraph">
  <img src="https://img.shields.io/badge/Qdrant-Vector%20DB-red" alt="Qdrant">
  <img src="https://img.shields.io/badge/Version-2.0.0-purple" alt="Version">
</p>

A robust **Multimodal Retrieval-Augmented Generation (RAG)** system designed to ingest, store, retrieve, and generate responses from heterogeneous real-world data sources including text documents (PDF, DOC, TXT), images (scans, diagrams, photographs), and audio recordings.

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Multimodal Ingestion** | Process text, images, and audio with modality-specific pipelines |
| ğŸ§  **Unified Embeddings** | CLIP-based embeddings for cross-modal semantic alignment |
| ğŸ¯ **Intent-Aware Retrieval** | Dynamic modality selection based on query analysis |
| ğŸ“Š **Evidence Grounding** | Responses strictly grounded in retrieved evidence |
| âš¡ **GPU Acceleration** | Optimized for NVIDIA RTX GPUs (tested on RTX 3050 6GB) |
| ğŸ” **Hybrid Search** | BM25 + vector search with MMR reranking |
| ğŸ¤– **LangGraph Orchestration** | Multi-agent workflow for complex reasoning |
| ğŸ›¡ï¸ **Hallucination Prevention** | Refusal logic when evidence is insufficient |
| ğŸ’¬ **Chat History** | Persistent session-based conversation management |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              PLUTO SYSTEM                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Frontend     â”‚         Backend             â”‚       Data Layer        â”‚
â”‚    (React)      â”‚        (FastAPI)            â”‚                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                             â”‚                         â”‚
â”‚  â€¢ Dashboard    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â€¢ File Upload  â”‚  â”‚   LangGraph Agent   â”‚    â”‚   â”‚     Qdrant      â”‚   â”‚
â”‚  â€¢ Chat UI      â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚   â”‚   Vector DB     â”‚   â”‚
â”‚  â€¢ Sessions     â”‚  â”‚   â”‚Query Analysis â”‚ â”‚â—„â”€â”€â”€â”¼â”€â”€â–ºâ”‚                 â”‚   â”‚
â”‚                 â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚   â”‚  â€¢ Embeddings   â”‚   â”‚
â”‚  React 18       â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚   â”‚  â€¢ Metadata     â”‚   â”‚
â”‚  Tailwind CSS   â”‚  â”‚   â”‚  Retrieval    â”‚ â”‚    â”‚   â”‚  â€¢ Documents    â”‚   â”‚
â”‚  Framer Motion  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  Three.js       â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚                         â”‚
â”‚                 â”‚  â”‚   â”‚  Generation   â”‚ â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                 â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚   â”‚     Ollama      â”‚   â”‚
â”‚                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚   (LLM Server)  â”‚   â”‚
â”‚                 â”‚                             â”‚   â”‚                 â”‚   â”‚
â”‚                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚  â€¢ Llama 3.2    â”‚   â”‚
â”‚                 â”‚  â”‚   Ingestion Engine  â”‚    â”‚   â”‚  â€¢ Mistral      â”‚   â”‚
â”‚                 â”‚  â”‚   â€¢ PDF Processor   â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚  â”‚   â€¢ Image OCR       â”‚    â”‚                         â”‚
â”‚                 â”‚  â”‚   â€¢ Audio Whisper   â”‚    â”‚                         â”‚
â”‚                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+**
- **Node.js 18+**
- **NVIDIA GPU** with CUDA 12.1+ (optional but recommended)
- **Ollama** for LLM inference
- **Docker & Docker Compose** (for containerized deployment)

### Option 1: Docker Deployment (Recommended)

```bash
# Clone the repository
git clone https://github.com/droit8/Pluto.git
cd Pluto

# Start all services
docker-compose up -d

# Access the application
# Frontend: http://localhost:80
# Backend API: http://localhost:8000
# Qdrant Dashboard: http://localhost:6333/dashboard
```

### Option 2: Manual Setup

#### 1. Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Start Qdrant (required)
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant

# Start the backend server
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

#### 2. Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev

# Access at http://localhost:5173
```

#### 3. Ollama Setup

```bash
# Install Ollama (https://ollama.ai)
# Then pull the model
ollama pull llama3.2:1b

# Verify it's running
ollama list
```

---

## ğŸ“ Project Structure

```
Pluto/
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/v1/            # REST API endpoints
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/     # ingest, query, session, vector
â”‚   â”‚   â”œâ”€â”€ graph/             # LangGraph workflow
â”‚   â”‚   â”‚   â”œâ”€â”€ nodes/         # Graph nodes (retrieval, generation, etc.)
â”‚   â”‚   â”‚   â””â”€â”€ agents/        # Multi-agent architecture
â”‚   â”‚   â”œâ”€â”€ ingestion/         # Document processing
â”‚   â”‚   â”‚   â””â”€â”€ processors/    # PDF, image, audio processors
â”‚   â”‚   â”œâ”€â”€ retrieval/         # Search & retrieval
â”‚   â”‚   â”‚   â””â”€â”€ retrievers/    # BM25, hybrid, MMR
â”‚   â”‚   â”œâ”€â”€ reasoning/         # LLM reasoning
â”‚   â”‚   â”‚   â””â”€â”€ llm/           # Ollama integration
â”‚   â”‚   â”œâ”€â”€ embeddings/        # CLIP embeddings
â”‚   â”‚   â””â”€â”€ storage/           # Vector store (Qdrant)
â”‚   â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/                   # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”œâ”€â”€ pages/             # Page components
â”‚   â”‚   â”œâ”€â”€ services/          # API clients
â”‚   â”‚   â””â”€â”€ styles/            # Tailwind styles
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ backend.yaml
â”‚   â”œâ”€â”€ frontend.yaml
â”‚   â””â”€â”€ models.yaml
â”‚
â”œâ”€â”€ data/                       # Data directories
â”‚   â”œâ”€â”€ uploads/               # Uploaded files
â”‚   â”œâ”€â”€ vectorstore/           # Qdrant persistence
â”‚   â””â”€â”€ models/                # ML models
â”‚
â””â”€â”€ docker-compose.yml         # Docker orchestration
```

---

## ğŸ”Œ API Endpoints

### Ingestion

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/ingest/upload` | Upload and process files |
| `GET` | `/api/v1/ingest/status/{job_id}` | Check ingestion status |
| `DELETE` | `/api/v1/ingest/{doc_id}` | Delete a document |

### Query

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/query` | Submit a query |
| `POST` | `/api/v1/query/stream` | Stream query response |
| `GET` | `/api/v1/query/history` | Get query history |

### Session

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/session/create` | Create a new session |
| `GET` | `/api/v1/session/{id}` | Get session details |
| `DELETE` | `/api/v1/session/{id}` | Delete a session |

### Vector Store

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/v1/vector/info` | Get collection info |
| `GET` | `/api/v1/vector/search` | Direct vector search |
| `DELETE` | `/api/v1/vector/reset` | Reset vector store |

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `QDRANT_HOST` | `localhost` | Qdrant server host |
| `QDRANT_PORT` | `6333` | Qdrant server port |
| `OLLAMA_HOST` | `http://localhost:11434` | Ollama server URL |
| `OLLAMA_MODEL` | `llama3.2:1b` | LLM model to use |
| `LOG_LEVEL` | `INFO` | Logging verbosity |
| `DEBUG` | `true` | Debug mode |

### GPU / VRAM Configuration

The system is optimized for RTX 3050 6GB but scales to other GPUs:

```
VRAM Allocation (6GB):
â”œâ”€â”€ Ollama LLM (1B): ~1.5GB
â”œâ”€â”€ CLIP Embeddings: ~0.8GB
â”œâ”€â”€ Whisper (base):  ~0.5GB
â”œâ”€â”€ Qdrant/Other:    ~0.5GB
â””â”€â”€ Buffer:          ~0.5GB
    Total: ~3.8GB
```

---

## ğŸ”„ LangGraph Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Input  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Query Analysisâ”‚ â—„â”€â”€â”€ Intent detection, modality routing
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compat. Gate â”‚ â—„â”€â”€â”€ Topic-concept compatibility check
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Retrieval   â”‚ â—„â”€â”€â”€ Hybrid search (BM25 + Vector + MMR)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Evidence Gradeâ”‚ â—„â”€â”€â”€ GPU-accelerated relevance scoring
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conflict    â”‚ â—„â”€â”€â”€ Detect contradictory evidence
â”‚  Detection   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generation  â”‚ OR  â”‚   Refusal    â”‚ â—„â”€â”€â”€ If evidence insufficient
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š ML Models

| Component | Model | Purpose |
|-----------|-------|---------|
| **Embeddings** | CLIP ViT-B/32 | Multimodal embeddings |
| **Transcription** | Whisper Base | Audio-to-text |
| **LLM** | Llama 3.2 1B (via Ollama) | Response generation |
| **Reranking** | Cross-Encoder | Result reranking |
| **BM25** | rank-bm25 | Sparse retrieval |

---

## ğŸ³ Docker Services

| Service | Port | Description |
|---------|------|-------------|
| `pluto-frontend` | 80 | React frontend (Nginx) |
| `pluto-backend` | 8000 | FastAPI backend |
| `pluto-qdrant` | 6333, 6334 | Qdrant vector database |

---

## ğŸ§ª Running Tests

```bash
cd backend

# Test ingestion pipeline
python scripts/test_ingestion.py

# Test chat history
python scripts/test_chat_history.py

# Test Ollama connection
python scripts/test_ollama.py

# Benchmark performance
python scripts/benchmark_performance.py

# Check Ollama GPU usage
python scripts/check_ollama_gpu.py
```

---

## ğŸ“ˆ Performance

### GPU Acceleration Benefits

| Component | CPU | GPU | Speedup |
|-----------|-----|-----|---------|
| CLIP Embeddings | ~500ms | ~50ms | **10x** |
| Whisper Transcription | ~30s/min | ~6s/min | **5x** |
| Vector Search (Qdrant) | ~10ms | ~10ms | N/A |

### Recommended Hardware

- **Minimum**: 8GB RAM, 4-core CPU
- **Recommended**: 16GB RAM, RTX 3050+ (6GB VRAM)
- **Optimal**: 32GB RAM, RTX 3080+ (10GB+ VRAM)

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License.

---

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

<p align="center">
  Built with â¤ï¸ using FastAPI, React, LangGraph, and Qdrant
</p>