DIRECTORY TREE
ðŸ“ backend/
â”‚ ðŸ“ app/
â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ ðŸ“„ config.py
â”‚ â”‚ ðŸ“„ main.py
â”‚ â”‚ ðŸ“ api/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“ middleware/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ error_handler.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ logging.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ metrics.py
â”‚ â”‚ â”‚ ðŸ“ v1/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ router.py
â”‚ â”‚ â”‚ â”‚ ðŸ“ endpoints/
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ evidence.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ health.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ ingest.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ query.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ retrieval.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ session.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ vector.py
â”‚ â”‚ â”‚ â”‚ ðŸ“ schemas/
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ evidence.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ ingest.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ query.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ response.py
â”‚ â”‚ â”‚ â”‚ â”‚ ðŸ“„ retrieval.py
â”‚ â”‚ ðŸ“ core/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“„ constants.py
â”‚ â”‚ â”‚ ðŸ“„ exceptions.py
â”‚ â”‚ â”‚ ðŸ“„ logging_config.py
â”‚ â”‚ â”‚ ðŸ“„ settings.py
â”‚ â”‚ ðŸ“ embeddings/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“„ manager.py
â”‚ â”‚ â”‚ ðŸ“ models/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ clip_embedder.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ multimodal_embedder.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ text_embedder.py
â”‚ â”‚ ðŸ“ graph/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“„ graph_builder.py
â”‚ â”‚ â”‚ ðŸ“„ state.py
â”‚ â”‚ â”‚ ðŸ“ edges/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ conditional_edges.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ routing_logic.py
â”‚ â”‚ â”‚ ðŸ“ nodes/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ conflict_detector_node.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ evidence_evaluation_node.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ evidence_grader_node.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ gate_node.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ generation_node.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ notification_node.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ query_analysis_node.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ refusal_node.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ retrieval_node.py
â”‚ â”‚ ðŸ“ ingestion/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“„ ingestion_service.py
â”‚ â”‚ â”‚ ðŸ“„ orchestrator.py
â”‚ â”‚ â”‚ ðŸ“ chunking/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ semantic_chunker.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ text_chunker.py
â”‚ â”‚ â”‚ ðŸ“ metadata/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ extractor.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ validator.py
â”‚ â”‚ â”‚ ðŸ“ processors/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ audio_processor.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ base.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ image_processor.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ ocr_processor.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ pdf_processor.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ text_processor.py
â”‚ â”‚ â”‚ ðŸ“ validators/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ file_validator.py
â”‚ â”‚ ðŸ“ integrations/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“ langsmith/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ tracer.py
â”‚ â”‚ â”‚ ðŸ“ whisper/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ audio_preprocessor.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ transcriber.py
â”‚ â”‚ ðŸ“ reasoning/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“ conflict/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ detector.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ presenter.py
â”‚ â”‚ â”‚ ðŸ“ evidence/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ citation_generator.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ confidence_scorer.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ evidence_evaluator.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ intent_detector.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ strength_assessor.py
â”‚ â”‚ â”‚ ðŸ“ hallucination/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ detector.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ refusal_engine.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ suppressor.py
â”‚ â”‚ â”‚ ðŸ“ llm/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ llama_reasoner.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ output_parser.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ prompt_builder.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ prompt_templates.py
â”‚ â”‚ â”‚ ðŸ“ uncertainty/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ communicator.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ quantifier.py
â”‚ â”‚ ðŸ“ retrieval/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“„ orchestrator.py
â”‚ â”‚ â”‚ ðŸ“ alignment/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ cross_modal_aligner.py
â”‚ â”‚ â”‚ ðŸ“ query/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ analyzer.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ expander.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ multi_query_generator.py
â”‚ â”‚ â”‚ ðŸ“ reranking/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ relevance_scorer.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ reranker.py
â”‚ â”‚ â”‚ ðŸ“ strategies/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ base_strategy.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ cross_modal_strategy.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ intent_aware_strategy.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ multimodal_strategy.py
â”‚ â”‚ ðŸ“ services/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“„ ingestion_service.py
â”‚ â”‚ â”‚ ðŸ“„ reasoning_service.py
â”‚ â”‚ â”‚ ðŸ“„ retrieval_service.py
â”‚ â”‚ ðŸ“ storage/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“„ vector_store.py
â”‚ â”‚ â”‚ ðŸ“ chat_store/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ history_manager.py
â”‚ â”‚ â”‚ ðŸ“ file_store/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ file_manager.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ local_storage.py
â”‚ â”‚ â”‚ ðŸ“ metadata_store/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“ qdrant/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ client.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ collections.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ indexing.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ query_builder.py
â”‚ â”‚ ðŸ“ tests/
â”‚ â”‚ ðŸ“ utils/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“„ async_utils.py
â”‚ â”‚ â”‚ ðŸ“„ file_utils.py
â”‚ â”‚ â”‚ ðŸ“„ gpu_check.py
â”‚ â”‚ â”‚ ðŸ“„ image_utils.py
â”‚ â”‚ â”‚ ðŸ“„ logging_utils.py
â”‚ â”‚ â”‚ ðŸ“„ retry_utils.py
â”‚ â”‚ â”‚ ðŸ“„ text_utils.py
â”‚ â”‚ â”‚ ðŸ“„ topic_catalog_logger.py
â”‚ â”‚ â”‚ ðŸ“„ topic_utils.py
â”‚ â”‚ â”‚ ðŸ“„ validation_utils.py
â”‚ ðŸ“ scripts/
â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ ðŸ“„ acid_test.py
â”‚ â”‚ ðŸ“„ benchmark.py
â”‚ â”‚ ðŸ“„ benchmark_performance.py
â”‚ â”‚ ðŸ“„ cleanup_vectorstore.py
â”‚ â”‚ ðŸ“„ convert_clip_to_gguf.py
â”‚ â”‚ ðŸ“„ download_llama.py
â”‚ â”‚ ðŸ“„ initialize_vectorstore.py
â”‚ â”‚ ðŸ“„ reset_vectorstore.py
â”‚ â”‚ ðŸ“„ setup_models.py
â”‚ â”‚ ðŸ“„ struct_extract.py
â”‚ â”‚ ðŸ“„ test_chat_history.py
â”‚ â”‚ ðŸ“„ test_ingestion.py
â”‚ â”‚ ðŸ“„ verify_no_autowipe.py
â”‚ â”‚ ðŸ“ tests/
â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ ðŸ“„ test_backend_modelLoading.py
â”‚ â”‚ â”‚ ðŸ“ integration/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_api.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_contradictory_sources.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_end_to_end.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_ingestion_pipeline.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_missing_data.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_partial_failures.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_retrieval_pipeline.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_uncertainty_handling.py
â”‚ â”‚ â”‚ ðŸ“ unit/
â”‚ â”‚ â”‚ â”‚ ðŸ“„ __init__.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_chunking.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_embeddings.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_graph.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_processors.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_reasoning.py
â”‚ â”‚ â”‚ â”‚ ðŸ“„ test_retrieval.py
FILE CONTENTS
FILE: app/__init__.py
[EMPTY FILE]
FILE: app/api/__init__.py
[EMPTY FILE]
FILE: app/api/middleware/__init__.py
[EMPTY FILE]
FILE: app/api/middleware/error_handler.py
[EMPTY FILE]
FILE: app/api/middleware/logging.py
[EMPTY FILE]
FILE: app/api/middleware/metrics.py
"""
Prometheus metrics middleware
"""
from prometheus_client import Counter, Histogram, Gauge
import time

REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
REQUEST_LATENCY = Histogram('http_request_duration_seconds', 'HTTP request latency', ['method', 'endpoint'])
ACTIVE_SESSIONS = Gauge('active_sessions_total', 'Number of active sessions')
VECTOR_STORE_SIZE = Gauge('vector_store_documents_total', 'Total documents in vector store')
EMBEDDING_CACHE_HITS = Counter('embedding_cache_hits_total', 'Embedding cache hits')
EMBEDDING_CACHE_MISSES = Counter('embedding_cache_misses_total', 'Embedding cache misses')

async def metrics_middleware(request, call_next):
    """Record request metrics"""
    start_time = time.time()

    response = await call_next(request)

    duration = time.time() - start_time

    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()

    REQUEST_LATENCY.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)

    return response
FILE: app/api/v1/__init__.py
[EMPTY FILE]
FILE: app/api/v1/endpoints/__init__.py
[EMPTY FILE]
FILE: app/api/v1/endpoints/evidence.py
[EMPTY FILE]
FILE: app/api/v1/endpoints/health.py
[EMPTY FILE]
FILE: app/api/v1/endpoints/ingest.py
"""
Ingestion API endpoints
"""
import logging
from pathlib import Path
from typing import List, Dict, Any
from fastapi import APIRouter, UploadFile, File, Form, HTTPException, BackgroundTasks, Header
from fastapi.responses import JSONResponse

from app.ingestion.orchestrator import IngestionOrchestrator
from app.ingestion.validators.file_validator import FileValidator
from app.ingestion.metadata.extractor import MetadataExtractor
from app.config import settings

logger = logging.getLogger(__name__)

router = APIRouter()
ingestion_orchestrator = IngestionOrchestrator()
metadata_extractor = MetadataExtractor()

@router.post("/file", response_model=Dict[str, Any])
async def ingest_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    session_id: str = Header("default", alias="X-Session-ID"),
    chunking_strategy: str = Form("character", description="Chunking strategy: character, sentence, or page"),
    chunk_size: int = Form(235, description="Size of each chunk (default 235 for CLIP limit)"),
    chunk_overlap: int = Form(30, description="Overlap between chunks (default 30 chars)")
) -> Dict[str, Any]:
    """
    Ingest a single file for processing

    - **file**: File to ingest (PDF, image, audio, or text)
    - **chunking_strategy**: How to split the content into chunks
    - **chunk_size**: Maximum size of each chunk (default 235 for CLIP)
    - **chunk_overlap**: Overlap between consecutive chunks (default 30)
    """
    try:
        logger.info(f"Ingesting file: {file.filename}")

        temp_path = settings.uploads_dir / file.filename
        settings.uploads_dir.mkdir(parents=True, exist_ok=True)

        with open(temp_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        validator = FileValidator()
        if not validator.validate(temp_path):
            temp_path.unlink(missing_ok=True)
            raise HTTPException(
                status_code=400,
                detail=f"File validation failed: {validator.errors}"
            )

        result = ingestion_orchestrator.ingest_and_store(
            temp_path,
            session_id=session_id,
            chunking_strategy=chunking_strategy,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )

        result['filename'] = file.filename

        file_metadata = metadata_extractor.extract_file_metadata(temp_path)
        if isinstance(result, list):
            processing_metadata = metadata_extractor.extract_processing_metadata({"data": result})
        else:
            processing_metadata = metadata_extractor.extract_processing_metadata(result)
        combined_metadata = metadata_extractor.combine_metadata(file_metadata, processing_metadata)

        result['metadata'] = combined_metadata

        background_tasks.add_task(temp_path.unlink, missing_ok=True)

        if result.get('status') == 'success':
            logger.info(f"Successfully ingested {file.filename}: {result.get('stored_chunks', 0)} chunks stored in vector DB")

            try:
                from app.storage.qdrant.client import QdrantClientWrapper
                from app.utils.topic_catalog_logger import log_topic_catalog

                qdrant_client = QdrantClientWrapper()
                current_topics = []
                log_topic_catalog(current_topics)
                logger.info(f"Topic catalog updated: {len(current_topics)} topics")
            except Exception as e:
                logger.warning(f"Failed to update topic catalog: {e}")

            return result
        else:
            raise HTTPException(
                status_code=500,
                detail=f"Processing failed: {result.get('error', 'Unknown error')}"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to ingest file {file.filename}: {e}")
        raise HTTPException(status_code=500, detail=f"Ingestion failed: {str(e)}")

@router.post("/batch", response_model=List[Dict[str, Any]])
async def ingest_batch(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    chunking_strategy: str = Form("character", description="Chunking strategy: character, sentence, or page"),
    chunk_size: int = Form(235, description="Size of each chunk (default 235 for CLIP limit)"),
    chunk_overlap: int = Form(30, description="Overlap between chunks (default 30 chars)")
) -> List[Dict[str, Any]]:
    """
    Ingest multiple files for batch processing

    - **files**: List of files to ingest
    - **chunking_strategy**: How to split the content into chunks
    - **chunk_size**: Maximum size of each chunk
    - **chunk_overlap**: Overlap between consecutive chunks
    """
    try:
        logger.info(f"Ingesting batch of {len(files)} files")

        if len(files) == 0:
            raise HTTPException(status_code=400, detail="No files provided")

        batch_validator = BatchValidator()
        if not batch_validator.validate([]):
            raise HTTPException(
                status_code=400,
                detail=f"Batch validation failed: {batch_validator.errors}"
            )

        temp_paths = []
        for file in files:
            temp_path = settings.uploads_dir / file.filename
            settings.uploads_dir.mkdir(parents=True, exist_ok=True)

            with open(temp_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)

            temp_paths.append(temp_path)

        file_validator = FileValidator()
        valid_paths = []
        invalid_files = []

        for temp_path in temp_paths:
            if file_validator.validate(temp_path):
                valid_paths.append(temp_path)
            else:
                invalid_files.append({
                    'file_name': temp_path.name,
                    'errors': file_validator.errors
                })

        if not valid_paths:
            for temp_path in temp_paths:
                temp_path.unlink(missing_ok=True)
            raise HTTPException(
                status_code=400,
                detail=f"No valid files to process. Invalid files: {invalid_files}"
            )

        results = ingestion_orchestrator.ingest_batch(
            valid_paths,
            chunking_strategy=chunking_strategy,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )

        for result in results:
            if result.get('status') == 'success':
                file_path = Path(result['file_path'])
                result['filename'] = file_path.name

                file_metadata = metadata_extractor.extract_file_metadata(file_path)
                if isinstance(result, list):
                    processing_metadata = metadata_extractor.extract_processing_metadata({"data": result})
                else:
                    processing_metadata = metadata_extractor.extract_processing_metadata(result)
                combined_metadata = metadata_extractor.combine_metadata(file_metadata, processing_metadata)
                result['metadata'] = combined_metadata

        for temp_path in temp_paths:
            background_tasks.add_task(temp_path.unlink, missing_ok=True)

        successful = sum(1 for r in results if r.get('status') == 'success')
        logger.info(f"Batch ingestion completed: {successful}/{len(results)} files successful")

        return results

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to ingest batch: {e}")
        raise HTTPException(status_code=500, detail=f"Batch ingestion failed: {str(e)}")

@router.post("/validate", response_model=Dict[str, Any])
async def validate_files(
    files: List[UploadFile] = File(...)
) -> Dict[str, Any]:
    """
    Validate files without processing them

    - **files**: List of files to validate
    """
    try:
        logger.info(f"Validating {len(files)} files")

        temp_paths = []
        for file in files:
            temp_path = settings.uploads_dir / f"validate_{file.filename}"
            settings.uploads_dir.mkdir(parents=True, exist_ok=True)

            with open(temp_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)

            temp_paths.append(temp_path)

        validation_results = ingestion_service.validate_files(temp_paths)

        summaries = []
        for temp_path in temp_paths:
            summary = ingestion_service.get_file_summary(temp_path)
            summaries.append(summary)

        for temp_path in temp_paths:
            temp_path.unlink(missing_ok=True)

        result = {
            'validation': validation_results,
            'summaries': summaries,
            'total_files': len(files)
        }

        logger.info(f"Validation completed: {validation_results['total_valid']}/{len(files)} files valid")
        return result

    except Exception as e:
        logger.error(f"Failed to validate files: {e}")
        raise HTTPException(status_code=500, detail=f"Validation failed: {str(e)}")

@router.get("/supported-formats", response_model=Dict[str, List[str]])
async def get_supported_formats() -> Dict[str, List[str]]:
    """
    Get list of supported file formats for ingestion
    """
    return {
        "supported_formats": [
            "pdf", "txt", "md", "csv", "json", "xml", "html",
            "jpg", "jpeg", "png", "bmp", "tiff", "tif",
            "mp3", "wav", "flac", "m4a", "aac", "ogg"
        ],
        "chunking_strategies": ["character", "sentence", "page"]
    }
FILE: app/api/v1/endpoints/query.py
"""
Query API endpoints
"""
import logging
from typing import Dict, Any, List, Optional
from fastapi import APIRouter, HTTPException, Header
from pydantic import BaseModel
from fastapi.responses import StreamingResponse
import json

from app.graph.graph_builder import GraphBuilder
from app.graph.state import GraphState
from app.storage.chat_store import ChatHistoryManager

logger = logging.getLogger(__name__)

router = APIRouter()

graph_builder = GraphBuilder()
graph = graph_builder.build_graph()

chat_history = ChatHistoryManager()

class QueryRequest(BaseModel):
    """Query request model"""
    query: str
    user_id: str = "default_user"

class CitedSource(BaseModel):
    """Individual cited source with multimodal metadata"""
    modality: str = "text"
    content: str = ""
    score: float = 0.0
    filename: str = "Unknown"
    page: Optional[int] = None
    timestamp: Optional[str] = None

class QueryResponse(BaseModel):
    """Query response model - Frontend-compatible structure"""
    query: str
    response: str
    refusal: Optional[str] = None
    confidence_score: float
    cited_sources: List[CitedSource]
    sources: List[str] = []
    conflicts: List[str] = []
    conflicts_detected: bool = False
    processing_time: float = 0.0
    status: str = "success"
    assumptions: List[str] = []
    evidence_grade: float = 0.0
    confidence_per_source: Dict[str, float] = {}

@router.post("/", response_model=QueryResponse)
async def query_documents(
    request: QueryRequest,
    session_id: str = Header("default", alias="X-Session-ID")
) -> QueryResponse:
    """
    Query the multimodal RAG system

    - **query**: The question to ask
    - **user_id**: Optional user identifier
    """
    try:
        logger.info(f"Processing query: {request.query[:50]}... [session={session_id}]")

        conversation_history = await chat_history.get_history(session_id, max_turns=5)
        logger.info(f"Loaded {len(conversation_history)} previous turns for session {session_id}")

        initial_state = {
            "query": request.query,
            "session_id": session_id,
            "conversation_history": conversation_history
        }

        final_state = await graph.ainvoke(initial_state)

        refusal_message = None
        response_status = "success"

        if final_state.get("status") == "refused" or not final_state.get("is_allowed", True):
            refusal_message = final_state.get("final_response", "Unable to answer this query.")
            response_status = "refused"

            return QueryResponse(
                query=request.query,
                response="",
                refusal=refusal_message,
                confidence_score=0.0,
                cited_sources=[],
                sources=[],
                conflicts=[],
                conflicts_detected=False,
                processing_time=final_state.get("processing_time", 0.0),
                status=response_status,
                assumptions=[],
                evidence_grade=0.0,
                confidence_per_source={}
            )

        plain_text_answer = final_state.get("final_response") or "I don't have enough information to answer this question."
        retrieved_docs = final_state.get("retrieved_documents", [])

        cited_sources = []
        source_scores = {}

        for doc in retrieved_docs:
            metadata = doc.get('metadata', {})

            source_file = metadata.get('source_file') or metadata.get('file_name', 'Unknown')
            if isinstance(source_file, str):
                source_file = source_file.replace('\\', '/').replace('\\', '/')
                if '/' in source_file:
                    source_file = source_file.split('/')[-1]
            else:
                source_file = 'Unknown'

            modality = metadata.get('modality', 'text')

            score = doc.get('score', 0.0)
            if score > 1.0:
                score = score / 100.0

            if source_file not in source_scores:
                source_scores[source_file] = []
            source_scores[source_file].append(score)

            cited_sources.append(CitedSource(
                modality=modality,
                content=doc.get('content', '')[:500],
                score=score,
                filename=source_file,
                page=metadata.get('page_number'),
                timestamp=metadata.get('timestamp')
            ))

        confidence_per_source = {
            source: sum(scores) / len(scores)
            for source, scores in source_scores.items()
        }

        evidence_scores = final_state.get("evidence_scores", [])
        global_confidence = sum(evidence_scores) / len(evidence_scores) if evidence_scores else 0.0

        unique_sources = list(source_scores.keys())

        conflicts = final_state.get("conflicts", [])
        is_conflicting = final_state.get("is_conflicting", False)
        assumptions = final_state.get("assumptions", [])
        processing_time = final_state.get("processing_time", 0.0)

        logger.info(f"Query processed, confidence: {global_confidence:.3f}, unique sources: {len(unique_sources)}")
        logger.info(f"Evidence grade: {global_confidence:.3f}, Conflicts: {len(conflicts)}, Is conflicting: {is_conflicting}")
        logger.info(f"Per-source confidence: {confidence_per_source}")

        await chat_history.save_turn(
            session_id=session_id,
            user_query=request.query,
            system_response=plain_text_answer,
            cited_sources=unique_sources,
            confidence_score=global_confidence,
            is_conflicting=is_conflicting,
            conflicts=conflicts
        )
        logger.info(f"Saved conversation turn for session {session_id}")

        return QueryResponse(
            query=request.query,
            response=plain_text_answer,
            refusal=None,
            confidence_score=global_confidence,
            cited_sources=cited_sources,
            sources=unique_sources,
            conflicts=conflicts,
            conflicts_detected=is_conflicting,
            processing_time=processing_time,
            status=response_status,
            assumptions=assumptions,
            evidence_grade=global_confidence,
            confidence_per_source=confidence_per_source
        )

    except Exception as e:
        logger.error(f"Query failed: {e}")
        raise HTTPException(status_code=500, detail=f"Query processing failed: {str(e)}")

@router.get("/health")
async def query_health():
    """Health check for query endpoint"""
    return {"status": "healthy", "service": "query"}

@router.post("/stream")
async def query_stream(
    request: QueryRequest,
    session_id: str = Header("default", alias="X-Session-ID")
):
    """Stream response chunks for better UX with long queries"""

    async def generate():
        conversation_history = await chat_history.get_history(session_id, max_turns=5)

        initial_state = {
            "query": request.query,
            "session_id": session_id,
            "conversation_history": conversation_history
        }

        yield json.dumps({"event": "started", "message": "Processing query..."}) + "\n"

        final_state = await graph.ainvoke(initial_state)

        retrieved_docs = final_state.get("retrieved_documents", [])
        yield json.dumps({
            "event": "retrieved",
            "count": len(retrieved_docs),
            "documents": [{"id": d['id'], "score": d['score']} for d in retrieved_docs]
        }) + "\n"

        response = final_state.get("final_response", "")
        yield json.dumps({
            "event": "complete",
            "response": response,
            "confidence": final_state.get("evidence_score", 0.0)
        }) + "\n"

    return StreamingResponse(generate(), media_type="application/x-ndjson")
FILE: app/api/v1/endpoints/retrieval.py
[EMPTY FILE]
FILE: app/api/v1/endpoints/session.py
"""
Session management endpoints
"""
import logging
from fastapi import APIRouter, HTTPException, Header
from pydantic import BaseModel

from app.storage.vector_store import VectorStore
from app.storage.chat_store import ChatHistoryManager

logger = logging.getLogger(__name__)

router = APIRouter()
vector_store = VectorStore()
chat_history = ChatHistoryManager()

class SessionClearResponse(BaseModel):
    """Response for session clear operation"""
    status: str
    session_id: str
    message: str

@router.delete("/clear", response_model=SessionClearResponse)
async def clear_session(session_id: str = Header(..., alias="X-Session-ID")):
    """
    Clear all documents and chat history for the current session

    - **session_id**: Session to clear (from header)
    """
    try:
        logger.info(f"Clearing session: {session_id}")

        deleted_count = vector_store.delete_by_session(session_id)

        history_cleared = await chat_history.clear_session(session_id)

        return SessionClearResponse(
            status="success",
            session_id=session_id,
            message=f"Session cleared: {deleted_count} documents deleted, chat history cleared"
        )

    except Exception as e:
        logger.error(f"Failed to clear session: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to clear session: {str(e)}"
        )

@router.get("/info")
async def get_session_info(session_id: str = Header(..., alias="X-Session-ID")):
    """
    Get information about the current session including chat history

    - **session_id**: Session to query (from header)
    """
    try:
        catalog = vector_store.get_knowledge_catalog()

        history_info = await chat_history.get_session_info(session_id)

        return {
            "session_id": session_id,
            "document_count": len(catalog.get("topics", [])),
            "topics": catalog.get("topics", []),
            "status": "active",
            "chat_history": {
                "turn_count": history_info.get("turn_count", 0),
                "first_query": history_info.get("first_query"),
                "last_query": history_info.get("last_query"),
                "last_timestamp": history_info.get("last_timestamp")
            }
        }

    except Exception as e:
        logger.error(f"Failed to get session info: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get session info: {str(e)}"
        )
FILE: app/api/v1/endpoints/vector.py
"""
Vector management endpoints
"""
import logging
from typing import Dict, Any, List
from pathlib import Path

from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import JSONResponse

from app.storage.vector_store import VectorStore
from app.config import settings

logger = logging.getLogger(__name__)

router = APIRouter()

@router.delete("/reset")
async def reset_vector_store() -> Dict[str, Any]:
    """
    Reset the vector store by deleting all embeddings and data.

    This is a destructive operation that cannot be undone.
    Use with caution in development/testing environments.
    """
    try:
        vector_store = VectorStore()
        vector_store.reset()

        logger.info("Vector store reset completed successfully")
        return {
            "status": "success",
            "message": "Vector store has been reset. All embeddings and data have been deleted."
        }

    except Exception as e:
        logger.error(f"Failed to reset vector store: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to reset vector store: {str(e)}"
        )

@router.get("/stats")
async def get_vector_store_stats() -> Dict[str, Any]:
    """
    Get statistics about the vector store including counts by modality.

    Returns:
        Statistics showing total documents and breakdown by modality (text, image, audio)
    """
    try:
        vector_store = VectorStore()

        basic_stats = vector_store.get_stats()

        modality_counts = {
            "text": 0,
            "image": 0,
            "audio": 0
        }

        stats = {
            "total_documents": basic_stats.get('total_documents', 0),
            "modalities": modality_counts,
            "collection": basic_stats.get('collection_name', 'unknown')
        }

        logger.info(f"Vector store stats: {stats['total_documents']} total docs, {modality_counts}")
        return stats

    except Exception as e:
        logger.error(f"Failed to get vector store stats: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get vector store stats: {str(e)}"
        )

@router.post("/cleanup-orphans")
async def cleanup_orphaned_documents() -> Dict[str, Any]:
    """
    Clean up orphaned documents whose source files no longer exist on disk.

    This is useful when files were manually deleted but their vectors remain in the vector store.

    Returns:
        Number of orphaned documents deleted and list of cleaned files
    """
    try:
        vector_store = VectorStore()

        logger.warning("Orphaned document cleanup not yet implemented for Qdrant")

        return {
            "status": "not_implemented",
            "message": "Orphaned document cleanup is not yet implemented for Qdrant",
            "deleted_count": 0,
            "deleted_sources": []
        }

    except Exception as e:
        logger.error(f"Failed to cleanup orphaned documents: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to cleanup orphaned documents: {str(e)}"
        )

@router.delete("/source/{source_file:path}")
async def delete_documents_by_source(source_file: str) -> Dict[str, Any]:
    """
    Delete all documents from a specific source file.

    Args:
        source_file: Path to the source file (can be relative or absolute)

    Returns:
        Number of documents deleted
    """
    try:
        vector_store = VectorStore()
        deleted_count = vector_store.delete_by_source(source_file)

        logger.info(f"Deleted {deleted_count} documents from source: {source_file}")
        return {
            "status": "success",
            "message": f"Deleted {deleted_count} documents from {source_file}",
            "deleted_count": deleted_count,
            "source_file": source_file
        }

    except Exception as e:
        logger.error(f"Failed to delete documents by source: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete documents: {str(e)}"
        )
FILE: app/api/v1/router.py
"""
API Router for version 1 endpoints
"""
from fastapi import APIRouter

from app.api.v1.endpoints import ingest, query, vector, session

api_router = APIRouter()

api_router.include_router(
    ingest.router,
    prefix="/ingest",
    tags=["ingestion"]
)

api_router.include_router(
    query.router,
    prefix="/query",
    tags=["query"]
)

api_router.include_router(
    vector.router,
    prefix="/vector",
    tags=["vector"]
)

api_router.include_router(
    session.router,
    prefix="/session",
    tags=["session"]
)
FILE: app/api/v1/schemas/__init__.py
[EMPTY FILE]
FILE: app/api/v1/schemas/evidence.py
[EMPTY FILE]
FILE: app/api/v1/schemas/ingest.py
"""
Pydantic schemas for ingestion endpoints
"""
from enum import Enum
from typing import List, Optional

from pydantic import BaseModel, Field

class Modality(str, Enum):
    """Supported modalities for ingestion"""
    TEXT = "text"
    IMAGE = "image"
    AUDIO = "audio"

class IngestRequest(BaseModel):
    """Request schema for document ingestion"""
    modality: Modality = Field(..., description="The modality of the content")
    title: Optional[str] = Field(None, description="Optional title for the document")
    description: Optional[str] = Field(None, description="Optional description")
    tags: Optional[List[str]] = Field(default_factory=list, description="Optional tags")
    metadata: Optional[dict] = Field(default_factory=dict, description="Additional metadata")

class IngestResponse(BaseModel):
    """Response schema for successful ingestion"""
    document_id: str = Field(..., description="Unique identifier for the ingested document")
    modality: Modality = Field(..., description="The modality that was processed")
    chunks_created: int = Field(..., description="Number of chunks created")
    embeddings_generated: int = Field(..., description="Number of embeddings generated")
    status: str = Field("success", description="Ingestion status")

class BatchIngestRequest(BaseModel):
    """Request schema for batch ingestion"""
    documents: List[IngestRequest] = Field(..., description="List of documents to ingest")

class BatchIngestResponse(BaseModel):
    """Response schema for batch ingestion"""
    total_documents: int = Field(..., description="Total number of documents processed")
    successful_ingests: int = Field(..., description="Number of successful ingestions")
    failed_ingests: int = Field(..., description="Number of failed ingestions")
    results: List[IngestResponse] = Field(..., description="Detailed results for each document")

class IngestStatus(str, Enum):
    """Status of ingestion process"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class IngestStatusResponse(BaseModel):
    """Response schema for ingestion status"""
    document_id: str = Field(..., description="Document identifier")
    status: IngestStatus = Field(..., description="Current status")
    progress: float = Field(..., description="Progress percentage (0-100)")
    message: Optional[str] = Field(None, description="Status message")
    error: Optional[str] = Field(None, description="Error message if failed")
FILE: app/api/v1/schemas/query.py
"""
Pydantic schemas for query endpoints
"""
from enum import Enum
from typing import List, Optional, Dict, Any

from pydantic import BaseModel, Field

class QueryType(str, Enum):
    """Types of queries supported"""
    TEXT = "text"
    MULTIMODAL = "multimodal"

class QueryRequest(BaseModel):
    """Request schema for querying the system"""
    query: str = Field(..., description="The query text")
    query_type: QueryType = Field(QueryType.TEXT, description="Type of query")
    modality_filter: Optional[List[str]] = Field(None, description="Filter by modalities")
    max_results: Optional[int] = Field(None, description="Maximum number of results to return")
    include_evidence: bool = Field(True, description="Include evidence in response")
    include_citations: bool = Field(True, description="Include citations in response")

class Evidence(BaseModel):
    """Schema for evidence supporting the answer"""
    document_id: str = Field(..., description="Source document identifier")
    modality: str = Field(..., description="Modality of the evidence")
    content: str = Field(..., description="Evidence content")
    confidence_score: float = Field(..., description="Confidence score (0-1)")
    similarity_score: float = Field(..., description="Similarity score to query")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")

class Assumption(BaseModel):
    """Schema for assumptions made in the response"""
    description: str = Field(..., description="Description of the assumption")
    confidence_score: float = Field(..., description="Confidence score (0-1)")
    reasoning: str = Field(..., description="Reasoning behind the assumption")

class Citation(BaseModel):
    """Schema for citations in the response"""
    evidence_id: str = Field(..., description="Reference to evidence")
    text_span: str = Field(..., description="Specific text span being cited")
    page_number: Optional[int] = Field(None, description="Page number if applicable")
    timestamp: Optional[float] = Field(None, description="Timestamp if audio/video")

class QueryResponse(BaseModel):
    """Response schema for query results"""
    query_id: str = Field(..., description="Unique identifier for the query")
    answer: str = Field(..., description="Generated answer")
    confidence_score: float = Field(..., description="Overall confidence score (0-1)")
    evidence: List[Evidence] = Field(default_factory=list, description="Supporting evidence")
    assumptions: List[Assumption] = Field(default_factory=list, description="Assumptions made")
    citations: List[Citation] = Field(default_factory=list, description="Citations in the answer")
    processing_time: float = Field(..., description="Time taken to process query (seconds)")
    status: str = Field("success", description="Query status")

    class Config:
        protected_namespaces = ()

class ConflictInfo(BaseModel):
    """Schema for conflict information"""
    conflict_type: str = Field(..., description="Type of conflict detected")
    description: str = Field(..., description="Description of the conflict")
    conflicting_evidence: List[str] = Field(..., description="IDs of conflicting evidence")
    resolution_suggestion: Optional[str] = Field(None, description="Suggested resolution")

class UncertaintyInfo(BaseModel):
    """Schema for uncertainty information"""
    uncertainty_type: str = Field(..., description="Type of uncertainty")
    description: str = Field(..., description="Description of uncertainty")
    confidence_range: tuple[float, float] = Field(..., description="Confidence range (min, max)")

class RefusalReason(str, Enum):
    """Reasons for refusing to answer"""
    INSUFFICIENT_EVIDENCE = "insufficient_evidence"
    CONFLICTING_EVIDENCE = "conflicting_evidence"
    HIGH_UNCERTAINTY = "high_uncertainty"
    HALLUCINATION_RISK = "hallucination_risk"

class RefusalResponse(BaseModel):
    """Response schema when query is refused"""
    query_id: str = Field(..., description="Unique identifier for the query")
    refusal_reason: RefusalReason = Field(..., description="Reason for refusal")
    explanation: str = Field(..., description="Explanation of why the query was refused")
    suggestions: List[str] = Field(default_factory=list, description="Suggestions for improvement")
    conflicts: Optional[List[ConflictInfo]] = Field(None, description="Detected conflicts")
    uncertainties: Optional[List[UncertaintyInfo]] = Field(None, description="Detected uncertainties")
    processing_time: float = Field(..., description="Time taken to process query (seconds)")
    status: str = Field("refused", description="Query status")
FILE: app/api/v1/schemas/response.py
[EMPTY FILE]
FILE: app/api/v1/schemas/retrieval.py
[EMPTY FILE]
FILE: app/config.py
"""
Configuration settings for the Multimodal RAG System
"""

from pathlib import Path
from typing import List

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    """Application settings loaded from environment variables"""

    openai_api_key: str = Field(default="", alias="OPENAI_API_KEY")
    twilio_account_sid: str = Field(default="", alias="TWILIO_ACCOUNT_SID")
    twilio_auth_token: str = Field(default="", alias="TWILIO_AUTH_TOKEN")
    twilio_whatsapp_number: str = Field(default="", alias="TWILIO_WHATSAPP_NUMBER")
    whatsapp_recipient_number: str = Field(default="", alias="WHATSAPP_RECIPIENT_NUMBER")
    langsmith_api_key: str = Field(default="", alias="LANGSMITH_API_KEY")
    langsmith_project_name: str = Field(
        default="multimodal-rag-system", alias="LANGSMITH_PROJECT_NAME"
    )

    app_name: str = Field(default="Multimodal RAG System", alias="APP_NAME")
    app_version: str = Field(default="1.0.0", alias="APP_VERSION")
    debug: bool = Field(default=True, alias="DEBUG")
    host: str = Field(default="0.0.0.0", alias="HOST")
    port: int = Field(default=8000, alias="PORT")

    base_path: Path = Path(__file__).resolve().parent.parent

    llm_backend: str = Field(default="llama_cpp", alias="LLM_BACKEND")

    llama_cpp_n_gpu_layers: int = Field(default=-1, alias="LLAMA_CPP_N_GPU_LAYERS")
    llama_cpp_main_gpu: int = Field(default=0, alias="LLAMA_CPP_MAIN_GPU")
    llama_cpp_n_ctx: int = Field(default=4096, alias="LLAMA_CPP_N_CTX")
    llama_cpp_n_batch: int = Field(default=512, alias="LLAMA_CPP_N_BATCH")
    llama_cpp_n_threads: int = Field(default=8, alias="LLAMA_CPP_N_THREADS")

    temperature: float = Field(default=0.1, alias="TEMPERATURE")
    max_tokens: int = Field(default=2048, alias="MAX_TOKENS")

    whisper_model_name: str = Field(default="base", alias="WHISPER_MODEL_NAME")

    collection_name: str = Field(default="pluto-rag", alias="COLLECTION_NAME")
    embedding_dimension: int = Field(default=512, alias="EMBEDDING_DIMENSION")
    qdrant_host: str = Field(default="localhost", alias="QDRANT_HOST")
    qdrant_port: int = Field(default=6333, alias="QDRANT_PORT")

    max_retrieval_results: int = Field(default=10, alias="MAX_RETRIEVAL_RESULTS")
    similarity_threshold: float = Field(default=0.4, alias="SIMILARITY_THRESHOLD")
    reranking_enabled: bool = Field(default=True, alias="RERANKING_ENABLED")

    max_upload_size: int = Field(default=100_000_000, alias="MAX_UPLOAD_SIZE")

    allowed_extensions: List[str] = Field(
        default_factory=lambda: [
            "pdf", "doc", "docx", "txt",
            "jpg", "jpeg", "png",
            "mp3", "wav", "flac",
        ]
    )

    notification_enabled: bool = Field(default=True, alias="NOTIFICATION_ENABLED")
    notification_conflict_threshold: float = Field(
        default=0.3, alias="NOTIFICATION_CONFLICT_THRESHOLD"
    )
    notification_uncertainty_threshold: float = Field(
        default=0.5, alias="NOTIFICATION_UNCERTAINTY_THRESHOLD"
    )

    log_level: str = Field(default="INFO", alias="LOG_LEVEL")

    model_config = SettingsConfigDict(
        env_file=".env",
        case_sensitive=False,
        extra="allow",
    )

    @property
    def data_dir(self) -> Path:
        return self.base_path / "data"

    @property
    def models_dir(self) -> Path:
        return self.data_dir / "models"

    @property
    def vectorstore_dir(self) -> Path:
        return self.data_dir / "vectorstore"

    @property
    def qdrant_storage_dir(self) -> Path:
        return self.vectorstore_dir / "qdrant_storage"

    @property
    def uploads_dir(self) -> Path:
        return self.data_dir / "uploads"

    @property
    def logs_dir(self) -> Path:
        return self.data_dir / "logs"

    @property
    def cache_dir(self) -> Path:
        return self.data_dir / "cache"

    @property
    def log_file_path(self) -> Path:
        return self.logs_dir / "application" / "app.log"

    def ensure_directories(self) -> None:
        directories = [
            self.models_dir,
            self.vectorstore_dir,
            self.qdrant_storage_dir,
            self.uploads_dir / "text",
            self.uploads_dir / "images",
            self.uploads_dir / "audio",
            self.uploads_dir / "processed",
            self.logs_dir / "application",
            self.logs_dir / "errors",
            self.cache_dir / "embeddings",
            self.cache_dir / "ocr_results",
            self.cache_dir / "transcriptions",
        ]

        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)

settings = Settings()
settings.ensure_directories()
FILE: app/core/__init__.py
[EMPTY FILE]
FILE: app/core/constants.py
[EMPTY FILE]
FILE: app/core/exceptions.py
[EMPTY FILE]
FILE: app/core/logging_config.py
[EMPTY FILE]
FILE: app/core/settings.py
[EMPTY FILE]
FILE: app/embeddings/__init__.py
[EMPTY FILE]
FILE: app/embeddings/manager.py
"""
Embeddings Manager - Unified interface for multimodal embeddings with caching
"""
import logging
from typing import List, Union, Dict, Any
from pathlib import Path
from PIL import Image
import numpy as np
import hashlib
from functools import lru_cache

from app.embeddings.models.multimodal_embedder import MultimodalEmbedder
from app.config import settings

logger = logging.getLogger(__name__)

class EmbeddingsManager:
    """Centralized manager for generating embeddings across modalities"""

    def __init__(self):
        self.embedder = MultimodalEmbedder()
        self._text_cache = {}
        self._cache_max_size = 1000
        logger.info("EmbeddingsManager initialized with caching")

    def embed_text(self, text: str, use_cache: bool = True) -> List[float]:
        """Generate embedding for text with caching"""
        try:
            if use_cache:
                text_hash = hashlib.md5(text.encode()).hexdigest()
                if text_hash in self._text_cache:
                    logger.debug("Cache hit for text embedding")
                    return self._text_cache[text_hash]

            embedding = self.embedder.embed(text)

            if use_cache:
                if len(self._text_cache) >= self._cache_max_size:
                    first_key = next(iter(self._text_cache))
                    del self._text_cache[first_key]
                self._text_cache[text_hash] = embedding

            return embedding
        except Exception as e:
            logger.error(f"Failed to embed text: {e}")
            raise

    def embed_image(self, image: Union[str, Path, Image.Image]) -> List[float]:
        """
        Generate embedding for image

        Args:
            image: PIL Image, or path to image file

        Returns:
            List of floats representing the embedding
        """
        try:
            if isinstance(image, (str, Path)):
                image = Image.open(image)

            embedding = self.embedder.embed(image)
            return embedding
        except Exception as e:
            logger.error(f"Failed to embed image: {e}")
            raise

    def embed_batch_text(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """Generate embeddings with batching for better throughput"""
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            logger.debug(f"Processing batch {i//batch_size + 1}")

            for text in batch:
                try:
                    embedding = self.embed_text(text, use_cache=True)
                    embeddings.append(embedding)
                except Exception as e:
                    logger.error(f"Failed to embed text: {e}")
                    embeddings.append([0.0] * 512)

        return embeddings

    def clear_cache(self):
        """Clear embedding cache"""
        self._text_cache.clear()
        logger.info("Embedding cache cleared")

    def embed_batch_images(self, images: List[Union[str, Path, Image.Image]]) -> List[List[float]]:
        """
        Generate embeddings for multiple images

        Args:
            images: List of PIL Images or paths

        Returns:
            List of embeddings
        """
        embeddings = []
        for image in images:
            try:
                embedding = self.embed_image(image)
                embeddings.append(embedding)
            except Exception as e:
                logger.error(f"Failed to embed image in batch: {e}")
                embeddings.append([0.0] * 512)

        return embeddings

    def embed_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Add embeddings to text chunks

        Args:
            chunks: List of chunk dictionaries with 'content' field

        Returns:
            Chunks with added 'embedding' field
        """
        for chunk in chunks:
            try:
                content = chunk.get('content', chunk.get('text', ''))
                if content:
                    chunk['embedding'] = self.embed_text(content)
                else:
                    logger.warning(f"Empty content in chunk: {chunk.get('chunk_id', 'unknown')}")
                    chunk['embedding'] = [0.0] * 512
            except Exception as e:
                logger.error(f"Failed to embed chunk: {e}")
                chunk['embedding'] = [0.0] * 512

        return chunks

    def get_embedding_dimension(self) -> int:
        """Get the dimension of embeddings"""
        return 512
FILE: app/embeddings/models/__init__.py
[EMPTY FILE]
FILE: app/embeddings/models/clip_embedder.py
"""
CLIP Embedder for multimodal embeddings
"""
import logging
from typing import List, Union
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import numpy as np

from app.config import settings

logger = logging.getLogger(__name__)

class CLIPEmbedder:
    """CLIP-based embedder for text and image embeddings"""

    def __init__(self):
        self.model_name = settings.clip_model_name

        if not torch.cuda.is_available():
            errorMsg = (
                "FATAL: CUDA/GPU not available! "
                "CLIP embeddings require GPU acceleration. "
                "CPU execution is disabled for performance reasons. "
                "Please ensure CUDA is installed and a compatible GPU is available."
            )
            logger.error(errorMsg)
            raise RuntimeError(errorMsg)

        self.device = "cuda"
        logger.info(f"GPU detected for CLIP: {torch.cuda.get_device_name(0)}")
        logger.info(f"CUDA version: {torch.version.cuda}")
        logger.info(f"Using device: {self.device}")

        self.model = None
        self.processor = None
        self._load_model()

    def _load_model(self):
        """Load CLIP model and processor"""
        try:
            logger.info(f"Loading CLIP model: {self.model_name}")
            self.model = CLIPModel.from_pretrained(self.model_name, local_files_only=True).to(self.device)
            self.processor = CLIPProcessor.from_pretrained(self.model_name, local_files_only=True)
            logger.info("CLIP model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load CLIP model: {e}")
            raise

    def encode_text(self, texts: Union[str, List[str]]) -> np.ndarray:
        """Encode text(s) to embeddings"""
        if isinstance(texts, str):
            texts = [texts]

        try:
            inputs = self.processor(text=texts, return_tensors="pt", padding=True).to(self.device)
            with torch.no_grad():
                text_features = self.model.get_text_features(**inputs)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            return text_features.cpu().numpy()
        except Exception as e:
            logger.error(f"Failed to encode text: {e}")
            raise

    def encode_image(self, images: Union[Image.Image, List[Image.Image]]) -> np.ndarray:
        """Encode image(s) to embeddings"""
        if isinstance(images, Image.Image):
            images = [images]

        try:
            inputs = self.processor(images=images, return_tensors="pt").to(self.device)
            with torch.no_grad():
                image_features = self.model.get_image_features(**inputs)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            return image_features.cpu().numpy()
        except Exception as e:
            logger.error(f"Failed to encode image: {e}")
            raise

    def encode_multimodal(self, texts: List[str], images: List[Image.Image]) -> np.ndarray:
        """Encode combined text and image for multimodal embedding"""
        try:
            text_inputs = self.processor(text=texts, return_tensors="pt", padding=True).to(self.device)
            image_inputs = self.processor(images=images, return_tensors="pt").to(self.device)

            with torch.no_grad():
                text_features = self.model.get_text_features(**text_inputs)
                image_features = self.model.get_image_features(**image_inputs)

                combined_features = (text_features + image_features) / 2
                combined_features = combined_features / combined_features.norm(dim=-1, keepdim=True)

            return combined_features.cpu().numpy()
        except Exception as e:
            logger.error(f"Failed to encode multimodal: {e}")
            raise
FILE: app/embeddings/models/multimodal_embedder.py
from fastembed import ImageEmbedding, TextEmbedding
import numpy as np
from typing import List, Union
from PIL import Image
import tempfile
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class MultimodalEmbedder:
    def __init__(self):
        cpu_provider = ["CPUExecutionProvider"]
        self.text_model = TextEmbedding(
            model_name="Qdrant/clip-ViT-B-32-text",
            providers=cpu_provider
        )
        self.image_model = ImageEmbedding(
            model_name="Qdrant/clip-ViT-B-32-vision",
            providers=cpu_provider
        )

    def encode_text(self, text: Union[str, List[str]]) -> Union[np.ndarray, List[np.ndarray]]:
        texts = [text] if isinstance(text, str) else text
        embeddings = list(self.text_model.embed(texts))
        return embeddings[0] if isinstance(text, str) else embeddings

    def encode_image(self, image_path: Union[str, List[str]]) -> Union[np.ndarray, List[np.ndarray]]:
        paths = [image_path] if isinstance(image_path, str) else image_path
        embeddings = list(self.image_model.embed(paths))
        return embeddings[0] if isinstance(image_path, str) else embeddings

    def embed(self, input_data: Union[str, Image.Image]) -> list:
        """
        Embed text or image into a 512D vector using fastembed.

        Args:
            input_data: str (text) or PIL.Image (image)
        Returns:
            list: 1D list of 512 floats
        """
        if isinstance(input_data, str):
            logger.debug(f"[fastembed] Embedding text: {input_data[:50]}...")
            embedding = self.encode_text(input_data)
            embedding_array = np.array(embedding)
            result = embedding_array.tolist()
            logger.debug(f"[fastembed] Generated text embedding: {len(result)}D")
            return result
        elif isinstance(input_data, Image.Image):
            logger.debug("[fastembed] Embedding image")
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                input_data.save(tmp.name, format='PNG')
                tmp_path = tmp.name
            try:
                embedding = self.encode_image(tmp_path)
                embedding_array = np.array(embedding)
                result = embedding_array.tolist()
                logger.debug(f"[fastembed] Generated image embedding: {len(result)}D")
                return result
            finally:
                Path(tmp_path).unlink(missing_ok=True)
        else:
            raise ValueError("Input must be a string (text) or PIL.Image (image)")
FILE: app/embeddings/models/text_embedder.py
[EMPTY FILE]
FILE: app/graph/__init__.py
[EMPTY FILE]
FILE: app/graph/edges/__init__.py
[EMPTY FILE]
FILE: app/graph/edges/conditional_edges.py
[EMPTY FILE]
FILE: app/graph/edges/routing_logic.py
[EMPTY FILE]
FILE: app/graph/graph_builder.py
"""
LangGraph builder for multimodal RAG workflow
"""
import logging
from typing import Dict, Any, List, Optional
from langgraph.graph import StateGraph
from langgraph.constants import END
from langchain_core.messages import HumanMessage, AIMessage

from app.graph.state import GraphState
from app.graph.nodes.query_analysis_node import QueryAnalysisNode
from app.graph.nodes.retrieval_node import RetrievalNode
from app.graph.nodes.gate_node import compatibility_gate
from app.graph.nodes.evidence_evaluation_node import EvidenceEvaluationNode
from app.graph.nodes.evidence_grader_node import evidence_grader
from app.graph.nodes.conflict_detector_node import conflict_detector
from app.graph.nodes.generation_node import GenerationNode
from app.graph.nodes.refusal_node import RefusalNode
from app.retrieval.orchestrator import RetrievalOrchestrator

logger = logging.getLogger(__name__)

class GraphBuilder:
    """Builder for the multimodal RAG LangGraph"""

    def __init__(self):
        self.graph: Optional[StateGraph] = None
        self.nodes = {}
        self._initialize_nodes()

    def _initialize_nodes(self):
        """Initialize all graph nodes"""
        retrieval_orchestrator = RetrievalOrchestrator()

        self.nodes = {
            "query_analysis": QueryAnalysisNode(),
            "compatibility_gate": compatibility_gate,
            "retrieval": RetrievalNode(retrieval_orchestrator),
            "evidence_grader": evidence_grader,
            "conflict_detector": conflict_detector,
            "evidence_evaluation": EvidenceEvaluationNode(),
            "generation": GenerationNode(),
            "refusal": RefusalNode(),
        }

    def build_graph(self):
        """Build the complete LangGraph workflow"""
        self.graph = StateGraph(GraphState)

        assert self.graph is not None, "StateGraph initialization failed"

        for node_name, node_instance in self.nodes.items():
            self.graph.add_node(node_name, node_instance.run)

        self._define_edges()

        self.graph.set_entry_point("query_analysis")

        compiled_graph = self.graph.compile()

        logger.info("LangGraph workflow built successfully")
        return compiled_graph

    def _define_edges(self):
        """Define the edges between nodes with Chakravyuh 1.0 agentic workflow"""
        if self.graph is None:
            raise RuntimeError("Graph not initialized; call build_graph() first")

        self.graph.add_edge("query_analysis", "compatibility_gate")

        self.graph.add_conditional_edges(
            "compatibility_gate",
            self.route_after_gate,
            {
                "retrieval": "retrieval",
                "refusal": "refusal"
            }
        )

        self.graph.add_edge("retrieval", "evidence_grader")

        self.graph.add_conditional_edges(
            "evidence_grader",
            self._route_after_grading,
            {
                "conflict_detector": "conflict_detector",
                "refusal": "refusal"
            }
        )

        self.graph.add_edge("conflict_detector", "evidence_evaluation")

        self.graph.add_conditional_edges(
            "evidence_evaluation",
            self.check_evidence_sufficiency,
            {
                "generation": "generation",
                "refusal": "refusal"
            }
        )

        self.graph.add_edge("generation", END)
        self.graph.add_edge("refusal", END)

    def _route_after_grading(self, state: GraphState) -> str:
        """
        Relevance Gatekeeper: Route after evidence grading.

        Refuses if:
        1. is_sufficient=False (no documents passed threshold)
        2. Average evidence score < 0.4 (semantic drift - wrong topic)

        This prevents hallucinations where CLIP finds "best matches" in irrelevant data.
        """
        is_sufficient = state.get('is_sufficient', False)
        evidence_scores = state.get('evidence_scores', [])

        avg_score = sum(evidence_scores) / len(evidence_scores) if evidence_scores else 0.0

        if not is_sufficient:
            logger.warning(f"Evidence insufficient after grading (0 docs passed threshold). Routing to refusal.")
            return "refusal"

        if avg_score < 0.4:
            logger.warning(f"Evidence score too low (avg={avg_score:.3f} < 0.4). Topic mismatch detected. Routing to refusal.")
            return "refusal"

        logger.info(f"Evidence sufficient (avg score={avg_score:.3f}). Proceeding to conflict detection.")
        return "conflict_detector"

    def route_after_gate(self, state: GraphState) -> str:
        """
        Route after compatibility gate: check if query is allowed.
        """
        if not state.get("is_allowed", True):
            logger.info("[GATE] Query not allowed - routing to refusal")
            return "refusal"
        logger.info("[GATE] Query allowed - routing to retrieval")
        return "retrieval"

    def check_evidence_sufficiency(self, state: GraphState) -> str:
        """
        Gatekeeper edge for hallucination suppression.
        """
        if state["evidence_score"] < 0.4:
            return "refusal"

        if state.get("is_conflicting"):
            return "generation"

        return "generation"

    def get_graph_visualization(self) -> str:
        """Get a text representation of the graph structure"""
        return """
Chakravyuh 1.0 Compliant Multimodal RAG Workflow:

1. Query Analysis -> Detect intent and required modalities
   â†“
2. Retrieval -> Fetch top-K chunks from text/image/audio sources
   â†“
3. Evidence Grader (GPU) -> Score each chunk for relevance (0-1)
   â”œâ”€â”€ is_sufficient=False -> Refusal Node (no relevant evidence)
   â””â”€â”€ is_sufficient=True -> Conflict Detector
   â†“
4. Conflict Detector (GPU) -> Cross-reference sources for contradictions
   â”œâ”€â”€ Sets is_conflicting=True if conflicts found
   â””â”€â”€ Proceeds to Evidence Evaluation
   â†“
5. Evidence Evaluation -> Legacy confidence scoring
   â†“
6. Generation Node (Conflict-Aware)
   â”œâ”€â”€ is_conflicting=False -> Standard grounded answer
   â””â”€â”€ is_conflicting=True -> Multi-interpretation format (both sides)
   â†“
7. END

Refusal Node: "I cannot find enough verified evidence to answer this."
"""
FILE: app/graph/nodes/__init__.py
[EMPTY FILE]
FILE: app/graph/nodes/conflict_detector_node.py
"""
Conflict Detector Node - GPU Accelerated
Cross-references sources to identify contradictory information across modalities.
"""
import logging
from typing import Dict, List
from itertools import combinations
from app.graph.state import GraphState
from app.reasoning.llm.llama_reasoner import LlamaReasoner

logger = logging.getLogger(__name__)

class ConflictDetector:
    """Detects contradictions between retrieved sources using LLM"""

    def __init__(self):
        self.reasoner = LlamaReasoner()

    def check_conflict(self, doc1: Dict, doc2: Dict, query: str) -> str:
        """
        Compare two documents to detect contradictions.

        Args:
            doc1: First document dict
            doc2: Second document dict
            query: User's question for context

        Returns:
            Conflict description if found, empty string otherwise
        """
        content1 = doc1.get('content', '')[:1500]
        content2 = doc2.get('content', '')[:1500]

        metadata1 = doc1.get('metadata', {})
        metadata2 = doc2.get('metadata', {})

        source1_path = metadata1.get('source_file') or metadata1.get('file_path', 'Unknown')
        source2_path = metadata2.get('source_file') or metadata2.get('file_path', 'Unknown')

        if isinstance(source1_path, str):
            source1_path = source1_path.replace('\\', '/').replace('\\', '/')
            source1_name = source1_path.split('/')[-1] if '/' in source1_path else source1_path
        else:
            source1_name = 'Unknown'

        if isinstance(source2_path, str):
            source2_path = source2_path.replace('\\', '/').replace('\\', '/')
            source2_name = source2_path.split('/')[-1] if '/' in source2_path else source2_path
        else:
            source2_name = 'Unknown'

        system_prompt = """You are a conflict detection expert. Your task is to identify contradictory information between two evidence sources.

Respond ONLY in this exact format:
Conflict: [yes/no]
Description: [brief summary of the contradiction, or 'No conflict']

A conflict exists when the sources provide incompatible or contradictory answers to the same question.
Minor differences in detail are NOT conflicts unless they fundamentally contradict each other."""

        user_prompt = f"""Question: {query}

Source A ({source1} - {source1_name}):
{content1}

Source B ({source2} - {source2_name}):
{content2}

Do these sources provide contradictory information relevant to the question?"""

        full_prompt = f"{system_prompt}\n\n{user_prompt}"

        try:
            response = self.reasoner.generate(
                prompt=full_prompt,
                max_tokens=150,
                temperature=0.1,
                stop_sequences=["Question:", "Source A:", "Source B:"]
            )

            conflict_line = ""
            description_line = ""
            for line in response.split('\n'):
                if line.startswith('Conflict:'):
                    conflict_line = line.lower()
                elif line.startswith('Description:'):
                    description_line = line.split('Description:', 1)[1].strip()

            has_conflict = 'yes' in conflict_line

            if has_conflict and description_line and 'no conflict' not in description_line.lower():
                conflict_desc = f"Conflict between {source1_name} and {source2_name}: {description_line}"
                logger.info(f"Conflict detected: {conflict_desc}")
                return conflict_desc
            else:
                logger.debug(f"No conflict between {source1_name} and {source2_name}")
                return ""

        except Exception as e:
            logger.error(f"Error checking conflict: {e}")
            return ""

    async def run(self, state: GraphState) -> GraphState:
        """
        Graph-compatible entrypoint.
        Detect conflicts across all pairs of retrieved documents.

        Sets:
            - conflicts: List of conflict descriptions
            - is_conflicting: Boolean flag (True if any conflicts found)
        """
        query = state["query"]
        documents = state.get("retrieved_documents", [])

        logger.info(f"Checking for conflicts among {len(documents)} documents...")

        conflicts = []

        if len(documents) < 2:
            logger.info("Less than 2 documents, skipping conflict detection")
            state["conflicts"] = []
            state["is_conflicting"] = False
            return state

        docs_to_check = documents[:5]
        pairs = list(combinations(enumerate(docs_to_check), 2))

        logger.info(f"Checking {len(pairs)} document pairs for conflicts...")

        for (idx1, doc1), (idx2, doc2) in pairs:
            metadata1 = doc1.get('metadata', {})
            metadata2 = doc2.get('metadata', {})

            source1_path = metadata1.get('source_file') or metadata1.get('file_path', '')
            source2_path = metadata2.get('source_file') or metadata2.get('file_path', '')

            if source1_path and source2_path and source1_path == source2_path:
                continue

            conflict = self.check_conflict(doc1, doc2, query)
            if conflict:
                conflicts.append(conflict)

        state["conflicts"] = conflicts
        state["is_conflicting"] = len(conflicts) > 0

        if state["is_conflicting"]:
            logger.warning(f"Detected {len(conflicts)} conflicts in evidence")
            for conflict in conflicts:
                logger.warning(f"  - {conflict}")
        else:
            logger.info("No conflicts detected in evidence")

        return state

conflict_detector = ConflictDetector()
FILE: app/graph/nodes/evidence_evaluation_node.py
from app.reasoning.evidence.evidence_evaluator import evaluate_evidence
from app.graph.state import GraphState
from app.utils.logging_utils import safe_text
import logging

logger = logging.getLogger(__name__)

class EvidenceEvaluationNode:
    """Node C: The Judge (Evidence Evaluation - Modality-Agnostic)"""
    async def run(self, state: GraphState) -> GraphState:
        retrieved_docs = state.get('retrieved_documents', [])
        query = state.get('query', '')

        logger.info(f"[EVIDENCE] Evaluating {len(retrieved_docs)} retrieved documents")

        modality_breakdown = {}
        if retrieved_docs:
            for doc in retrieved_docs:
                modality = doc.get('modality', 'unknown')
                modality_breakdown[modality] = modality_breakdown.get(modality, 0) + 1

            logger.info(f"[EVIDENCE] Modality breakdown: {modality_breakdown}")

            for i, doc in enumerate(retrieved_docs[:3]):
                content_safe = safe_text(doc.get('content', ''), max_length=80)
                logger.info(f"  Doc {i+1}: score={doc.get('score', 0):.3f}, modality={doc.get('modality')}, content={content_safe}...")
        else:
            logger.warning("[EVIDENCE] No documents retrieved!")


        if not retrieved_docs:
            logger.info("[EVIDENCE] ZERO evidence - refusing")
            state['confidence_score'] = 0.0
            state['evidence_sufficient'] = False
            return state

        score = await evaluate_evidence(retrieved_docs)
        state['confidence_score'] = score

        evidence_threshold = 0.25
        state['evidence_sufficient'] = score > evidence_threshold and len(retrieved_docs) > 0

        logger.info(f"[EVIDENCE] Modality-agnostic evaluation: score={score:.3f}, threshold={evidence_threshold}, sufficient={state['evidence_sufficient']}")
        logger.info(f"[EVIDENCE] Evidence from {len(modality_breakdown)} modalities will be passed to LLM for enumeration")

        return state
FILE: app/graph/nodes/evidence_grader_node.py
"""
Evidence Grader Node - GPU Accelerated
Grades the relevance of each retrieved chunk to filter out irrelevant documents before generation.
"""
import logging
from typing import Dict, Any
from app.graph.state import GraphState
from app.reasoning.llm.llama_reasoner import LlamaReasoner

logger = logging.getLogger(__name__)

class EvidenceGrader:
    """Grades retrieved documents for relevance using LLM"""

    def __init__(self):
        self.reasoner = LlamaReasoner()
        self.relevance_threshold = 0.5

    def grade_document(self, document: Dict, query: str) -> float:
        """
        Grade a single document for relevance to the query.

        Args:
            document: Document dict with 'content', 'source_type', 'metadata'
            query: User's question

        Returns:
            Relevance score between 0.0 and 1.0
        """
        content = document.get('content', '')
        source_type = document.get('source_type', 'unknown')

        content = content[:2000]

        prompt = f"""Task: Is this document relevant to the question?
Question: {query}
Document: {content}
Respond with only 'YES' or 'NO'."""

        try:
            response = self.reasoner.generate(
                prompt=prompt,
                max_tokens=50,
                temperature=0.0,
                stop_sequences=["\n\n", "Question:"]
            )

            response_clean = response.strip().upper()
            is_relevant = 'YES' in response_clean

            if is_relevant:
                final_score = 0.9
                logger.debug(f"Graded document from {source_type}: {final_score:.2f} (YES - relevant)")
            else:
                final_score = 0.0
                logger.debug(f"Graded document from {source_type}: {final_score:.2f} (NO - not relevant)")

            return final_score

        except Exception as e:
            logger.error(f"Error grading document: {e}")
            return 0.5

    async def run(self, state: GraphState) -> GraphState:
        """
        Graph-compatible entrypoint.
        Grades all retrieved documents and filters out irrelevant ones.

        Sets:
            - evidence_scores: List of relevance scores
            - is_sufficient: Boolean flag (True if at least one doc scores >= threshold)
            - retrieved_documents: Filtered list (only relevant docs)
        """
        query = state.get("query", "")
        documents = state.get("retrieved_documents", [])

        logger.info(f"Grading {len(documents)} retrieved documents...")

        if not documents:
            logger.warning("No documents to grade. Setting is_sufficient=False")
            state["evidence_scores"] = []
            state["is_sufficient"] = False
            state["retrieved_documents"] = []
            return state

        evidence_scores = []
        graded_docs = []

        for doc in documents:
            score = self.grade_document(doc, query)
            evidence_scores.append(score)

            if score >= self.relevance_threshold:
                graded_docs.append(doc)

        avg_score = sum(evidence_scores) / len(evidence_scores) if evidence_scores else 0.0
        max_score = max(evidence_scores) if evidence_scores else 0.0
        passed_count = len(graded_docs)

        logger.info(f"Evidence grading complete: {passed_count}/{len(documents)} documents passed threshold {self.relevance_threshold}")
        logger.info(f"Scores - Avg: {avg_score:.3f}, Max: {max_score:.3f}")

        state["evidence_scores"] = evidence_scores
        state["is_sufficient"] = passed_count > 0
        state["retrieved_documents"] = graded_docs
        state["evidence_score"] = max_score

        return state

evidence_grader = EvidenceGrader()
FILE: app/graph/nodes/gate_node.py
"""
Compatibility Gate Node - Topic and Concept Matching
Binary text-based check before retrieval to prevent semantic overlap hallucinations
"""
import logging
from app.graph.state import GraphState
from app.storage.vector_store import VectorStore
from app.utils.topic_utils import normalize_topic

logger = logging.getLogger(__name__)

class CompatibilityGateNode:
    """
    Binary gate that checks if query topics/concepts exist in knowledge base.
    Prevents retrieval when query is clearly out of scope.
    """

    def __init__(self):
        self.vector_store = VectorStore()
        self.llm = None

    def _get_llm(self):
        """Lazy load LLM only when needed for semantic fallback"""
        if self.llm is None:
            from app.reasoning.llm.llama_reasoner import LlamaReasoner
            self.llm = LlamaReasoner()
        return self.llm

    def check_semantic_relationship(self, query_topic: str, doc_topics: list) -> bool:
        """
        Asks LLM if the topics are related to handle synonyms/typos.
        This is a fallback when strict string matching fails.
        """
        if not query_topic or not doc_topics:
            return False

        topics_str = ", ".join(doc_topics)
        prompt = f"""Task: Determine if the Query Topic belongs to the Knowledge Base.
Query Topic: {query_topic}
Knowledge Base Topics: {topics_str}

Is '{query_topic}' related to or a sub-topic of the Knowledge Base?
Respond with exactly YES or NO."""

        try:
            llm = self._get_llm()
            response = llm.generate(prompt=prompt, max_tokens=10, temperature=0.0)
            result = "YES" in response.strip().upper()
            logger.info(f"[GATE] LLM semantic check: '{query_topic}' -> {response.strip()}")
            return result
        except Exception as e:
            logger.error(f"[GATE] Semantic check failed: {e}")
            return False

    async def run(self, state: GraphState) -> GraphState:
        """
        Check if query is compatible with knowledge base using direct catalog matching.

        Uses direct catalog lookup instead of VectorStore.query() safeguards.
        """
        query_text = state.get('query', '')

        try:
            query_topic = normalize_topic(state.get("query_topic", ""))
            query_concepts = [normalize_topic(c) for c in state.get("query_concepts", [])]

            catalog = self.vector_store.get_knowledge_catalog()
            doc_topics = [normalize_topic(t) for t in catalog["topics"]]
            doc_concepts = [normalize_topic(c) for c in catalog["concepts"]]

            logger.info(f"[GATE] Checking Query Topic: '{query_topic}' against Doc Topics: {doc_topics}")

            for concept in query_concepts:
                for kb_concept in doc_concepts:
                    if concept in kb_concept or kb_concept in concept:
                        logger.info(f"[GATE] Direct concept match: '{concept}' <-> '{kb_concept}'")
                        state["is_allowed"] = True
                        state["gate_reason"] = f"concept_match: {concept}"
                        logger.info("[GATE] Query allowed - routing to retrieval")
                        return state

            for concept in query_concepts:
                for doc_t in doc_topics:
                    if concept in doc_t.lower():
                        logger.info(f"[GATE] Concept found in topic: '{concept}' in '{doc_t}'")
                        state["is_allowed"] = True
                        state["gate_reason"] = f"concept_in_topic: {concept}"
                        logger.info("[GATE] Query allowed - routing to retrieval")
                        return state

            for doc_t in doc_topics:
                if query_topic in doc_t or doc_t in query_topic:
                    logger.info(f"[GATE] Fuzzy topic match: {query_topic} <-> {doc_t}")
                    state["is_allowed"] = True
                    state["gate_reason"] = f"fuzzy_topic_match: {query_topic} <-> {doc_t}"
                    logger.info("[GATE] Query allowed - routing to retrieval")
                    return state

            logger.info("[GATE] No string match found. Attempting semantic fallback...")
            if self.check_semantic_relationship(query_topic, doc_topics):
                logger.info(f"[GATE] Semantic match confirmed by LLM.")
                state["is_allowed"] = True
                state["gate_reason"] = "semantic_match_llm"
                logger.info("[GATE] Query allowed - routing to retrieval")
                return state

            logger.warning(f"[GATE] Query refused: no_match: query topic '{query_topic}' and concepts {query_concepts} not found in knowledge base topics {doc_topics} or concepts {doc_concepts[:20]}...")
            state["is_allowed"] = False
            state["gate_reason"] = f"no_match: query topic '{query_topic}' and concepts {query_concepts} not found in knowledge base topics {doc_topics} or concepts {doc_concepts}"
            state["refusal_explanation"] = f"Your question about '{query_topic}' is not covered by the uploaded documents."
            state["is_out_of_scope"] = True
            return state

        except Exception as e:
            logger.error(f"[GATE] Failed to check compatibility: {e}")
            state['is_allowed'] = False
            state['gate_reason'] = f"compatibility_check_failed: {str(e)}"
            state['refusal_explanation'] = "Unable to verify if your question is within the scope of uploaded documents. Please try rephrasing or check document content."
            state['is_out_of_scope'] = True
            return state

compatibility_gate = CompatibilityGateNode()
FILE: app/graph/nodes/generation_node.py
from app.reasoning.llm.llama_reasoner import LlamaReasoner
from app.graph.state import GraphState
import logging

logger = logging.getLogger(__name__)

class GenerationNode:
    """Node D: Plain Text Answer Generation (LLaMA outputs text ONLY)"""
    def __init__(self):
        self.llama_client = LlamaReasoner()

    async def run(self, state: GraphState) -> GraphState:
        """Generate plain text answer from evidence. Conflict-aware generation."""
        try:
            query = state.get('query', '')
            retrieved_docs = state.get('retrieved_documents', [])
            is_conflicting = state.get('is_conflicting', False)
            conflicts = state.get('conflicts', [])
            conversation_history = state.get('conversation_history', [])

            if is_conflicting and conflicts:
                prompt = self._build_conflict_aware_prompt(retrieved_docs, query, conflicts, conversation_history)
            else:
                prompt = self._build_plain_text_prompt(retrieved_docs, query, conversation_history)

            answer_text = self.llama_client.generate(
                prompt,
                max_tokens=400,
                stop_sequences=["\n\nEvidence", "\n\nUser Question", "Answer:", "\n\n\n"]
            )
            answer_text = answer_text.strip()

            answer_text = answer_text.strip('"\'\'\'\'"')

            answer_text = self._remove_repetitions(answer_text)

            answer_text = self._add_citations(answer_text, retrieved_docs)

            logger.info(f"[GENERATION] LLaMA output (conflict-aware={is_conflicting}): {answer_text[:100]}...")

            state['final_response'] = answer_text
            state['status'] = 'success'

            return state

        except Exception as e:
            logger.error(f"Generation failed: {e}")
            state['final_response'] = "An error occurred during response generation."
            state['status'] = 'error'
            return state

    def _build_plain_text_prompt(self, retrieved_docs: list, query: str, conversation_history: list = None) -> str:
        """
        Build simple plain text prompt for LLaMA.
        LLaMA outputs PLAIN TEXT answer - NO JSON, NO metadata, NO sources.
        Includes conversation history for follow-up questions.
        """
        conversation_context = ""
        if conversation_history and len(conversation_history) > 0:
            conversation_context = "Previous Conversation:\n"
            for turn in conversation_history[-3:]:
                user_q = turn.get('user_query', '')
                system_resp = turn.get('system_response', '')[:200]
                conversation_context += f"User: {user_q}\nAssistant: {system_resp}\n\n"
            conversation_context += "---\n\n"

        evidence_parts = []
        for idx, doc in enumerate(retrieved_docs[:5], 1):
            content = doc.get('content', '')
            if content:
                evidence_parts.append(f"Evidence {idx}: {content[:300]}")

        evidence_context = "\n\n".join(evidence_parts) if evidence_parts else ""
        has_evidence = len(evidence_parts) > 0

        prompt = f"""You are a retrieval-grounded assistant.
Answer ONLY using the provided evidence.
If evidence exists, you MUST answer.
Return ONE concise plain-text answer.
Do NOT repeat sentences.
Do NOT output JSON or lists.
Do NOT mention sources or files.

{conversation_context}Evidence:
{evidence_context if has_evidence else 'No evidence available.'}

User Question: {query}

Answer (plain text only, no repetition):"""

        return prompt

    def _build_conflict_aware_prompt(self, retrieved_docs: list, query: str, conflicts: list, conversation_history: list = None) -> str:
        """
        Build conflict-aware prompt when contradictions are detected.
        Presents both sides and acknowledges uncertainty.
        Includes conversation history for context.
        """
        conversation_context = ""
        if conversation_history and len(conversation_history) > 0:
            conversation_context = "Previous Conversation:\n"
            for turn in conversation_history[-3:]:
                user_q = turn.get('user_query', '')
                system_resp = turn.get('system_response', '')[:200]
                conversation_context += f"User: {user_q}\nAssistant: {system_resp}\n\n"
            conversation_context += "---\n\n"

        evidence_parts = []
        for idx, doc in enumerate(retrieved_docs[:5], 1):
            content = doc.get('content', '')
            source_name = doc.get('metadata', {}).get('file_path', f'Source {idx}')
            if content:
                evidence_parts.append(f"Source {idx} ({source_name}): {content[:300]}")

        evidence_context = "\n\n".join(evidence_parts) if evidence_parts else ""

        conflict_summary = "\n".join([f"- {c}" for c in conflicts])

        prompt = f"""You are a retrieval-grounded assistant trained to acknowledge contradictions.

The evidence contains CONFLICTING information:
{conflict_summary}

{conversation_context}Evidence from multiple sources:
{evidence_context}

User Question: {query}

INSTRUCTIONS:
Since there are contradictions, you MUST present both perspectives.
Use this EXACT format:

"There is a conflict in the evidence. [Source A name] indicates [perspective A], whereas [Source B name] suggests [perspective B]. Based on the available evidence, [provide your reasoned assessment if possible, or state that more information is needed]."

Answer (acknowledge conflict, present both sides):"""

        return prompt

    def _remove_repetitions(self, text: str) -> str:
        """Remove repeated sentences from LLM output."""
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        seen = set()
        unique_sentences = []
        for sentence in sentences:
            if sentence.lower() not in seen:
                seen.add(sentence.lower())
                unique_sentences.append(sentence)
        return '. '.join(unique_sentences) + ('.' if unique_sentences else '')

    def _add_citations(self, text: str, documents: list) -> str:
        """Add source citations to the answer using bracketed notation."""
        if not documents:
            return text

        citations = []
        seen_sources = set()

        for doc in documents[:5]:
            metadata = doc.get('metadata', {})
            source_path = metadata.get('source_file') or metadata.get('file_path', 'Unknown')

            if isinstance(source_path, str):
                source_path = source_path.replace('\\', '/').replace('\\', '/')
                if '/' in source_path:
                    filename = source_path.split('/')[-1]
                else:
                    filename = source_path
            else:
                filename = 'Unknown'

            page = metadata.get('page_number')

            if page:
                citation = f"{filename}, Page {page}"
            else:
                citation = filename

            if citation not in seen_sources:
                seen_sources.add(citation)
                citations.append(citation)

        if citations:
            citation_text = " [" + "; ".join(citations) + "]"
            return text + citation_text

        return text
FILE: app/graph/nodes/notification_node.py
"""
Notification node for the LangGraph workflow
"""
import logging
from typing import Dict, Any
from app.graph.state import GraphState
from app.integrations.twilio.client import TwilioClient

logger = logging.getLogger(__name__)

class NotificationNode:
    """Node responsible for sending notifications"""

    def __init__(self):
        self.twilio_client = TwilioClient()

    def process(self, state: GraphState) -> Dict[str, Any]:
        """Process the notification step"""
        try:
            if not state.needs_notification:
                logger.info("No notification needed")
                return state.dict()

            notification_message = self._generate_notification_message(state)

            success = self.twilio_client.send_notification(
                message=notification_message,
                notification_type=state.notification_type
            )

            if success:
                logger.info(f"Notification sent: {state.notification_type}")
                state.notification_message = notification_message
            else:
                logger.error("Failed to send notification")
                state.notification_message = "Failed to send notification"

            return state.dict()

        except Exception as e:
            logger.error(f"Notification failed: {e}")
            state.notification_message = f"Notification error: {str(e)}"
            return state.dict()

    def _generate_notification_message(self, state: GraphState) -> str:
        """Generate appropriate notification message"""
        if state.notification_type == "conflict":
            return f"""ðŸš¨ Evidence Conflict Detected

Query: {state.query[:100]}...

Found {len(state.conflicts)} conflicting pieces of evidence.
Consistency score: {state.consistency_score:.2f}

Response refused due to conflicting information."""

        elif state.notification_type == "uncertainty":
            return f"""âš ï¸ Low Confidence Response

Query: {state.query[:100]}...

Response generated with confidence score: {state.confidence_score:.2f}
Please review the response carefully."""

        elif state.notification_type == "refusal":
            return f"""âŒ Query Refused

Query: {state.query[:100]}...

Reason: {state.refusal_reason or 'Insufficient evidence'}

Unable to provide reliable answer."""

        else:
            return f"""â„¹ï¸ System Notification

Query: {state.query[:100]}...

{state.notification_message or 'Notification triggered'}"""
FILE: app/graph/nodes/query_analysis_node.py
from app.reasoning.llm.llama_reasoner import LlamaReasoner
from app.retrieval.query.multi_query_generator import generate_multi_queries
from app.graph.state import GraphState
import logging

logger = logging.getLogger(__name__)

class QueryAnalysisNode:
    """Node A: The Strategist (Query Analysis)"""
    def __init__(self):
        self.llama_client = LlamaReasoner()

    async def run(self, state: GraphState) -> GraphState:
        query = state.get('query', '')

        try:
            from app.storage.vector_store import VectorStore
            vector_store = VectorStore()
            kb_summary = vector_store.get_knowledge_catalog()
            kb_topics = kb_summary.get('topics', [])
            kb_concepts = kb_summary.get('concepts', [])

            if not kb_topics and not kb_concepts:
                logger.info("[ANALYSIS] KB is empty - skipping LLM analysis")
                query_words = query.split()[:2]
                state['query_topic'] = ' '.join(query_words).title() if query_words else 'Unknown'
                state['query_concepts'] = [word.lower() for word in query.split()[:3] if len(word) > 2]
                state['expanded_queries'] = [query]
                return state
        except Exception as e:
            logger.warning(f"[ANALYSIS] Failed to check KB status: {e} - proceeding with analysis")

        analysis_prompt = f"""Extract the TOPIC (1-3 words) and KEY CONCEPTS (important nouns/entities) from this question.

Format:
Topic: [topic name]
Concepts: [concept1, concept2, concept3]

Examples:
Question: "What is photosynthesis?" â†’ Topic: Photosynthesis | Concepts: photosynthesis, plants, energy
Question: "How does carbon dioxide affect plants?" â†’ Topic: Plant Biology | Concepts: carbon dioxide, plants, CO2, gas exchange
Question: "Explain machine learning algorithms" â†’ Topic: Machine Learning | Concepts: algorithms, AI, training, models

Question: {query}
Output:"""

        try:
            from app.utils.topic_utils import clean_llm_topic_response, extract_concepts_from_text

            analysis_response = self.llama_client.generate(
                prompt=analysis_prompt,
                max_tokens=50,
                temperature=0.0,
                stop_sequences=["\n\n", "Question:", "Example"]
            )

            query_topic = "Unknown"
            query_concepts = []

            lines = analysis_response.strip().split('|')
            for line in lines:
                line = line.strip()
                if line.lower().startswith('topic:'):
                    query_topic = clean_llm_topic_response(line.replace('Topic:', '').replace('topic:', '').strip())
                elif line.lower().startswith('concepts:'):
                    concepts_str = line.replace('Concepts:', '').replace('concepts:', '').strip()
                    query_concepts = [c.strip().lower() for c in concepts_str.split(',') if c.strip()]

            if not query_topic or len(query_topic) < 3:
                query_words = query.split()[:3]
                query_topic = ' '.join(query_words).title()

            if not query_concepts:
                query_concepts = extract_concepts_from_text(query)

            logger.info(f"[ANALYSIS] Topic: '{query_topic}', Concepts: {query_concepts}")
            state['query_topic'] = query_topic
            state['query_concepts'] = query_concepts

        except Exception as e:
            logger.error(f"Query analysis failed: {e}")
            query_words = query.split()[:2]
            state['query_topic'] = ' '.join(query_words).title()
            state['query_concepts'] = extract_concepts_from_text(query)

        expanded_queries = await generate_multi_queries(query, self.llama_client)
        state['expanded_queries'] = expanded_queries
        return state
FILE: app/graph/nodes/refusal_node.py
"""
Refusal node for the LangGraph workflow
"""
import logging
import json
from typing import Dict, Any
from app.graph.state import GraphState

logger = logging.getLogger(__name__)

class RefusalNode:
    """Node responsible for generating refusal responses in evidence-gated RAG"""

    def __init__(self):
        pass

    async def run(self, state: GraphState) -> GraphState:
        """Generate structured refusal response when evidence is insufficient"""
        try:
            query = state.get('query', '')
            logger.info(f"Generating refusal for query: {query[:50]}...")

            if state.get('refusal_explanation'):
                refusal_text = state['refusal_explanation']
                logger.info(f"Using gate-provided refusal explanation: {refusal_text[:100]}...")
            elif (state.get("is_out_of_scope") or not state.get("is_allowed", True)) and not state.get('refusal_explanation'):
                try:
                    from app.storage.vector_store import VectorStore
                    vs = VectorStore()
                    kb_summary = vs.get_knowledge_catalog()
                    if not kb_summary.get('topics') and not kb_summary.get('concepts'):
                        refusal_text = "No documents are uploaded yet. Please upload documents before asking questions."
                    else:
                        refusal_text = f"I cannot answer the question: '{query}'.\n\nReason: No supporting evidence was found in the knowledge base.\nSuggestion: Please verify that relevant documents are uploaded."
                except Exception as e:
                    logger.error(f"Failed to re-query KB in refusal node: {e}")
                    refusal_text = f"I cannot answer the question: '{query}'.\n\nReason: No supporting evidence was found in the knowledge base.\nSuggestion: Please verify that relevant documents are uploaded."
            elif state.get("is_out_of_scope") or not state.get("is_allowed", True):
                query_topic = state.get('query_topic', 'Unknown')
                query_concepts = state.get('query_concepts', [])
                kb_summary = state.get('knowledge_base_summary', {})
                document_topics = kb_summary.get('topics', [])
                document_concepts = kb_summary.get('concepts', [])

                topic_list = ', '.join(document_topics) if document_topics else 'Unknown'
                concept_preview = ', '.join(document_concepts[:10]) if document_concepts else 'Unknown'

                refusal_text = (
                    f"I cannot answer the question: '{query}'.\n\n"
                    f"Reason: Topic/Concept Mismatch. Your question is about '{query_topic}' "
                    f"with concepts {query_concepts}, but my current knowledge base only contains:\n"
                    f"  - Topics: {topic_list}\n"
                    f"  - Sample Concepts: {concept_preview}{'...' if len(document_concepts) > 10 else ''}\n\n"
                    f"Suggestion: Please upload documents relevant to '{query_topic}'."
                )
            else:
                refusal_text = (
                    f"I cannot answer the question: '{query}'.\n\n"
                    f"Reason: No supporting evidence was found in the knowledge base.\n"
                    f"Suggestion: Please verify that relevant documents are uploaded."
                )

            return {
                **state,
                "final_response": refusal_text,
                "status": "refused"
            }

        except Exception as e:
            logger.error(f"Refusal generation failed: {e}")
            state['final_response'] = "Unable to process query due to insufficient evidence."
            state['confidence_score'] = 0.0
            state['cited_sources'] = []
            return state
FILE: app/graph/nodes/retrieval_node.py
from app.retrieval.orchestrator import RetrievalOrchestrator
from app.retrieval.strategies.multimodal_strategy import multimodal_retrieve
from app.retrieval.query.multi_query_generator import generate_multi_queries
from app.reasoning.llm.llama_reasoner import LlamaReasoner
from app.graph.state import GraphState
from app.utils.logging_utils import safe_text
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)

class RetrievalNode:
    """Node B: The Librarian (Retrieval - Modality-Agnostic)"""
    def __init__(self, orchestrator):
        self.orchestrator = orchestrator
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.llama_client = LlamaReasoner()

    async def run(self, state: GraphState) -> GraphState:
        """Parallel multi-query retrieval"""
        query = state["query"]
        session_id = state.get("session_id", "default")
        top_k = state.get("top_k", 10)

        queries = await generate_multi_queries(query, self.llama_client)

        logger.info(f"Executing {len(queries)} queries in parallel")

        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(
                self.executor,
                self.orchestrator.vector_store.query,
                q,
                session_id,
                top_k
            )
            for q in queries
        ]

        results_list = await asyncio.gather(*tasks)

        all_results = []
        seen_ids = set()

        for results in results_list:
            documents = results.get('documents', [])
            ids = results.get('ids', [])
            metadatas = results.get('metadatas', [])
            distances = results.get('distances', [])

            for i in range(len(documents)):
                doc_id = ids[i] if i < len(ids) else f'doc_{i}'

                if doc_id not in seen_ids:
                    seen_ids.add(doc_id)
                    all_results.append({
                        'id': doc_id,
                        'content': documents[i],
                        'metadata': metadatas[i] if i < len(metadatas) else {},
                        'score': 1.0 - distances[i] if i < len(distances) else 0.0,
                        'modality': metadatas[i].get('modality', 'text') if i < len(metadatas) else 'text'
                    })

        all_results.sort(key=lambda x: x['score'], reverse=True)
        state["retrieved_documents"] = all_results[:top_k]

        logger.info(f"Retrieved {len(state['retrieved_documents'])} unique documents")
        return state
FILE: app/graph/state.py
from typing import TypedDict, List, Dict, Annotated, Optional
import operator

class GraphState(TypedDict, total=False):
    query: str
    query_topic: str
    query_concepts: List[str]
    is_allowed: bool
    knowledge_base_summary: Dict
    expanded_queries: List[str]
    retrieved_documents: List[Dict]
    final_response: str
    confidence_score: float
    is_hallucination: bool
    cited_sources: List[Dict]
    assumptions: List[str]
    status: str
    evidence_sufficient: bool

    query_intent: str
    required_modalities: List[str]

    evidence_scores: List[float]
    evidence_score: float
    conflicts: List[str]
    is_conflicting: bool
    is_sufficient: bool
    is_out_of_scope: bool
    document_topics: List[str]

    processing_time: float
    model_used: Optional[str]
    timestamp: Optional[str]

    session_id: Optional[str]
    conversation_history: List[Dict]
FILE: app/ingestion/__init__.py
[EMPTY FILE]
FILE: app/ingestion/chunking/__init__.py
[EMPTY FILE]
FILE: app/ingestion/chunking/semantic_chunker.py
from typing import List, Dict
import numpy as np
import re

def dummy_embed(text: str) -> np.ndarray:
	np.random.seed(abs(hash(text)) % (2**32))
	return np.random.rand(384)

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
	return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))

def semantic_chunk_text(text: str, similarity_threshold: float = 0.80) -> List[Dict]:
	"""
	Splits text into semantically coherent chunks by detecting topic shifts.
	Each chunk is a group of sentences with high semantic similarity.
	"""
	sentences = re.split(r'(?<=[.!?])\s+', text.strip())
	if not sentences:
		return []
	chunks = []
	current_chunk = [sentences[0]]
	current_embedding = dummy_embed(sentences[0])
	for sent in sentences[1:]:
		sent_emb = dummy_embed(sent)
		sim = cosine_similarity(current_embedding, sent_emb)
		if sim < similarity_threshold:
			chunks.append(' '.join(current_chunk))
			current_chunk = [sent]
			current_embedding = sent_emb
		else:
			current_chunk.append(sent)
			current_embedding = (current_embedding + sent_emb) / 2
	if current_chunk:
		chunks.append(' '.join(current_chunk))
	return [{
		'chunk': chunk,
		'start_sentence': i,
		'end_sentence': i + chunk.count('.') + chunk.count('!') + chunk.count('?')
	} for i, chunk in enumerate(chunks)]
FILE: app/ingestion/chunking/text_chunker.py
from typing import List, Dict
import uuid
from langchain.text_splitter import RecursiveCharacterTextSplitter

def micro_chunk_text(text: str, chunk_size: int = 235, chunk_overlap: int = 30) -> List[Dict]:
	"""
	Splits text into micro-chunks of specified size to fit CLIP's 77-token (235-char) limit.
	Includes prev/next IDs for context stitching.

	FIX 5: If chunk_size > CLIP limit, RE-CHUNK instead of capping.

	Args:
		text: Input text to chunk
		chunk_size: Maximum characters per chunk (default 235 for CLIP's 77-token limit)
		chunk_overlap: Overlap between chunks for context preservation

	Returns:
		List of chunk dictionaries with content, IDs, and linking metadata
	"""
	CLIP_MAX_CHARS = 235
	if chunk_size > CLIP_MAX_CHARS:
		import logging
		logging.warning(f"chunk_size {chunk_size} exceeds CLIP limit, re-chunking with size={CLIP_MAX_CHARS}")
		chunk_size = CLIP_MAX_CHARS
		chunk_overlap = min(chunk_overlap, 50)

	splitter = RecursiveCharacterTextSplitter(
		chunk_size=chunk_size,
		chunk_overlap=chunk_overlap,
		length_function=len,
		separators=["\n\n", "\n", ". ", " ", ""]
	)
	raw_chunks = splitter.split_text(text)
	processed_chunks = []
	chunk_ids = [str(uuid.uuid4()) for _ in range(len(raw_chunks))]
	parent_id = str(uuid.uuid4())

	for i, chunk_content in enumerate(raw_chunks):
		processed_chunks.append({
			"id": chunk_ids[i],
			"chunk": chunk_content,
			"prev_chunk_id": chunk_ids[i-1] if i > 0 else None,
			"next_chunk_id": chunk_ids[i+1] if i < len(chunk_ids) - 1 else None,
			"parent_chunk_id": parent_id
		})
	return processed_chunks
FILE: app/ingestion/ingestion_service.py
"""
Ingestion service for processing various document types
"""
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from app.ingestion.processors.pdf_processor import PDFProcessor
from app.ingestion.processors.image_processor import ImageProcessor
from app.ingestion.processors.audio_processor import AudioProcessor
from app.ingestion.processors.text_processor import TextProcessor
from app.ingestion.chunking.text_chunker import micro_chunk_text
from app.config import settings

logger = logging.getLogger(__name__)

def get_chunker(chunking_strategy: str, chunk_size: int = 235, overlap: int = 30):
    """Return a chunker function based on strategy. Only 'character' is supported for now."""
    from app.ingestion.chunking.text_chunker import micro_chunk_text
    class CharacterChunker:
        def chunk_text(self, text, metadata=None):
            raw_chunks = micro_chunk_text(text, chunk_size=chunk_size, chunk_overlap=overlap)
            for chunk in raw_chunks:
                if metadata:
                    chunk.update(metadata)
                chunk.setdefault('content', chunk.get('chunk', ''))
                chunk.setdefault('modality', metadata.get('modality', 'text') if metadata else 'text')
                chunk['chunk_id'] = chunk['id']
            return raw_chunks
    if chunking_strategy == "character":
        return CharacterChunker()
    else:
        raise NotImplementedError(f"Chunking strategy '{chunking_strategy}' not implemented.")

class IngestionService:

    def __init__(self):
        self.processors = {
            'pdf': PDFProcessor(),
            'image': ImageProcessor(),
            'audio': AudioProcessor(),
            'text': TextProcessor()
        }

    def get_processor(self, file_path: Path):
        """Get the appropriate processor for a file"""
        for processor in self.processors.values():
            if processor.can_process(file_path):
                return processor
        return None

    def process_file(self, file_path: Path, chunking_strategy: str = "character",
                    chunk_size: int = 235, chunk_overlap: int = 30) -> Dict[str, Any]:
        """
        Process a single file and return chunks

        Args:
            file_path: Path to the file to process
            chunking_strategy: Strategy for chunking ("character", "sentence", "page")
            chunk_size: Size of each chunk (default 235 for CLIP limit)
            chunk_overlap: Overlap between chunks (default 30 chars)

        Returns:
            Dictionary containing processing results and chunks
        """
        try:
            logger.info(f"Processing file: {file_path}")

            processor = self.get_processor(file_path)
            if not processor:
                raise ValueError(f"No processor available for file type: {file_path.suffix}")

            extraction_result = processor.extract_text(file_path)

            if isinstance(extraction_result, list):
                if not extraction_result:
                    logger.warning(f"Empty extraction result for {file_path}")
                    return {
                        'status': 'error',
                        'message': 'No content could be extracted from the file',
                        'file_path': str(file_path),
                        'chunks': []
                    }

                first_item = extraction_result[0]

                if 'embedding' in first_item:
                    chunks = []
                    for idx, entry in enumerate(extraction_result):
                        content = entry.get('text') or entry.get('content') or entry.get('ocr_text', '')
                        if entry.get('type') == 'visual' and not content:
                            content = '[IMAGE_VISUAL_EMBEDDING]'

                        chunk = {
                            'content': content,
                            'embedding': entry.get('embedding'),
                            'modality': entry.get('modality', 'text'),
                            'source_type': entry.get('source_type', 'unknown'),
                            'chunk_index': entry.get('chunk_index', idx),
                            'total_chunks': entry.get('total_chunks', len(extraction_result)),
                            'chunk_id': entry.get('chunk_id'),
                            'prev_chunk_id': entry.get('prev_chunk_id'),
                            'next_chunk_id': entry.get('next_chunk_id'),
                            'metadata': {
                                'source_file': str(file_path),
                                **entry.get('metadata', {})
                            }
                        }

                        if 'start' in entry and 'end' in entry:
                            chunk['metadata']['timestamps'] = {
                                'start': entry['start'],
                                'end': entry['end']
                            }

                        if entry.get('type') in ['visual', 'ocr']:
                            chunk['metadata']['extraction_type'] = entry['type']
                            chunk['metadata']['ocr_confidence'] = entry.get('ocr_confidence', 0)

                        chunks.append(chunk)

                    total_chars = sum(len(chunk.get('content', '')) for chunk in chunks if chunk.get('content') != '[IMAGE_VISUAL_EMBEDDING]')

                    result = {
                        'file_path': str(file_path),
                        'file_name': file_path.name,
                        'file_size': file_path.stat().st_size,
                        'processor_type': type(processor).__name__,
                        'extraction_result': extraction_result,
                        'chunks': chunks,
                        'total_chunks': len(chunks),
                        'chunking_strategy': 'pre-chunked',
                        'chunk_size': 235,
                        'chunk_overlap': 30,
                        'total_chars': total_chars,
                        'status': 'success'
                    }
                    logger.info(f"Successfully processed {file_path.name}: {len(chunks)} pre-chunked segments")
                    return result

                elif 'page_number' in first_item:
                    content = '\n\n'.join(page.get('text', '') for page in extraction_result if page.get('text', '').strip())
                    modality = 'text'
                    metadata = first_item.get('metadata', {})
                    source_type = 'pdf'
                    total_chars = len(content)
                else:
                    main_result = extraction_result[0]
                    content = main_result.get('content', '')
                    modality = main_result.get('modality', 'text')
                    metadata = main_result.get('metadata', {})
                    source_type = main_result.get('source_type', 'text')
                    total_chars = len(content)
            else:
                main_result = extraction_result
                content = extraction_result.get('content', '')
                modality = extraction_result.get('modality', 'text')
                metadata = extraction_result.get('metadata', {})
                source_type = extraction_result.get('source_type', 'unknown')
                total_chars = extraction_result.get('total_chars', 0)

                if 'chunk_index' in extraction_result and 'embedding' not in extraction_result:
                    chunks = [{
                        'content': content,
                        'modality': modality,
                        'source_type': source_type,
                        'chunk_index': extraction_result.get('chunk_index', 0),
                        'total_chunks': extraction_result.get('total_chunks', 1),
                        'prev_chunk_id': extraction_result.get('prev_chunk_id'),
                        'next_chunk_id': extraction_result.get('next_chunk_id'),
                        'metadata': {
                            'source_file': str(file_path),
                            **metadata
                        }
                    }]

                    result = {
                        'file_path': str(file_path),
                        'file_name': file_path.name,
                        'file_size': file_path.stat().st_size,
                        'processor_type': type(processor).__name__,
                        'extraction_result': extraction_result,
                        'chunks': chunks,
                        'total_chunks': 1,
                        'chunking_strategy': 'single',
                        'chunk_size': 235,
                        'chunk_overlap': 0,
                        'total_chars': total_chars,
                        'status': 'success'
                    }
                    logger.info(f"Successfully processed {file_path.name}: 1 chunk (no splitting needed)")
                    return result

            chunker = get_chunker(chunking_strategy, chunk_size=chunk_size, overlap=chunk_overlap)

            chunks = chunker.chunk_text(
                content,
                metadata={
                    'source_file': str(file_path),
                    'modality': modality,
                    'source_type': source_type,
                    'extraction_metadata': metadata,
                    'total_chars': total_chars
                }
            )

            result = {
                'file_path': str(file_path),
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'processor_type': type(processor).__name__,
                'extraction_result': extraction_result,
                'chunks': [self._chunk_to_dict(chunk) for chunk in chunks],
                'total_chunks': len(chunks),
                'chunking_strategy': chunking_strategy,
                'chunk_size': chunk_size,
                'chunk_overlap': chunk_overlap,
                'total_chars': total_chars,
                'status': 'success'
            }

            logger.info(f"Successfully processed {file_path.name}: {len(chunks)} chunks created")
            return result

        except Exception as e:
            logger.error(f"Failed to process file {file_path}: {e}")
            return {
                'file_path': str(file_path),
                'file_name': file_path.name,
                'error': str(e),
                'status': 'failed'
            }

    def process_batch(self, file_paths: List[Path], chunking_strategy: str = "character",
                     chunk_size: int = 235, chunk_overlap: int = 30,
                     max_workers: int = 4) -> List[Dict[str, Any]]:
        """
        Process multiple files in parallel

        Args:
            file_paths: List of file paths to process
            chunking_strategy: Strategy for chunking
            chunk_size: Size of each chunk (default 235 for CLIP limit)
            chunk_overlap: Overlap between chunks (default 30 chars)
            max_workers: Maximum number of parallel workers

        Returns:
            List of processing results
        """
        logger.info(f"Processing batch of {len(file_paths)} files")

        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_path = {
                executor.submit(self.process_file, file_path, chunking_strategy,
                              chunk_size, chunk_overlap): file_path
                for file_path in file_paths
            }

            for future in as_completed(future_to_path):
                file_path = future_to_path[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.error(f"Batch processing failed for {file_path}: {e}")
                    results.append({
                        'file_path': str(file_path),
                        'file_name': file_path.name,
                        'error': str(e),
                        'status': 'failed'
                    })

        results.sort(key=lambda x: file_paths.index(Path(x['file_path'])))

        successful = sum(1 for r in results if r.get('status') == 'success')
        logger.info(f"Batch processing completed: {successful}/{len(results)} files successful")

        return results

    def get_file_summary(self, file_path: Path) -> Dict[str, Any]:
        """
        Get summary information about a file without full processing

        Args:
            file_path: Path to the file

        Returns:
            Summary dictionary
        """
        try:
            processor = self.get_processor(file_path)
            if processor:
                return processor.get_summary(file_path)
            else:
                return {
                    'file_name': file_path.name,
                    'file_size': file_path.stat().st_size,
                    'can_process': False,
                    'error': 'Unsupported file type'
                }
        except Exception as e:
            logger.error(f"Failed to get summary for {file_path}: {e}")
            return {
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'can_process': False,
                'error': str(e)
            }

    def validate_files(self, file_paths: List[Path]) -> Dict[str, Any]:
        """
        Validate a list of files for processing

        Args:
            file_paths: List of file paths to validate

        Returns:
            Validation results
        """
        valid_files = []
        invalid_files = []

        for file_path in file_paths:
            if file_path.exists():
                processor = self.get_processor(file_path)
                if processor:
                    valid_files.append(str(file_path))
                else:
                    invalid_files.append({
                        'file_path': str(file_path),
                        'error': 'Unsupported file type'
                    })
            else:
                invalid_files.append({
                    'file_path': str(file_path),
                    'error': 'File does not exist'
                })

        return {
            'valid_files': valid_files,
            'invalid_files': invalid_files,
            'total_valid': len(valid_files),
            'total_invalid': len(invalid_files)
        }

    def _chunk_to_dict(self, chunk: dict) -> Dict[str, Any]:
        """Convert chunk object to dictionary (for compatibility)"""
        return dict(chunk) if isinstance(chunk, dict) else {}
FILE: app/ingestion/metadata/__init__.py
[EMPTY FILE]
FILE: app/ingestion/metadata/extractor.py
"""
Metadata extraction utilities
"""
import logging
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

class MetadataExtractor:
    """Extract metadata from files and content"""

    def __init__(self):
        pass

    def extract_file_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        Extract basic file metadata

        Args:
            file_path: Path to the file

        Returns:
            Dictionary with file metadata
        """
        try:
            stat = file_path.stat()

            metadata = {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'file_size': stat.st_size,
                'file_extension': file_path.suffix.lower(),
                'created_time': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'is_readable': True
            }

            return metadata

        except Exception as e:
            logger.error(f"Failed to extract file metadata for {file_path}: {e}")
            return {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'error': str(e),
                'is_readable': False
            }

    def extract_content_metadata(self, content: str, source_type: str) -> Dict[str, Any]:
        """
        Extract metadata from content

        Args:
            content: Text content
            source_type: Type of source (pdf, text, audio, etc.)

        Returns:
            Dictionary with content metadata
        """
        try:
            metadata = {
                'content_length': len(content),
                'word_count': len(content.split()),
                'line_count': len(content.splitlines()),
                'source_type': source_type,
                'has_content': bool(content.strip())
            }

            words = content.split()
            if words:
                avg_word_length = sum(len(word) for word in words) / len(words)
                metadata['avg_word_length'] = round(avg_word_length, 2)

            metadata['estimated_language'] = self._estimate_language(content)

            metadata['content_type'] = self._detect_content_type(content)

            return metadata

        except Exception as e:
            logger.error(f"Failed to extract content metadata: {e}")
            return {
                'content_length': len(content),
                'error': str(e)
            }

    def extract_processing_metadata(self, processing_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract metadata from processing results

        Args:
            processing_result: Result from ingestion processing

        Returns:
            Dictionary with processing metadata
        """
        try:
            metadata = {
                'processing_status': processing_result.get('status', 'unknown'),
                'processing_time': processing_result.get('processing_time'),
                'chunk_count': processing_result.get('total_chunks', 0),
                'chunking_strategy': processing_result.get('chunking_strategy', 'unknown'),
                'chunk_size': processing_result.get('chunk_size', 0),
                'chunk_overlap': processing_result.get('chunk_overlap', 0),
                'total_chars': processing_result.get('total_chars', 0)
            }

            if 'extraction_result' in processing_result:
                extraction = processing_result['extraction_result']
                if isinstance(extraction, list) and extraction:
                    first = extraction[0]
                    if 'page_number' in first:
                        metadata.update({
                            'extraction_modality': 'text',
                            'extraction_source_type': 'pdf'
                        })
                    elif 'start' in first and 'end' in first:
                        metadata.update({
                            'extraction_modality': 'audio',
                            'extraction_source_type': 'audio'
                        })
                    else:
                        metadata.update({
                            'extraction_modality': first.get('type'),
                            'extraction_source_type': first.get('source_type', 'image')
                        })
                elif isinstance(extraction, dict):
                    metadata.update({
                        'extraction_modality': extraction.get('modality'),
                        'extraction_source_type': extraction.get('source_type')
                    })

                if isinstance(extraction, dict) and extraction.get('modality') == 'text':
                    if extraction.get('source_type') == 'pdf':
                        metadata.update({
                            'pdf_pages': extraction.get('metadata', {}).get('pages', 0),
                            'pdf_title': extraction.get('metadata', {}).get('title')
                        })
                    elif extraction.get('source_type') == 'audio':
                        metadata.update({
                            'audio_duration': extraction.get('metadata', {}).get('duration', 0),
                            'audio_language': extraction.get('metadata', {}).get('language')
                        })
                elif isinstance(extraction, list) and extraction and 'start' in extraction[0]:
                    first_segment = extraction[0]
                    segment_metadata = first_segment.get('metadata', {})
                    metadata.update({
                        'audio_duration': segment_metadata.get('duration', 0),
                        'audio_language': segment_metadata.get('language', 'unknown')
                    })

            return metadata

        except Exception as e:
            logger.error(f"Failed to extract processing metadata: {e}")
            return {
                'processing_status': 'error',
                'error': str(e)
            }

    def _estimate_language(self, text: str) -> str:
        """Simple language estimation based on character patterns"""
        text = text.lower()

        english_chars = sum(1 for c in text if c in 'abcdefghijklmnopqrstuvwxyz ')
        english_ratio = english_chars / len(text) if text else 0

        cyrillic_chars = sum(1 for c in text if ord(c) in range(1024, 1279))
        chinese_chars = sum(1 for c in text if ord(c) in range(19968, 40959))

        if chinese_chars > len(text) * 0.3:
            return 'chinese'
        elif cyrillic_chars > len(text) * 0.3:
            return 'cyrillic'
        elif english_ratio > 0.8:
            return 'english'
        else:
            return 'unknown'

    def _detect_content_type(self, content: str) -> str:
        """Detect the type of content"""
        content_lower = content.lower()

        code_indicators = ['import ', 'function ', 'class ', 'def ', 'public ', 'private ', 'const ', 'let ']
        if any(indicator in content_lower for indicator in code_indicators):
            return 'code'

        if content.strip().startswith('{') and content.strip().endswith('}'):
            try:
                import json
                json.loads(content)
                return 'json'
            except:
                pass

        lines = content.split('\n')[:5]
        if len(lines) > 1:
            commas = [line.count(',') for line in lines if line.strip()]
            if commas and all(count == commas[0] for count in commas) and commas[0] > 0:
                return 'csv'

        if any(line.strip().startswith(('
            return 'markdown'

        return 'text'

    def combine_metadata(self, *metadata_dicts: Dict[str, Any]) -> Dict[str, Any]:
        """
        Combine multiple metadata dictionaries

        Args:
            *metadata_dicts: Metadata dictionaries to combine

        Returns:
            Combined metadata dictionary
        """
        combined = {}

        for metadata in metadata_dicts:
            if metadata:
                combined.update(metadata)

        combined['combined_at'] = datetime.now().isoformat()

        return combined
FILE: app/ingestion/metadata/validator.py
[EMPTY FILE]
FILE: app/ingestion/orchestrator.py
"""
Ingestion Orchestrator - Connects ingestion to vector storage

==============================================================================
SYSTEM PROTOCOL: HYBRID METADATA ARCHITECTURE
==============================================================================

OBJECTIVE:
Achieve 100% extraction reliability for 'Document Topic' and 'Key Concepts'
without sacrificing semantic depth.

THE PROBLEM:
- LLMs (Tier 1) provide high-quality, nuanced labels but are prone to
  timeouts and 'json_decode' errors on large files.
- NLP Frequency (Tier 2) is 100% reliable and fast but lacks semantic
  understanding (e.g., might miss that 'Einstein' implies 'Physics').
- Relying on just one leads to either "Unknown" labels or slow ingestion.

THE SOLUTION (HYBRID PIPELINE):
The system must execute a Failover Workflow:

1. **Attempt Tier 1 (LLM Analysis):**
   Send the first 3000 tokens to Llama 3.1. Request a JSON summary.

2. **Catch Failure:**
   If the LLM times out, returns invalid JSON, or hallucinates, CATCH the
   error. Do not stop.

3. **Execute Tier 2 (NLP Fallback):**
   Immediately run the `NLPProcessor` on the full text. Derive the topic
   from bigram frequency (e.g., "Kinetic Energy") and concepts from noun
   density.

4. **Unified Tagging:**
   Apply the *winner's* metadata to ALL extracted chunks (Text AND Images)
   to ensure the Visual Lane remains searchable.

RESULT:
- Best case: Deep semantic labels from LLM.
- Worst case: Accurate frequency-based labels from NLP.
- Never "Unknown".

ARCHITECTURAL NOTE: DUAL-LANE INGESTION
========================================
This orchestrator implements a Fork-Join pattern for PDF/DOCX files:

Lane A (Text): Extract text â†’ Chunk â†’ NLP/LLM Topic Extraction â†’ Text Embedder
Lane B (Visual): Extract images â†’ Filter noise (>200x200px) â†’ CLIP Vision Embedder

Both lanes merge with shared metadata:
- document_topic (from Lane A topic extraction)
- document_concepts (from Lane A concept extraction)
- session_id (for isolation)

This solves the "Textbook Problem": A physics textbook explains "Gravity" in
text, but the diagram of the falling apple explains the vectors. Without
visual extraction, you lose half the information.

==============================================================================
"""
import logging
import re
from pathlib import Path
from typing import List, Dict, Any
import uuid

from app.ingestion.ingestion_service import IngestionService
from app.storage.vector_store import VectorStore
from app.embeddings.manager import EmbeddingsManager
from app.reasoning.llm.llama_reasoner import LlamaReasoner
from app.config import settings

logger = logging.getLogger(__name__)

class IngestionOrchestrator:
    """Orchestrates end-to-end ingestion pipeline with hybrid metadata extraction"""

    def __init__(self):
        self.ingestion_service = IngestionService()
        self.vector_store = VectorStore()
        self.embeddings_manager = EmbeddingsManager()
        self.llama_reasoner = LlamaReasoner()
        logger.info("IngestionOrchestrator initialized")

    def ingest_and_store(self, file_path: Path,
                        session_id: str = "default",
                        chunking_strategy: str = "character",
                        chunk_size: int = 235,
                        chunk_overlap: int = 30,
                        deduplicate: bool = True) -> Dict[str, Any]:
        """
        Complete ingestion pipeline: process file -> generate embeddings -> store

        Args:
            file_path: Path to file to ingest
            session_id: Session ID for tracking (default "default")
            chunking_strategy: Chunking strategy
            chunk_size: Chunk size (default 235 for CLIP limit)
            chunk_overlap: Chunk overlap (default 30 chars)
            deduplicate: If True, delete existing chunks from this file before re-ingesting (default True)

        Returns:
            Ingestion result dictionary
        """
        try:
            logger.info(f"Starting ingestion pipeline for: {file_path}")

            deleted_count = 0
            if deduplicate:
                file_path_str = str(file_path)
                logger.info(f"[DEDUP] Checking for existing chunks from: {file_path_str}")
                try:
                    deleted_count = self.vector_store.delete_by_source(file_path_str)
                    if deleted_count > 0:
                        logger.info(f"[DEDUP] Deleted {deleted_count} old chunks to prevent duplicates")
                except Exception as e:
                    logger.warning(f"Failed to delete old chunks (continuing anyway): {e}")

            processing_result = self.ingestion_service.process_file(
                file_path,
                chunking_strategy=chunking_strategy,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )

            if processing_result.get('status') != 'success':
                return processing_result

            chunks = processing_result.get('chunks', [])

            if not chunks:
                logger.warning(f"No chunks generated for {file_path}")
                return {
                    **processing_result,
                    'vector_store_status': 'skipped',
                    'stored_chunks': 0
                }

            document_topic, document_concepts = self._extract_document_knowledge(chunks, file_path)

            logger.info(f"ðŸ“Š [METADATA APPLIED] All chunks tagged with:")
            logger.info(f"   ðŸ·ï¸  Topic: '{document_topic}'")
            logger.info(f"   ðŸ”‘ Concepts: {document_concepts[:5]}{'...' if len(document_concepts) > 5 else ''}")

            prepared_chunks = self._prepare_chunks_for_storage(
                chunks,
                file_path,
                processing_result,
                document_topic,
                document_concepts
            )

            text_chunks = sum(1 for c in prepared_chunks if c.get('modality') == 'text')
            image_chunks = sum(1 for c in prepared_chunks if c.get('modality') == 'image')

            logger.info(f"ðŸ“¦ [DUAL-LANE SUMMARY] Prepared {len(prepared_chunks)} total chunks:")
            logger.info(f"   ðŸ“ Text Lane: {text_chunks} chunks")
            logger.info(f"   ðŸ–¼ï¸  Visual Lane: {image_chunks} chunks")
            logger.info(f"   âœ… All inherit topic: '{document_topic}'")

            storage_result = self.vector_store.add_documents(prepared_chunks, session_id=session_id)

            logger.info(f"âœ… [INGESTION COMPLETE] {storage_result.get('count', 0)} chunks stored in vector DB")

            return {
                **processing_result,
                'vector_store_status': storage_result.get('status'),
                'stored_chunks': storage_result.get('count', 0),
                'collection': storage_result.get('collection'),
                'deleted_old_chunks': deleted_count
            }

        except Exception as e:
            logger.error(f"Ingestion pipeline failed for {file_path}: {e}")
            return {
                'file_path': str(file_path),
                'status': 'failed',
                'error': str(e)
            }

    def _prepare_chunks_for_storage(self, chunks: List[Dict],
                                   file_path: Path,
                                   processing_result: Dict[str, Any],
                                   document_topic: str = 'Unknown',
                                   document_concepts: List[str] = None) -> List[Dict[str, Any]]:
        """Prepare chunks with embeddings for vector storage"""
        if document_concepts is None:
            document_concepts = []
        prepared = []

        for i, chunk in enumerate(chunks):
            chunk_id = chunk.get('chunk_id') or str(uuid.uuid4())

            prepared_chunk = {
                'chunk_id': chunk_id,
                'content': chunk.get('content', chunk.get('text', '')),
                'source_file': str(file_path),
                'modality': chunk.get('modality', 'text'),
                'source_type': chunk.get('source_type', 'text'),
                'chunk_index': chunk.get('chunk_index', i),
                'total_chunks': chunk.get('total_chunks', len(chunks)),
                'prev_chunk_id': chunk.get('prev_chunk_id'),
                'next_chunk_id': chunk.get('next_chunk_id'),
                'document_topic': document_topic,
                'document_concepts': document_concepts,
                'metadata': {
                    'file_name': file_path.name,
                    'file_size': processing_result.get('file_size', 0),
                    'processor_type': processing_result.get('processor_type', 'unknown'),
                    'chunking_strategy': processing_result.get('chunking_strategy', 'unknown'),
                    **chunk.get('metadata', {})
                }
            }

            if 'embedding' in chunk:
                prepared_chunk['embedding'] = chunk['embedding']
            else:
                content = prepared_chunk['content']
                if content and content != '[IMAGE_VISUAL_EMBEDDING]':
                    if len(content) > 235:
                        logger.error(f"CRITICAL: Chunk {i} exceeds CLIP limit ({len(content)} chars) - this indicates chunking failure!")
                        continue

                    try:
                        prepared_chunk['embedding'] = self.embeddings_manager.embed_text(content)
                    except Exception as e:
                        logger.error(f"Failed to embed chunk {i}: {e}")
                        continue
                else:
                    logger.warning(f"Skipping chunk {i}: no content or embedding")
                    continue

            prepared.append(prepared_chunk)

        return prepared

    def _extract_document_knowledge(self, chunks: List[Dict], file_path: Path) -> tuple:
        """
        Extract topic and concepts using HYBRID 4-tier fallback system:

        TIER 1: LLM extraction (BEST QUALITY - Deep semantic understanding)
        TIER 2: NLP frequency analysis (HIGH RELIABILITY - Fast, deterministic)
        TIER 3: Filename parsing (ACCEPTABLE QUALITY - Always available)
        TIER 4: Generic fallback (LAST RESORT - Never returns "Unknown")

        This implements the HYBRID METADATA ARCHITECTURE protocol documented
        at the top of this file. The goal is 100% reliability without
        sacrificing quality.

        EDGE CASE HANDLING: Scanned PDFs with only images (no text layer)
        - Collects OCR text from image chunks to ensure metadata extraction works
        - Never fails on image-only documents

        Args:
            chunks: List of document chunks
            file_path: Path to the source file

        Returns:
            Tuple of (document_topic, document_concepts)
        """
        from app.utils.topic_utils import clean_llm_topic_response

        text_parts = []
        for c in chunks[:10]:
            content = c.get('content', c.get('text', ''))

            if content and content != '[IMAGE_VISUAL_EMBEDDING]':
                text_parts.append(content)

        sample_text = " ".join(text_parts)[:3000]

        if not sample_text.strip():
            logger.warning(f"[EXTRACTION] No text content found (image-only PDF)")
            logger.info(f"[EXTRACTION] Relying on CLIP visual embeddings for retrieval")

            document_topic = "Visual Content"
            document_concepts = []

            logger.info(f"   ðŸ–¼ï¸  [IMAGE-ONLY MODE] Topic: '{document_topic}' | Vector-based retrieval enabled")
            return document_topic, document_concepts

        logger.info(f"ðŸ§  [HYBRID EXTRACTION] Starting metadata extraction for {file_path.name}")

        document_topic = None
        document_concepts = []

        try:
            logger.info("   ðŸ”¬ [TIER 1: LLM] Attempting semantic extraction...")
            document_topic, document_concepts = self._llm_extract_knowledge(sample_text, file_path)

            if document_topic and len(document_topic) >= 3 and document_concepts:
                logger.info(f"   âœ… [TIER 1: LLM SUCCESS] Topic: '{document_topic}' | Concepts: {len(document_concepts)}")
                return document_topic, document_concepts[:20]
            else:
                raise ValueError(f"LLM returned incomplete data: topic={document_topic}, concepts={len(document_concepts)}")

        except Exception as e:
            logger.warning(f"   âš ï¸ [TIER 1: LLM FAILED] {str(e)[:100]} - Switching to Tier 2...")

        try:
            logger.info("   ðŸ“Š [TIER 2: NLP] Running frequency analysis on full text...")

            if len(sample_text.strip()) < 20:
                raise ValueError(f"Insufficient text for NLP analysis ({len(sample_text)} chars)")

            if not document_topic or len(document_topic) < 3:
                document_topic = self._nlp_extract_topic(sample_text)
                logger.info(f"   ðŸ“ [TIER 2: NLP] Extracted topic: '{document_topic}'")

            if not document_concepts:
                document_concepts = self._nlp_extract_concepts(sample_text)
                logger.info(f"   ðŸ“ [TIER 2: NLP] Extracted {len(document_concepts)} concepts")

            if document_topic and document_concepts:
                logger.info(f"   âœ… [TIER 2: NLP SUCCESS] Proceeding with extracted metadata")
                return document_topic, document_concepts[:20]
            else:
                raise ValueError(f"NLP returned incomplete data: topic={document_topic}, concepts={len(document_concepts)}")

        except Exception as e:
            logger.warning(f"   âš ï¸ [TIER 2: NLP FAILED] {str(e)[:100]} - Switching to Tier 3...")

        if not document_topic:
            document_topic = self._fallback_topic_from_filename(file_path)
            logger.info(f"   ðŸ“‚ [TIER 3: FILENAME] Using topic: '{document_topic}'")

        if not document_concepts:
            from app.utils.topic_utils import extract_concepts_from_text
            document_concepts = extract_concepts_from_text(sample_text, max_concepts=15)
            logger.info(f"   ðŸ” [TIER 4: BASIC] Extracted {len(document_concepts)} concepts from text")

        logger.info(f"   âœ… [HYBRID EXTRACTION COMPLETE] Final: '{document_topic}' with {len(document_concepts)} concepts")
        return document_topic, document_concepts[:20]

    def _llm_extract_knowledge(self, text: str, file_path: Path) -> tuple:
        """Tier 1: LLM-based extraction with strict formatting"""
        knowledge_prompt = f"""Extract the main academic topic and key technical concepts from this document.

IMPORTANT RULES:
- Topic: Use 2-4 words describing the academic field (e.g., "Cell Biology", "Quantum Mechanics", "Machine Learning")
- Concepts: List 10-15 domain-specific technical nouns ONLY
- NO generic words like: process, system, both, inside, outside, have, that, needed
- Focus on scientific/technical terminology unique to the subject

Format your response EXACTLY like this:
Topic: [Your topic here]
Concepts: concept1, concept2, concept3, concept4, concept5

Document text:
{text}

Your extraction:"""

        response = self.llama_reasoner.generate(
            prompt=knowledge_prompt,
            max_tokens=100,
            temperature=0.0,
            stop_sequences=["\n\n", "Document:", "Text:"]
        )

        logger.debug(f"[LLM] Raw response: {response[:200]}")

        topic = None
        concepts = []

        for line in response.strip().split('\n'):
            line = line.strip()
            if line.lower().startswith('topic:'):
                from app.utils.topic_utils import clean_llm_topic_response
                raw_topic = line.split(':', 1)[1].strip()
                topic = clean_llm_topic_response(raw_topic)
            elif line.lower().startswith('concepts:'):
                concepts_str = line.split(':', 1)[1].strip()
                concepts = [c.strip().lower() for c in concepts_str.split(',') if c.strip() and len(c.strip()) > 2]

        if not topic or not concepts:
            raise ValueError("LLM returned incomplete extraction")

        return topic, concepts

    def _nlp_extract_topic(self, text: str) -> str:
        """
        Tier 2: Extract topic using NLP frequency analysis (100% reliable)

        This method NEVER returns "Unknown" - it always finds something.
        Implements bigram frequency analysis as specified in the hybrid protocol.
        """
        import re
        from collections import Counter

        capitalized_phrases = re.findall(r'\b[A-Z][a-z]+(?: [A-Z][a-z]+)*\b', text)

        if capitalized_phrases:
            phrase_counts = Counter(capitalized_phrases)
            for phrase, count in phrase_counts.most_common(15):
                word_count = len(phrase.split())
                if 2 <= word_count <= 4 and count >= 2:
                    logger.debug(f"[NLP] Found topic via bigram frequency: '{phrase}' (count={count})")
                    return phrase

        academic_keywords = {
            'biology': 'Biology', 'physics': 'Physics', 'chemistry': 'Chemistry',
            'mathematics': 'Mathematics', 'statistics': 'Statistics',
            'photosynthesis': 'Plant Biology', 'cell': 'Cell Biology',
            'gravity': 'Physics', 'force': 'Mechanics', 'atom': 'Chemistry',
            'neuron': 'Neuroscience', 'molecule': 'Molecular Biology',
            'ecosystem': 'Ecology', 'evolution': 'Evolutionary Biology',

            'history': 'History', 'geography': 'Geography', 'economics': 'Economics',
            'psychology': 'Psychology', 'sociology': 'Sociology',

            'algorithm': 'Computer Science', 'code': 'Programming',
            'software': 'Software Engineering', 'database': 'Data Management',
            'network': 'Networking', 'security': 'Cybersecurity',
            'machine learning': 'Machine Learning', 'artificial intelligence': 'AI',

            'calculus': 'Calculus', 'algebra': 'Algebra', 'geometry': 'Geometry',
            'trigonometry': 'Trigonometry', 'probability': 'Probability Theory'
        }

        text_lower = text.lower()
        for keyword, topic in academic_keywords.items():
            if keyword in text_lower:
                logger.debug(f"[NLP] Matched keyword '{keyword}' â†’ topic '{topic}'")
                return topic

        if capitalized_phrases:
            most_common = phrase_counts.most_common(1)[0][0]
            logger.debug(f"[NLP] Using most frequent capitalized phrase: '{most_common}'")
            return most_common

        words = re.findall(r'\b[A-Z][a-z]{3,}\b', text)
        if words:
            logger.debug(f"[NLP] Using first capitalized word: '{words[0]}'")
            return words[0]

        logger.warning(f"[NLP] All strategies failed - using 'General Document' as topic")
        return "General Document"

    def _nlp_extract_concepts(self, text: str) -> list:
        """
        Tier 2: Extract concepts using noun density analysis (100% reliable)

        This method implements the NLP Processor concept extraction from the
        hybrid protocol. It uses TF-IDF style frequency analysis to identify
        domain-specific technical terms.

        NEVER returns empty list - guarantees at least some concepts.
        """
        import re
        from collections import Counter

        words = re.findall(r'\b[a-z]{4,}\b', text.lower())

        stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'this', 'that', 'these', 'those', 'from', 'into', 'through', 'during', 'before',
            'after', 'above', 'below', 'between', 'under', 'again', 'further', 'then', 'once',
            'here', 'there', 'when', 'where', 'why', 'how', 'all', 'both', 'each', 'few', 'more',
            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'than', 'too', 'very',

            'can', 'will', 'just', 'should', 'now', 'have', 'has', 'had', 'having', 'been',
            'being', 'are', 'was', 'were', 'is', 'am', 'also', 'does', 'did', 'doing',
            'would', 'could', 'ought', 'make', 'made', 'been', 'uses', 'used', 'using',

            'they', 'them', 'their', 'what', 'which', 'who', 'whom', 'whose',

            'about', 'against', 'since', 'until', 'while', 'across', 'along',

            'process', 'system', 'function', 'inside', 'outside', 'provided', 'needed',
            'always', 'observed', 'cause', 'change', 'learnt', 'object', 'objects',
            'example', 'examples', 'result', 'results', 'study', 'studies', 'research',
            'method', 'methods', 'called', 'known', 'found', 'shown', 'given',
            'within', 'without', 'around', 'many', 'several', 'various', 'different',
            'important', 'main', 'based', 'shown', 'described', 'discussed',

            'chapter', 'section', 'figure', 'table', 'page', 'following', 'previous'
        }

        word_counts = Counter(w for w in words if w not in stopwords and len(w) >= 4)

        concepts = [word for word, count in word_counts.most_common(40) if count >= 2]

        if len(concepts) >= 5:
            logger.debug(f"[NLP] Extracted {len(concepts)} high-frequency concepts: {concepts[:10]}...")
            return concepts[:20]

        if len(concepts) >= 2:
            additional = [word for word, count in word_counts.most_common(20) if word not in concepts]
            concepts.extend(additional[:10])
            logger.debug(f"[NLP] Extended to {len(concepts)} concepts including single occurrences")
            return concepts[:15]

        fallback_concepts = [word for word, _ in word_counts.most_common(10)]

        if fallback_concepts:
            logger.debug(f"[NLP] Using fallback: {len(fallback_concepts)} most frequent words")
            return fallback_concepts

        capitalized = re.findall(r'\b[A-Z][a-z]{3,}\b', text)
        if capitalized:
            unique_caps = list(set(capitalized[:10]))
            logger.warning(f"[NLP] Ultimate fallback: using capitalized words as concepts: {unique_caps}")
            return [w.lower() for w in unique_caps]

        logger.warning(f"[NLP] No concepts found - returning generic placeholders")
        return ['document', 'content', 'information']

        raise ValueError("No concepts found via NLP")

    def _fallback_topic_from_filename(self, file_path: Path) -> str:
        """Extract topic from filename as last resort"""
        name = file_path.stem
        name = re.sub(r'[_\-\d]+', ' ', name)
        words = name.split()[:3]
        topic = ' '.join(words).title()
        logger.info(f"[FALLBACK] Using filename-based topic: '{topic}'")
        return topic if topic else "General"

    def ingest_batch(self, file_paths: List[Path],
                    chunking_strategy: str = "character",
                    chunk_size: int = 235,
                    chunk_overlap: int = 30) -> List[Dict[str, Any]]:
        """Ingest multiple files"""
        results = []

        for file_path in file_paths:
            result = self.ingest_and_store(
                file_path,
                chunking_strategy,
                chunk_size,
                chunk_overlap
            )
            results.append(result)

        successful = sum(1 for r in results if r.get('status') == 'success')
        logger.info(f"Batch ingestion complete: {successful}/{len(results)} files successful")

        return results
FILE: app/ingestion/processors/__init__.py
[EMPTY FILE]
FILE: app/ingestion/processors/audio_processor.py
from app.embeddings.models.multimodal_embedder import MultimodalEmbedder
from app.ingestion.chunking.text_chunker import micro_chunk_text
import re
"""
Audio processor using faster-whisper for transcription (C++-powered, CTranslate2)
"""
import logging
from pathlib import Path
from typing import List, Dict, Any

from faster_whisper import WhisperModel

from app.config import settings

logger = logging.getLogger(__name__)

class AudioProcessor:
    """Processor for audio files using faster-whisper (CTranslate2 engine)"""

    def __init__(self):
        self.supported_extensions = ['.mp3', '.wav', '.flac', '.m4a', '.aac', '.ogg']
        self.model = None
        self.model_name = getattr(settings, 'WHISPER_MODEL', 'base')
        self.device = "cpu"
        self.compute_type = "int8"

    def _load_model(self):
        """Load faster-whisper model if not already loaded"""
        if self.model is None:
            logger.info(f"Loading faster-whisper model: {self.model_name}")
            logger.info(f"Device: {self.device}, Compute type: {self.compute_type}")

            self.model = WhisperModel(
                self.model_name,
                device=self.device,
                compute_type=self.compute_type,
                download_root=str(settings.models_dir / "whisper")
            )

            logger.info("faster-whisper model loaded successfully (CTranslate2 engine)")
            logger.info(f"[faster-whisper] Running on CPU with int8 precision for VRAM optimization")

    def can_process(self, file_path: Path) -> bool:
        """Check if the processor can handle the file"""
        return file_path.suffix.lower() in self.supported_extensions

    def extract_text(self, file_path: Path):
        """
        Transcribe audio file using faster-whisper, aggregate full transcript, chunk properly, embed each chunk.
        Returns: List[Dict] with chunked text, timestamps, embeddings, and complete metadata.
        """
        try:
            logger.info(f"Processing audio: {file_path}")
            self._load_model()
            audio_path = file_path.resolve()
            if not audio_path.exists():
                raise FileNotFoundError(f"Audio file not found: {audio_path}")

            segments_iter, info = self.model.transcribe(
                str(audio_path),
                beam_size=5,
                language=None
            )

            segments = list(segments_iter)

            metadata = {
                'duration': info.duration,
                'language': info.language,
                'file_size': file_path.stat().st_size,
            }

            full_transcript = ' '.join(seg.text.strip() for seg in segments if seg.text.strip())
            transcript_length = len(full_transcript)

            logger.info(f"Full transcript length: {transcript_length} chars")

            if transcript_length <= 235:
                embedder = MultimodalEmbedder()
                embedding = embedder.embed(full_transcript)
                processed_segments = [{
                    'text': full_transcript,
                    'start': float(segments[0].start) if segments else 0.0,
                    'end': float(segments[-1].end) if segments else metadata['duration'],
                    'embedding': embedding,
                    'metadata': metadata,
                    'file_path': str(file_path),
                    'chunk_index': 0,
                    'total_chunks': 1,
                    'prev_chunk_id': None,
                    'next_chunk_id': None
                }]
                logger.info(f"Audio processed: 1 chunk (no splitting needed)")
            else:
                logger.info(f"Transcript exceeds embedding limit â€” splitting into chunks")
                chunks = micro_chunk_text(full_transcript, chunk_size=235, chunk_overlap=30)

                embedder = MultimodalEmbedder()
                processed_segments = []

                total_duration = metadata['duration']
                chars_per_second = transcript_length / total_duration if total_duration > 0 else 0

                cumulative_chars = 0
                for chunk_data in chunks:
                    chunk_text = chunk_data['chunk']
                    chunk_length = len(chunk_text)

                    start_time = cumulative_chars / chars_per_second if chars_per_second > 0 else 0
                    end_time = (cumulative_chars + chunk_length) / chars_per_second if chars_per_second > 0 else total_duration

                    embedding = embedder.embed(chunk_text)

                    processed_segments.append({
                        'text': chunk_text,
                        'start': float(start_time),
                        'end': float(end_time),
                        'embedding': embedding,
                        'metadata': metadata,
                        'file_path': str(file_path),
                        'chunk_id': chunk_data['id'],
                        'prev_chunk_id': chunk_data['prev_chunk_id'],
                        'next_chunk_id': chunk_data['next_chunk_id']
                    })

                    cumulative_chars += chunk_length

                logger.info(f"Audio processed: {len(processed_segments)} chunks stored")

            return processed_segments

        except Exception as e:
            logger.error(f"Failed to process audio {file_path}: {e}")
            raise Exception(f"Audio processing failed: {str(e)}")

    def extract_segments(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        Extract individual segments as separate chunks

        Args:
            file_path: Path to the audio file

        Returns:
            List of segment dictionaries
        """
        result = self.extract_text(file_path)
        segments = result['segments']

        for segment in segments:
            segment.update({
                'source_file': str(file_path),
                'modality': 'text',
                'source_type': 'audio',
                'metadata': result['metadata']
            })

        return segments

    def get_summary(self, file_path: Path) -> Dict[str, Any]:
        """
        Get a summary of the audio file without full transcription

        Args:
            file_path: Path to the audio file

        Returns:
            Summary dictionary
        """
        try:
            file_size = file_path.stat().st_size

            self._load_model()
            audio_path = file_path.resolve()

            _, info = self.model.transcribe(
                str(audio_path),
                beam_size=1,
                language=None
            )

            return {
                'file_name': file_path.name,
                'file_size': file_size,
                'duration': info.duration,
                'can_process': True
            }

        except Exception as e:
            logger.error(f"Failed to get audio summary: {e}")
            return {
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'can_process': False,
                'error': str(e)
            }
FILE: app/ingestion/processors/base.py
[EMPTY FILE]
FILE: app/ingestion/processors/image_processor.py
from app.embeddings.models.multimodal_embedder import MultimodalEmbedder
from app.ingestion.chunking.text_chunker import micro_chunk_text
"""
Image processor with OCR capabilities using Tesseract
"""
import logging
from pathlib import Path
from typing import List, Dict, Any

import pytesseract
from PIL import Image
import cv2
import numpy as np

from app.config import settings

logger = logging.getLogger(__name__)

class ImageProcessor:
    """Processor for image files with OCR"""

    def __init__(self):
        self.supported_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif']

        if hasattr(settings, 'TESSERACT_PATH') and settings.TESSERACT_PATH:
            pytesseract.pytesseract.tesseract_cmd = settings.TESSERACT_PATH

    def can_process(self, file_path: Path) -> bool:
        """Check if the processor can handle the file"""
        return file_path.suffix.lower() in self.supported_extensions

    def preprocess_image(self, image: Image.Image) -> Image.Image:
        """
        Preprocess image for better OCR results

        Args:
            image: PIL Image object

        Returns:
            Preprocessed PIL Image
        """
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)

        gray = cv2.medianBlur(gray, 3)

        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        processed_image = Image.fromarray(thresh)

        return processed_image

    def extract_text(self, file_path: Path) -> Dict[str, Any]:
        """
        Dual-vector extraction for image: returns two entries (visual, OCR if confidence >70%).
        Each entry contains embedding, OCR text, resolution, and file path.
        Returns: List[Dict] with 'type' ('visual' or 'ocr'), 'embedding', 'ocr_text', 'ocr_confidence', 'metadata', 'file_path'.
        """
        try:
            logger.info(f"Processing image: {file_path}")
            image = Image.open(file_path)
            metadata = {
                'width': image.width,
                'height': image.height,
                'mode': image.mode,
                'format': image.format,
                'file_size': file_path.stat().st_size,
            }
            processed_image = self.preprocess_image(image)
            ocr_text = pytesseract.image_to_string(processed_image).strip()
            confidence_data = pytesseract.image_to_data(processed_image, output_type=pytesseract.Output.DICT)
            confidences = [int(conf) for conf in confidence_data['conf'] if conf != '-1']
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0

            embedder = MultimodalEmbedder()
            results = []

            visual_embedding = embedder.embed(image)
            results.append({
                'type': 'visual',
                'embedding': visual_embedding,
                'ocr_text': ocr_text,
                'ocr_confidence': avg_confidence,
                'metadata': metadata,
                'file_path': str(file_path),
                'chunk_index': 0,
                'total_chunks': 1
            })

            if avg_confidence > 70 and ocr_text:
                ocr_length = len(ocr_text)

                if ocr_length <= 235:
                    ocr_embedding = embedder.embed(ocr_text)
                    results.append({
                        'type': 'ocr',
                        'embedding': ocr_embedding,
                        'ocr_text': ocr_text,
                        'ocr_confidence': avg_confidence,
                        'metadata': metadata,
                        'file_path': str(file_path),
                        'chunk_index': 0,
                        'total_chunks': 1,
                        'prev_chunk_id': None,
                        'next_chunk_id': None
                    })
                    logger.info(f"Image processed: visual + 1 OCR chunk, confidence: {avg_confidence:.2f}%")
                else:
                    logger.info(f"OCR text exceeds limit ({ocr_length} chars) â€” splitting into chunks")
                    ocr_chunks = micro_chunk_text(ocr_text, chunk_size=235, chunk_overlap=30)

                    for i, chunk_data in enumerate(ocr_chunks):
                        ocr_embedding = embedder.embed(chunk_data['chunk'])
                        results.append({
                            'type': 'ocr',
                            'embedding': ocr_embedding,
                            'ocr_text': chunk_data['chunk'],
                            'ocr_confidence': avg_confidence,
                            'metadata': metadata,
                            'file_path': str(file_path),
                            'chunk_index': i,
                            'total_chunks': len(ocr_chunks),
                            'chunk_id': chunk_data['id'],
                            'prev_chunk_id': chunk_data['prev_chunk_id'],
                            'next_chunk_id': chunk_data['next_chunk_id']
                        })
                    logger.info(f"Image processed: visual + {len(ocr_chunks)} OCR chunks, confidence: {avg_confidence:.2f}%")
            else:
                logger.info(f"Image processed: visual only (OCR confidence: {avg_confidence:.2f}%)")

            return results
        except Exception as e:
            logger.error(f"Failed to process image {file_path}: {e}")
            raise Exception(f"Image processing failed: {str(e)}")

    def extract_regions(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        Extract text from different regions of the image

        Args:
            file_path: Path to the image file

        Returns:
            List of region dictionaries with text and coordinates
        """
        try:
            image = Image.open(file_path)
            processed_image = self.preprocess_image(image)

            data = pytesseract.image_to_data(processed_image, output_type=pytesseract.Output.DICT)

            regions = []
            n_boxes = len(data['text'])

            for i in range(n_boxes):
                if int(data['conf'][i]) > 60:
                    region = {
                        'text': data['text'][i],
                        'confidence': int(data['conf'][i]),
                        'x': data['left'][i],
                        'y': data['top'][i],
                        'width': data['width'][i],
                        'height': data['height'][i],
                        'source_file': str(file_path),
                        'modality': 'text',
                        'source_type': 'image'
                    }
                    regions.append(region)

            return regions

        except Exception as e:
            logger.error(f"Failed to extract regions from image {file_path}: {e}")
            return []

    def get_summary(self, file_path: Path) -> Dict[str, Any]:
        """
        Get a summary of the image without full OCR

        Args:
            file_path: Path to the image file

        Returns:
            Summary dictionary
        """
        try:
            image = Image.open(file_path)

            return {
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'width': image.width,
                'height': image.height,
                'mode': image.mode,
                'format': image.format,
                'can_process': True
            }

        except Exception as e:
            logger.error(f"Failed to get image summary: {e}")
            return {
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'can_process': False,
                'error': str(e)
            }
FILE: app/ingestion/processors/ocr_processor.py
[EMPTY FILE]
FILE: app/ingestion/processors/pdf_processor.py
import fitz
from app.embeddings.models.multimodal_embedder import MultimodalEmbedder
from app.ingestion.chunking.text_chunker import micro_chunk_text
"""
PDF document processor for text and image extraction
"""
import logging
from pathlib import Path
from typing import List, Dict, Any
import io
import pytesseract

import PyPDF2
from PyPDF2 import PdfReader
from PIL import Image

from app.config import settings

logger = logging.getLogger(__name__)

class PDFProcessor:
    """Processor for PDF documents with text and image extraction"""

    def __init__(self):
        self.supported_extensions = ['.pdf']

        if hasattr(settings, 'TESSERACT_PATH') and settings.TESSERACT_PATH:
            pytesseract.pytesseract.tesseract_cmd = settings.TESSERACT_PATH

    def can_process(self, file_path: Path) -> bool:
        """Check if the processor can handle the file"""
        return file_path.suffix.lower() in self.supported_extensions

    def extract_text(self, file_path: Path):
        """
        Extract both text and images from PDF pages using PyMuPDF.
        Returns a list of chunks with embeddings (text chunks and image chunks).
        """
        try:
            logger.info(f"Processing PDF: {file_path}")
            doc = fitz.open(str(file_path))
            metadata = {
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'pages': doc.page_count,
            }
            embedder = MultimodalEmbedder()
            chunks = []
            chunk_index = 0

            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)

                text = page.get_text().strip()
                if text:
                    text_length = len(text)
                    if text_length <= 235:
                        text_embedding = embedder.embed(text)
                        chunks.append({
                            'type': 'text',
                            'embedding': text_embedding,
                            'text': text,
                            'content': text,
                            'modality': 'text',
                            'source_type': 'pdf',
                            'metadata': {
                                **metadata,
                                'page_number': page_num + 1,
                            },
                            'file_path': str(file_path),
                            'chunk_index': chunk_index,
                            'total_chunks': 1
                        })
                        chunk_index += 1
                    else:
                        logger.info(f"Page {page_num + 1} text exceeds limit ({text_length} chars) â€” splitting")
                        text_chunks = micro_chunk_text(text, chunk_size=235, chunk_overlap=30)

                        for i, chunk_data in enumerate(text_chunks):
                            text_embedding = embedder.embed(chunk_data['chunk'])
                            chunks.append({
                                'type': 'text',
                                'embedding': text_embedding,
                                'text': chunk_data['chunk'],
                                'content': chunk_data['chunk'],
                                'modality': 'text',
                                'source_type': 'pdf',
                                'metadata': {
                                    **metadata,
                                    'page_number': page_num + 1,
                                },
                                'file_path': str(file_path),
                                'chunk_index': chunk_index,
                                'total_chunks': 1,
                                'chunk_id': chunk_data['id'],
                                'prev_chunk_id': chunk_data['prev_chunk_id'],
                                'next_chunk_id': chunk_data['next_chunk_id']
                            })
                            chunk_index += 1

                image_list = page.get_images(full=True)
                if image_list:
                    logger.info(f"Found {len(image_list)} images on page {page_num + 1}")

                    for img_index, img_info in enumerate(image_list):
                        try:
                            xref = img_info[0]
                            base_image = doc.extract_image(xref)
                            image_bytes = base_image["image"]
                            image_ext = base_image["ext"]

                            image = Image.open(io.BytesIO(image_bytes))

                            if image.width < 200 or image.height < 200:
                                logger.debug(f"Skipping small image {img_index} on page {page_num + 1} "
                                           f"({image.width}x{image.height}px - likely logo/icon)")
                                continue

                            logger.info(f"Processing image {img_index} on page {page_num + 1} "
                                       f"({image.width}x{image.height}px)")

                            visual_embedding = embedder.embed(image)

                            ocr_text = ""
                            ocr_confidence = 0
                            try:
                                ocr_text = pytesseract.image_to_string(image).strip()
                                confidence_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
                                confidences = [int(conf) for conf in confidence_data['conf'] if conf != '-1']
                                ocr_confidence = sum(confidences) / len(confidences) if confidences else 0
                            except Exception as ocr_error:
                                logger.warning(f"OCR failed for image {img_index} on page {page_num + 1}: {ocr_error}")

                            chunks.append({
                                'type': 'visual',
                                'embedding': visual_embedding,
                                'text': ocr_text if ocr_text else '[IMAGE_VISUAL_EMBEDDING]',
                                'content': ocr_text if ocr_text else '[IMAGE_VISUAL_EMBEDDING]',
                                'modality': 'image',
                                'source_type': 'pdf',
                                'metadata': {
                                    **metadata,
                                    'page_number': page_num + 1,
                                    'image_index': img_index + 1,
                                    'width': image.width,
                                    'height': image.height,
                                    'format': image_ext,
                                    'ocr_confidence': ocr_confidence,
                                    'extraction_type': 'visual'
                                },
                                'file_path': str(file_path),
                                'chunk_index': chunk_index,
                                'total_chunks': 1
                            })
                            chunk_index += 1

                            if ocr_confidence > 70 and ocr_text:
                                ocr_embedding = embedder.embed(ocr_text)
                                chunks.append({
                                    'type': 'ocr',
                                    'embedding': ocr_embedding,
                                    'text': ocr_text,
                                    'content': ocr_text,
                                    'modality': 'text',
                                    'source_type': 'pdf',
                                    'metadata': {
                                        **metadata,
                                        'page_number': page_num + 1,
                                        'image_index': img_index + 1,
                                        'ocr_confidence': ocr_confidence,
                                        'extraction_type': 'ocr'
                                    },
                                    'file_path': str(file_path),
                                    'chunk_index': chunk_index,
                                    'total_chunks': 1
                                })
                                chunk_index += 1

                        except Exception as img_error:
                            logger.warning(f"Failed to extract image {img_index} from page {page_num + 1}: {img_error}")

            total_chunks = len(chunks)
            for chunk in chunks:
                chunk['total_chunks'] = total_chunks

            text_chunks = sum(1 for c in chunks if c.get('type') == 'text')
            visual_chunks = sum(1 for c in chunks if c.get('type') == 'visual')
            ocr_chunks = sum(1 for c in chunks if c.get('type') == 'ocr')

            logger.info(f"âœ… PDF processed successfully: {total_chunks} total chunks from {doc.page_count} pages")
            logger.info(f"   ðŸ“ Text chunks: {text_chunks} | ðŸ–¼ï¸ Visual chunks: {visual_chunks} | ðŸ” OCR chunks: {ocr_chunks}")
            return chunks

        except Exception as e:
            logger.error(f"Failed to process PDF {file_path}: {e}")
            raise Exception(f"PDF processing failed: {str(e)}")

    def extract_pages(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        Extract individual pages as separate chunks (legacy method for compatibility)

        Args:
            file_path: Path to the PDF file

        Returns:
            List of chunk dictionaries
        """
        return self.extract_text(file_path)

    def get_summary(self, file_path: Path) -> Dict[str, Any]:
        """
        Get a summary of the PDF without full text extraction

        Args:
            file_path: Path to the PDF file

        Returns:
            Summary dictionary
        """
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PdfReader(file)

                return {
                    'file_name': file_path.name,
                    'file_size': file_path.stat().st_size,
                    'pages': len(pdf_reader.pages),
                    'title': pdf_reader.metadata.title,
                    'author': pdf_reader.metadata.author,
                    'can_process': True
                }

        except Exception as e:
            logger.error(f"Failed to get PDF summary: {e}")
            return {
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'can_process': False,
                'error': str(e)
            }
FILE: app/ingestion/processors/text_processor.py
"""
Text file processor for plain text documents
"""
import logging
from pathlib import Path
from typing import List, Dict, Any

from app.config import settings
from app.ingestion.chunking.text_chunker import micro_chunk_text

logger = logging.getLogger(__name__)

class TextProcessor:
    """Processor for plain text files"""

    def __init__(self):
        self.supported_extensions = ['.txt', '.md', '.csv', '.json', '.xml', '.html']

    def can_process(self, file_path: Path) -> bool:
        """Check if the processor can handle the file"""
        return file_path.suffix.lower() in self.supported_extensions

    def detect_encoding(self, file_path: Path) -> str:
        """
        Detect the encoding of a text file

        Args:
            file_path: Path to the text file

        Returns:
            Detected encoding string
        """
        try:
            import chardet

            with open(file_path, 'rb') as file:
                raw_data = file.read(10000)
                result = chardet.detect(raw_data)
                return result.get('encoding', 'utf-8')
        except ImportError:
            return 'utf-8'
        except Exception:
            return 'utf-8'

    def extract_text(self, file_path: Path) -> Dict[str, Any]:
        """
        Extract text content from file

        Args:
            file_path: Path to the text file

        Returns:
            Dictionary containing extracted text and metadata
        """
        try:
            logger.info(f"Processing text file: {file_path}")

            encoding = self.detect_encoding(file_path)

            with open(file_path, 'r', encoding=encoding, errors='replace') as file:
                content = file.read()

            metadata = {
                'encoding': encoding,
                'file_size': file_path.stat().st_size,
                'line_count': len(content.splitlines()),
            }

            if file_path.suffix.lower() == '.json':
                try:
                    import json
                    json_data = json.loads(content)
                    metadata['json_keys'] = list(json_data.keys()) if isinstance(json_data, dict) else len(json_data)
                    metadata['is_valid_json'] = True
                except:
                    metadata['is_valid_json'] = False

            elif file_path.suffix.lower() == '.csv':
                try:
                    import csv
                    from io import StringIO
                    csv_reader = csv.reader(StringIO(content))
                    rows = list(csv_reader)
                    metadata['csv_rows'] = len(rows)
                    metadata['csv_columns'] = len(rows[0]) if rows else 0
                    metadata['is_valid_csv'] = True
                except:
                    metadata['is_valid_csv'] = False

            if len(content) <= 235:
                result = {
                    'content': content,
                    'metadata': metadata,
                    'total_chars': len(content),
                    'modality': 'text',
                    'source_type': 'text',
                    'chunk_index': 0,
                    'total_chunks': 1,
                    'prev_chunk_id': None,
                    'next_chunk_id': None
                }
                logger.info(f"Successfully processed text file: {len(content)} characters (1 chunk)")
            else:
                logger.info(f"Text content exceeds embedding limit â€” splitting into chunks")
                chunks = micro_chunk_text(content, chunk_size=235, chunk_overlap=30)

                result = []
                for i, chunk_data in enumerate(chunks):
                    result.append({
                        'content': chunk_data['chunk'],
                        'metadata': metadata,
                        'total_chars': len(content),
                        'modality': 'text',
                        'source_type': 'text',
                        'chunk_index': i,
                        'total_chunks': len(chunks),
                        'chunk_id': chunk_data['id'],
                        'prev_chunk_id': chunk_data['prev_chunk_id'],
                        'next_chunk_id': chunk_data['next_chunk_id'],
                        'file_path': str(file_path)
                    })
                logger.info(f"Successfully processed text file: {len(result)} chunks created")

            return result

        except Exception as e:
            logger.error(f"Failed to process text file {file_path}: {e}")
            raise Exception(f"Text processing failed: {str(e)}")

    def extract_lines(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        Extract individual lines as separate chunks

        Args:
            file_path: Path to the text file

        Returns:
            List of line dictionaries
        """
        result = self.extract_text(file_path)
        lines = result['content'].splitlines()

        chunks = []
        for i, line in enumerate(lines):
            if line.strip():
                chunks.append({
                    'content': line.strip(),
                    'line_number': i + 1,
                    'source_file': str(file_path),
                    'modality': 'text',
                    'source_type': 'text',
                    'metadata': result['metadata']
                })

        return chunks

    def get_summary(self, file_path: Path) -> Dict[str, Any]:
        """
        Get a summary of the text file without full content extraction

        Args:
            file_path: Path to the text file

        Returns:
            Summary dictionary
        """
        try:
            encoding = self.detect_encoding(file_path)

            with open(file_path, 'r', encoding=encoding, errors='replace') as file:
                content = file.read()

            return {
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'encoding': encoding,
                'line_count': len(content.splitlines()),
                'char_count': len(content),
                'can_process': True
            }

        except Exception as e:
            logger.error(f"Failed to get text file summary: {e}")
            return {
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'can_process': False,
                'error': str(e)
            }
FILE: app/ingestion/validators/__init__.py
[EMPTY FILE]
FILE: app/ingestion/validators/file_validator.py
import os
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class FileValidator:
	ALLOWED_EXTENSIONS = {
		'.pdf',
		'.doc', '.docx',
		'.txt',
		'.png', '.jpg', '.jpeg',
		'.mp3', '.wav'
	}

	MAX_SIZE_MB = 50

	def __init__(self):
		self.errors = []

	def validate(self, file_path):
		self.errors = []
		path = Path(file_path)

		if not path.exists():
			self.errors.append(f"File does not exist: {file_path}")
			return False

		if not os.access(path, os.R_OK):
			self.errors.append(f"File is not readable: {file_path}")
			return False

		ext = path.suffix.lower()
		if ext not in self.ALLOWED_EXTENSIONS:
			logger.warning(f"[VALIDATOR] Rejected file with unsupported extension: {file_path} (extension: {ext})")
			self.errors.append(
				f"File type '{ext}' is not supported. "
				f"Allowed types: {', '.join(sorted(self.ALLOWED_EXTENSIONS))}"
			)
			return False

		size_mb = path.stat().st_size / (1024 * 1024)
		if size_mb > self.MAX_SIZE_MB:
			self.errors.append(f"File size {size_mb:.2f}MB exceeds max {self.MAX_SIZE_MB}MB.")
			return False

		logger.info(f"[VALIDATOR] File validated successfully: {file_path}")
		return True

class BatchValidator:
	def __init__(self):
		self.errors = []

	def validate(self, file_list):
		self.errors = []
		validator = FileValidator()
		all_valid = True
		for file_path in file_list:
			if not validator.validate(file_path):
				all_valid = False
				self.errors.append({
					'file': str(file_path),
					'errors': validator.errors.copy()
				})
		return all_valid
FILE: app/integrations/__init__.py
[EMPTY FILE]
FILE: app/integrations/langsmith/__init__.py
[EMPTY FILE]
FILE: app/integrations/langsmith/tracer.py
[EMPTY FILE]
FILE: app/integrations/whisper/__init__.py
[EMPTY FILE]
FILE: app/integrations/whisper/audio_preprocessor.py
[EMPTY FILE]
FILE: app/integrations/whisper/transcriber.py
[EMPTY FILE]
FILE: app/main.py
import os
import logging
import warnings
import shutil

os.environ["ORT_LOGGING_LEVEL"] = "3"
os.environ["WHISPER_LOG_LEVEL"] = "ERROR"

logging.getLogger("onnxruntime").setLevel(logging.ERROR)
logging.getLogger("fastembed").setLevel(logging.ERROR)
logging.getLogger("uvicorn.error").setLevel(logging.ERROR)

warnings.filterwarnings("ignore", category=UserWarning)

"""
Main FastAPI application for the Multimodal RAG System
"""
from contextlib import asynccontextmanager
from datetime import datetime

from rich.console import Console
from rich.logging import RichHandler
from rich.table import Table

from fastapi import FastAPI, Request
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse, FileResponse

from app.api.v1.router import api_router
from app.config import settings

console = Console()

logger = logging.getLogger("app.main")

def force_wipe_memory(db_path: str):
    """
    Nuclear option: Deletes the vector store folder entirely to prevent
    old documents from leaking into new queries and causing semantic drift.

    This ensures every run starts with a completely empty memory.
    """
    if os.path.exists(db_path):
        print(f"ðŸ§¹ [RESET] Wiping old memory at: {db_path}")
        try:
            shutil.rmtree(db_path)
            os.makedirs(db_path, exist_ok=True)
            print("âœ¨ [CLEAN] Database is now empty. Ready for fresh ingestion.")
        except Exception as e:
            print(f"âš ï¸ [ERROR] Failed to wipe memory: {e}")
    else:
        print(f"â„¹ï¸ [INFO] No existing database found at {db_path}. Will create fresh.")
        os.makedirs(db_path, exist_ok=True)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan context manager"""

    settings.ensure_directories()

    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, settings.log_level))

    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    rich_handler = RichHandler(
        console=console,
        show_time=True,
        show_level=True,
        show_path=False,
        rich_tracebacks=True,
        tracebacks_show_locals=True
    )
    rich_handler.setLevel(getattr(logging, settings.log_level))

    file_handler = logging.FileHandler(settings.log_file_path)
    file_handler.setLevel(getattr(logging, settings.log_level))
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_formatter)

    root_logger.addHandler(rich_handler)
    root_logger.addHandler(file_handler)

    console.print("\n[bold blue]ðŸš€ Starting Multimodal RAG System...[/bold blue]\n", style="bold")

    console.print("[yellow]âš¡ Validating GPU availability...[/yellow]")
    try:
        from app.utils.gpu_check import validate_gpu_availability, get_gpu_info
        validate_gpu_availability()
        gpu_info = get_gpu_info()
        console.print(f"[green][OK] Hardware Acceleration: {gpu_info['device']} ({', '.join(gpu_info['engines'])})[/green]")
    except RuntimeError as e:
        console.print(f"[bold red][FAIL] GPU Validation Failed![/bold red]")
        console.print(f"[red]{str(e)}[/red]")
        root_logger.error(f"GPU validation failed: {e}")
        raise

    table = Table(title="System Configuration")
    table.add_column("Component", style="cyan", no_wrap=True)
    table.add_column("Status/Value", style="magenta")

    table.add_row("Version", settings.app_version)
    table.add_row("Debug Mode", str(settings.debug))
    table.add_row("Hardware Acceleration", gpu_info['device'])
    table.add_row("C++ Engines", ", ".join(gpu_info['engines']))
    table.add_row("Qdrant Host", f"{settings.qdrant_host}:{settings.qdrant_port}")
    table.add_row("Models Path", str(settings.models_dir))
    table.add_row("Log Level", settings.log_level)

    console.print(table)
    console.print()

    console.print("[yellow]ðŸ”¥ Warming up models for instant inference...[/yellow]")
    try:
        from app.embeddings.models.multimodal_embedder import MultimodalEmbedder
        console.print("[dim]   â€¢ Initializing FastEmbed CLIP models...[/dim]")
        embedder = MultimodalEmbedder()
        test_text_embedding = embedder.encode_text("test query for warmup")
        console.print(f"[dim]   â€¢ CLIP Text: {len(test_text_embedding)}D vector generated[/dim]")

        from app.ingestion.processors.audio_processor import AudioProcessor
        console.print("[dim]   â€¢ Initializing faster-whisper model...[/dim]")
        audio_processor = AudioProcessor()
        audio_processor._load_model()
        console.print("[dim]   â€¢ Whisper: CTranslate2 model loaded on GPU[/dim]")

        from app.reasoning.llm.llama_reasoner import LlamaReasoner
        console.print("[dim]   â€¢ Initializing Llama 3.2 1B model...[/dim]")
        reasoner = LlamaReasoner()
        console.print("[dim]   â€¢ Llama: All 16 layers offloaded to GPU[/dim]")

        console.print("[green][OK] All models warmed up and GPU-accelerated![/green]")

    except Exception as e:
        console.print(f"[bold red][WARN] Model warmup failed: {e}[/bold red]")
        console.print("[yellow]   Models will load on first request instead[/yellow]")
        root_logger.warning(f"Model warmup failed: {e}")

    console.print("[yellow]ðŸ“š Loading knowledge base topic catalog...[/yellow]")
    try:
        from app.storage.qdrant.client import QdrantClientWrapper
        from app.utils.topic_catalog_logger import log_topic_catalog

        qdrant_client = QdrantClientWrapper()
        current_topics = []

        if current_topics:
            log_topic_catalog(current_topics)
            console.print(f"[green][OK] Knowledge base covers {len(current_topics)} topics: {', '.join(current_topics)}[/green]")
        else:
            console.print("[yellow][INFO] Knowledge base is empty - upload documents to begin[/yellow]")
    except Exception as e:
        console.print(f"[yellow][WARN] Topic catalog logging failed: {e}[/yellow]")
        root_logger.warning(f"Topic catalog logging failed: {e}")

    root_logger.info(f"Application version: {settings.app_version}")
    root_logger.info(f"Debug mode: {settings.debug}")
    root_logger.info(f"Qdrant host: {settings.qdrant_host}:{settings.qdrant_port}")
    root_logger.info(f"Models path: {settings.models_dir}")

    console.print("[green][OK] Application startup complete![/green]\n")

    yield

    console.print("\n[yellow]â¹ï¸  Shutting down Multimodal RAG System...[/yellow]")
    root_logger.info("Shutting down Multimodal RAG System...")

app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    description="Multimodal Retrieval-Augmented Generation System",
    lifespan=lifespan,
    debug=settings.debug
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    max_age=3600,
)

app.add_middleware(GZipMiddleware, minimum_size=1000)

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "version": settings.app_version,
        "timestamp": datetime.utcnow().isoformat() + "Z"
    }

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global exception handler"""
    logger.error(
        f"Unhandled exception in {request.method} {request.url.path}: {exc}",
        exc_info=True,
        extra={
            'request_method': request.method,
            'request_url': str(request.url),
            'client_ip': request.client.host if request.client else None
        }
    )

    console.print(f"\n[red]âŒ Error in {request.method} {request.url.path}: {exc}[/red]")

    return JSONResponse(
        status_code=500,
        content={
            "detail": "Internal server error",
            "request_id": getattr(request.state, 'request_id', None)
        }
    )

app.include_router(api_router, prefix="/api/v1")

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app.main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug,
        log_level=settings.log_level.lower()
    )
FILE: app/reasoning/__init__.py
[EMPTY FILE]
FILE: app/reasoning/conflict/__init__.py
[EMPTY FILE]
FILE: app/reasoning/conflict/detector.py
"""
Conflict detector for identifying contradictory evidence
"""
import logging
from typing import List, Dict, Any, Set
from app.reasoning.llm.llama_reasoner import LlamaReasoner

logger = logging.getLogger(__name__)

class ConflictDetector:
    """Detects conflicts and contradictions in retrieved evidence"""

    def __init__(self):
        self.llm_client = LlamaReasoner()

    def detect_conflicts(self, query: str, evidence: List[str]) -> Dict[str, Any]:
        """Detect conflicts in the evidence set"""
        conflicts = []
        conflict_pairs = []

        for i in range(len(evidence)):
            for j in range(i + 1, len(evidence)):
                conflict = self._check_pair_conflict(evidence[i], evidence[j])
                if conflict["has_conflict"]:
                    conflicts.append(conflict)
                    conflict_pairs.append((i, j))

        consistency_score = self._calculate_consistency_score(evidence, conflicts)

        return {
            "conflicts": conflicts,
            "conflict_count": len(conflicts),
            "conflict_pairs": conflict_pairs,
            "consistency_score": consistency_score,
            "overall_consistent": consistency_score > 0.8
        }

    def _check_pair_conflict(self, evidence1: str, evidence2: str) -> Dict[str, Any]:
        """Check if two pieces of evidence conflict"""
        prompt = f"""Compare these two pieces of evidence for contradictions or conflicts:

Evidence 1: {evidence1[:400]}...
Evidence 2: {evidence2[:400]}...

Do they contradict each other? (Yes/No)
If yes, explain the contradiction briefly."""

        try:
            response = self.llm_client.generate_response(prompt, max_tokens=100)

            has_conflict = "yes" in response.lower()

            return {
                "has_conflict": has_conflict,
                "evidence_pair": [evidence1, evidence2],
                "explanation": response if has_conflict else None,
                "severity": self._assess_severity(response) if has_conflict else None
            }
        except Exception as e:
            logger.error(f"Failed to check conflict: {e}")
            return {
                "has_conflict": False,
                "evidence_pair": [evidence1, evidence2],
                "error": str(e)
            }

    def _assess_severity(self, conflict_explanation: str) -> str:
        """Assess the severity of a conflict"""
        explanation_lower = conflict_explanation.lower()

        if any(word in explanation_lower for word in ["major", "significant", "fundamental", "opposite"]):
            return "high"
        elif any(word in explanation_lower for word in ["minor", "slight", "partial"]):
            return "medium"
        else:
            return "low"

    def _calculate_consistency_score(self, evidence: List[str], conflicts: List[Dict]) -> float:
        """Calculate overall consistency score"""
        if not evidence:
            return 1.0

        total_pairs = len(evidence) * (len(evidence) - 1) / 2
        if total_pairs == 0:
            return 1.0

        conflict_count = len(conflicts)
        consistency_score = 1.0 - (conflict_count / total_pairs)

        return max(0.0, consistency_score)

    def resolve_conflicts(self, conflicts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Attempt to resolve detected conflicts"""
        resolutions = []

        for conflict in conflicts:
            if conflict["severity"] == "high":
                resolution = self._resolve_high_conflict(conflict)
            else:
                resolution = self._resolve_low_conflict(conflict)

            resolutions.append({
                "conflict": conflict,
                "resolution": resolution
            })

        return resolutions

    def _resolve_high_conflict(self, conflict: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve high-severity conflicts"""
        prompt = f"""Two pieces of evidence conflict significantly:

Evidence 1: {conflict['evidence_pair'][0][:300]}...
Evidence 2: {conflict['evidence_pair'][1][:300]}...

Conflict: {conflict['explanation']}

How should this conflict be resolved? Provide a reasoned approach."""

        try:
            resolution = self.llm_client.generate_response(prompt, max_tokens=150)
            return {
                "approach": "manual_review",
                "reasoning": resolution,
                "requires_human": True
            }
        except Exception as e:
            return {
                "approach": "flag_for_review",
                "error": str(e),
                "requires_human": True
            }

    def _resolve_low_conflict(self, conflict: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve low-severity conflicts"""
        return {
            "approach": "accept_both",
            "reasoning": "Low-severity conflict can be noted but both pieces retained",
            "requires_human": False
        }
FILE: app/reasoning/conflict/presenter.py
[EMPTY FILE]
FILE: app/reasoning/evidence/__init__.py
[EMPTY FILE]
FILE: app/reasoning/evidence/citation_generator.py
[EMPTY FILE]
FILE: app/reasoning/evidence/confidence_scorer.py
"""
Confidence scoring for evidence
"""
import logging
from typing import Dict, List, Any

logger = logging.getLogger(__name__)

class ConfidenceScorer:
    """Score confidence in evidence for reasoning"""

    def __init__(self):
        self.threshold_high = 0.8
        self.threshold_medium = 0.5

    def score_evidence(self, evidence: List[Dict[str, Any]]) -> float:
        """
        Score confidence in evidence

        Args:
            evidence: List of evidence items with metadata

        Returns:
            Confidence score between 0 and 1
        """
        try:
            if not evidence:
                return 0.0

            scores = []
            for item in evidence:
                similarity = item.get('similarity', 0.0)

                metadata_quality = self._assess_metadata_quality(item.get('metadata', {}))

                score = (similarity * 0.7) + (metadata_quality * 0.3)
                scores.append(score)

            avg_confidence = sum(scores) / len(scores)

            logger.debug(f"Calculated confidence: {avg_confidence:.3f} from {len(evidence)} items")
            return avg_confidence

        except Exception as e:
            logger.error(f"Error scoring confidence: {e}")
            return 0.0

    def _assess_metadata_quality(self, metadata: Dict[str, Any]) -> float:
        """Assess quality of metadata"""
        quality_score = 0.5

        if metadata.get('source'):
            quality_score += 0.2

        if metadata.get('timestamp'):
            quality_score += 0.1

        if metadata.get('type'):
            quality_score += 0.1

        if metadata.get('chunk_index') is not None:
            quality_score += 0.1

        return min(quality_score, 1.0)

    def get_confidence_level(self, score: float) -> str:
        """Get confidence level label"""
        if score >= self.threshold_high:
            return "high"
        elif score >= self.threshold_medium:
            return "medium"
        else:
            return "low"
FILE: app/reasoning/evidence/evidence_evaluator.py
"""
Evidence evaluator for assessing retrieved information quality
"""
import logging
from typing import List, Dict, Any, Tuple
from app.reasoning.llm.llama_reasoner import LlamaReasoner

logger = logging.getLogger(__name__)

class EvidenceEvaluator:
    """Evaluates quality and relevance of retrieved evidence"""

    def __init__(self):
        self.llm_client = LlamaReasoner()

    def evaluate_relevance(self, query: str, evidence: List[str]) -> List[Dict[str, Any]]:
        """Evaluate relevance of each evidence piece to the query"""
        evaluations = []

        for i, evidence_text in enumerate(evidence):
            prompt = f"""Rate the relevance of this evidence to the query on a scale of 1-10 (10 being perfectly relevant):

Query: {query}

Evidence: {evidence_text[:500]}...

Relevance score (1-10):"""

            try:
                response = self.llm_client.generate_response(prompt, max_tokens=10)
                score = self._extract_score(response)
                evaluations.append({
                    "index": i,
                    "evidence": evidence_text,
                    "relevance_score": score,
                    "is_relevant": score >= 7
                })
            except Exception as e:
                logger.error(f"Failed to evaluate evidence {i}: {e}")
                evaluations.append({
                    "index": i,
                    "evidence": evidence_text,
                    "relevance_score": 5,
                    "is_relevant": True,
                    "error": str(e)
                })

        return evaluations

    def check_sufficiency(self, query: str, evidence: List[str]) -> Dict[str, Any]:
        """Check if evidence is sufficient to answer the query"""
        combined_evidence = "\n".join(evidence)

        prompt = f"""Determine if the provided evidence is sufficient to fully answer this query:

Query: {query}

Evidence: {combined_evidence[:1000]}...

Is the evidence sufficient? (Yes/No/Partial)
If partial or no, what additional information is needed?"""

        try:
            assessment = self.llm_client.generate_response(prompt, max_tokens=200)

            sufficiency = "unknown"
            if "yes" in assessment.lower():
                sufficiency = "sufficient"
            elif "partial" in assessment.lower():
                sufficiency = "partial"
            elif "no" in assessment.lower():
                sufficiency = "insufficient"

            return {
                "sufficiency": sufficiency,
                "assessment": assessment,
                "evidence_count": len(evidence)
            }
        except Exception as e:
            logger.error(f"Failed to check sufficiency: {e}")
            return {
                "sufficiency": "unknown",
                "assessment": "Evaluation failed",
                "evidence_count": len(evidence),
                "error": str(e)
            }

    def detect_conflicts(self, evidence: List[str]) -> List[Dict[str, Any]]:
        """Detect conflicting information in evidence"""
        conflicts = []

        if len(evidence) < 2:
            return conflicts

        for i in range(len(evidence)):
            for j in range(i + 1, len(evidence)):
                prompt = f"""Compare these two pieces of evidence for conflicts:

Evidence 1: {evidence[i][:300]}...
Evidence 2: {evidence[j][:300]}...

Do they conflict? (Yes/No)
If yes, describe the conflict:"""

                try:
                    response = self.llm_client.generate_response(prompt, max_tokens=100)

                    if "yes" in response.lower():
                        conflicts.append({
                            "evidence_indices": [i, j],
                            "conflict_description": response,
                            "severity": "high" if "significant" in response.lower() else "medium"
                        })
                except Exception as e:
                    logger.error(f"Failed to check conflict between {i} and {j}: {e}")

        return conflicts

    def _extract_score(self, response: str) -> int:
        """Extract numerical score from LLM response"""
        import re
        scores = re.findall(r'\b(\d{1,2})\b', response)
        if scores:
            score = int(scores[0])
            return max(1, min(10, score))
        return 5

async def evaluate_evidence(retrieved_documents: List[Dict[str, Any]]) -> float:
    """
    Evaluate confidence score for retrieved documents

    Args:
        retrieved_documents: List of retrieved document dicts

    Returns:
        Confidence score between 0 and 1
    """
    import asyncio

    if not retrieved_documents:
        logger.warning("No documents to evaluate")
        return 0.0

    try:
        scores = []
        for doc in retrieved_documents:
            score = doc.get('score') or doc.get('similarity') or 0.4
            scores.append(float(score))

        avg_score = sum(scores) / len(scores) if scores else 0.0

        if len(retrieved_documents) >= 3:
            avg_score = min(1.0, avg_score * 1.15)
        elif len(retrieved_documents) >= 2:
            avg_score = min(1.0, avg_score * 1.05)

        audio_count = sum(1 for doc in retrieved_documents if doc.get('modality') == 'audio')
        if audio_count > 0:
            avg_score = min(1.0, avg_score * 1.1)
            logger.info(f"[EVIDENCE] Audio evidence boost applied ({audio_count} audio docs)")

        if len(retrieved_documents) > 0 and avg_score < 0.35:
            avg_score = 0.35

        logger.info(f"Evidence evaluation: {avg_score:.3f} from {len(retrieved_documents)} docs (scores: {[f'{s:.2f}' for s in scores[:3]]})")
        return avg_score

    except Exception as e:
        logger.error(f"Evidence evaluation failed: {e}")
        return 0.3 if retrieved_documents else 0.0
FILE: app/reasoning/evidence/intent_detector.py
"""
Query intent detection for evidence validation
"""
import re
from typing import Literal

QueryIntent = Literal["identity", "locate", "general"]

def detect_query_intent(query: str) -> QueryIntent:
    """
    Detect the intent of a query without using LLMs

    Args:
        query: The user's query text

    Returns:
        'identity': Questions asking "who is X" or "what is X"
        'locate': Questions asking where something is mentioned
        'general': Everything else
    """
    query_lower = query.lower().strip()

    identity_patterns = [
        r'^who\s+(is|was|are|were)\s+',
        r'^what\s+(is|was|are|were)\s+',
        r"^who'?s\s+",
        r"^what'?s\s+",
        r'^tell\s+me\s+(about|who)\s+',
        r'^describe\s+',
    ]

    for pattern in identity_patterns:
        if re.search(pattern, query_lower):
            return "identity"

    locate_patterns = [
        r'where\s+(is|are|was|were)\s+.+\s+mentioned',
        r'where\s+(does|did)\s+.+\s+(mention|say|state)',
        r'find\s+(the\s+)?(line|location|place|part)',
        r'show\s+me\s+where',
        r'get\s+me\s+the\s+line',
        r'in\s+which\s+(file|document|line)',
    ]

    for pattern in locate_patterns:
        if re.search(pattern, query_lower):
            return "locate"

    return "general"

def has_descriptive_evidence(documents: list) -> bool:
    """
    Check if documents contain descriptive language (not just token matches)

    For identity queries, we need actual descriptive sentences like:
    - "is a developer"
    - "works as an engineer"
    - "known for building X"

    NOT just:
    - File trees with names
    - Code snippets with variable names
    - Token matches without context

    Args:
        documents: List of retrieved document dicts

    Returns:
        True if documents contain descriptive evidence
    """
    if not documents:
        return False

    descriptive_indicators = [
        r'\b(is|was|are|were)\s+a\s+',
        r'\b(is|was|are|were)\s+an\s+',
        r'\bworks?\s+(as|at|for)\s+',
        r'\bknown\s+for\s+',
        r'\bspecializes?\s+in\s+',
        r'\bexpert\s+in\s+',
        r'\b(developer|engineer|designer|manager|scientist|researcher)\b',
        r'\b(he|she|they)\s+(is|was|are|were)\s+',
        r'\bresponsible\s+for\s+',
        r'\brole\s+(is|was)\s+',
        r'\bposition\s+(is|was)\s+',
        r'\btitle\s+(is|was)\s+',
        r'\bbackground\s+in\s+',
        r'\bexperience\s+(in|with)\s+',
    ]

    anti_patterns = [
        r'^[\s\â”‚\â”œ\â””\-]+',
        r'^\s*[/\\]',
        r'^\s*(def|class|function|const|let|var)\s+',
        r'^\s*import\s+',
        r'^\s*
        r'^\s*package\s+',
    ]

    descriptive_count = 0

    for doc in documents:
        content = doc.get('content', '').lower()

        if not content or len(content.strip()) < 20:
            continue

        is_code_or_tree = any(re.search(pattern, content[:100]) for pattern in anti_patterns)
        if is_code_or_tree:
            continue

        has_descriptor = any(re.search(pattern, content) for pattern in descriptive_indicators)
        if has_descriptor:
            descriptive_count += 1

    return descriptive_count > 0
FILE: app/reasoning/evidence/strength_assessor.py
[EMPTY FILE]
FILE: app/reasoning/hallucination/__init__.py
[EMPTY FILE]
FILE: app/reasoning/hallucination/detector.py
[EMPTY FILE]
FILE: app/reasoning/hallucination/refusal_engine.py
"""
Refusal engine for handling insufficient evidence and hallucinations
"""
import logging
from typing import Dict, Any, Optional
from app.reasoning.llm.llama_reasoner import LlamaReasoner

logger = logging.getLogger(__name__)

class RefusalEngine:
    """Engine for generating appropriate refusals when evidence is insufficient"""

    def __init__(self):
        self.llm_client = LlamaReasoner()

    def should_refuse(self, query: str, evidence: list, confidence_score: float) -> bool:
        """Determine if the query should be refused based on evidence quality"""
        if not evidence:
            return True

        if confidence_score < 0.3:
            return True

        sufficiency = self._assess_sufficiency(query, evidence)
        return sufficiency["should_refuse"]

    def generate_refusal(self, query: str, evidence: list, reason: str = "insufficient_evidence") -> str:
        """Generate an appropriate refusal response"""
        if reason == "insufficient_evidence":
            return self._refuse_insufficient_evidence(query, evidence)
        elif reason == "hallucination_risk":
            return self._refuse_hallucination_risk(query)
        elif reason == "conflict":
            return self._refuse_conflict(query)
        else:
            return self._refuse_generic(query)

    def _assess_sufficiency(self, query: str, evidence: list) -> Dict[str, Any]:
        """Assess if evidence is sufficient for the query"""
        evidence_text = "\n".join(evidence[:3])

        prompt = f"""Evaluate if this evidence is sufficient to answer the query:

Query: {query}

Evidence: {evidence_text[:500]}...

Can this query be answered with confidence using only this evidence? (Yes/No)
If no, why not?"""

        try:
            response = self.llm_client.generate_response(prompt, max_tokens=100)

            should_refuse = "no" in response.lower() or "insufficient" in response.lower()

            return {
                "should_refuse": should_refuse,
                "reasoning": response,
                "evidence_count": len(evidence)
            }
        except Exception as e:
            logger.error(f"Failed to assess sufficiency: {e}")
            return {
                "should_refuse": True,
                "reasoning": "Assessment failed",
                "error": str(e)
            }

    def _refuse_insufficient_evidence(self, query: str, evidence: list) -> str:
        """Generate refusal for insufficient evidence"""
        evidence_summary = f"{len(evidence)} pieces of evidence" if evidence else "no evidence"

        return f"""I cannot provide a confident answer to your query: "{query}"

Based on the available information ({evidence_summary}), there is insufficient evidence to formulate a reliable response. The retrieved information does not adequately address your question.

Please provide more context or rephrase your question to help me locate more relevant information."""

    def _refuse_hallucination_risk(self, query: str) -> str:
        """Generate refusal for hallucination risk"""
        return f"""I must decline to answer your query: "{query}"

The available evidence, while present, carries a high risk of leading to inaccurate or fabricated information. To maintain reliability, I require more substantial and verifiable evidence before providing an answer."""

    def _refuse_conflict(self, query: str) -> str:
        """Generate refusal for conflicting evidence"""
        return f"""I cannot provide a definitive answer to your query: "{query}"

The retrieved evidence contains conflicting information that cannot be reliably reconciled. This creates uncertainty that would compromise the accuracy of any response.

Please consider refining your query or providing additional context to help resolve these conflicts."""

    def _refuse_generic(self, query: str) -> str:
        """Generate generic refusal"""
        return f"""I'm unable to provide a satisfactory answer to your query: "{query}"

The available evidence does not meet the threshold for generating a reliable response. This could be due to insufficient information, conflicting data, or other quality concerns.

Please try rephrasing your question or providing more specific details."""

    def generate_uncertainty_notice(self, confidence_score: float) -> str:
        """Generate a notice about uncertainty in the response"""
        if confidence_score < 0.5:
            return "âš ï¸ This response is based on limited evidence and should be treated with caution."
        elif confidence_score < 0.7:
            return "â„¹ï¸ This response has moderate confidence based on the available evidence."
        else:
            return "OK This response is based on strong supporting evidence."
FILE: app/reasoning/hallucination/suppressor.py
[EMPTY FILE]
FILE: app/reasoning/llm/__init__.py
[EMPTY FILE]
FILE: app/reasoning/llm/llama_reasoner.py
"""
Llama Reasoner using llama-cpp-python with CUDA GPU offloading
Singleton pattern to prevent multiple model loads
"""
import logging
import os
import threading
from typing import List, Dict, Any, Optional
from llama_cpp import Llama
from app.config import settings

logger = logging.getLogger(__name__)

class LlamaReasoner:
    """Llama-3.2-1B-Instruct GGUF Reasoner with full GPU offloading (Singleton)"""

    _instance = None
    _llm = None
    _lock = threading.Lock()

    def __new__(cls, model_path: Optional[str] = None):
        """Thread-safe singleton pattern: only one instance of LlamaReasoner"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(LlamaReasoner, cls).__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self, model_path: Optional[str] = None):
        if self._initialized:
            return

        model_path = model_path or str(settings.models_dir / "models--bartowski--Llama-3.2-1B-Instruct-GGUF" / "snapshots" / "067b946cf014b7c697f3654f621d577a3e3afd1c" / "Llama-3.2-1B-Instruct-Q4_K_M.gguf")

        self.model_path = model_path
        self.n_ctx = settings.llama_cpp_n_ctx
        self.n_threads = settings.llama_cpp_n_threads
        self.n_gpu_layers = settings.llama_cpp_n_gpu_layers
        self.n_batch = settings.llama_cpp_n_batch
        self.main_gpu = settings.llama_cpp_main_gpu
        self.verbose = False

        if LlamaReasoner._llm is None:
            with LlamaReasoner._lock:
                if LlamaReasoner._llm is None:
                    self._load_model()
        else:
            logger.info("Using existing Llama model instance (singleton)")

        self._initialized = True

    def _load_model(self):
        logger.info(f"Loading Llama model (singleton): {self.model_path}")

        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"Llama model file not found: {self.model_path}")

        LlamaReasoner._llm = Llama(
            model_path=self.model_path,
            n_ctx=self.n_ctx,
            n_threads=self.n_threads,
            n_gpu_layers=self.n_gpu_layers,
            n_batch=self.n_batch,
            main_gpu=self.main_gpu,
            verbose=self.verbose
        )
        actual_gpu_layers = getattr(LlamaReasoner._llm, 'n_gpu_layers', 0)
        if actual_gpu_layers > 0:
            logger.info(f"Llama model loaded with {actual_gpu_layers} layers on GPU.")
        else:
            logger.warning("Llama model loaded on CPU. GPU offloading not available.")

    @property
    def llm(self):
        """Access the shared LLM instance"""
        return LlamaReasoner._llm

    def generate_response(self, system_prompt: str, user_prompt: str, max_tokens: int = 512, temperature: float = 0.2) -> str:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        response = self.llm.create_chat_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response["choices"][0]["message"]["content"].strip()

    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7, stop_sequences: list = None) -> str:
        """Generates a response from the Llama model (alias for compatibility)."""
        try:
            completion_params = {
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant that answers questions based on provided context."},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": max_tokens,
                "temperature": temperature
            }

            if stop_sequences:
                completion_params["stop"] = stop_sequences

            response = self.llm.create_chat_completion(**completion_params)
            return response["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"LLM Generation Error: {e}")
            return "I encountered an error generating the final answer."
FILE: app/reasoning/llm/output_parser.py
[EMPTY FILE]
FILE: app/reasoning/llm/prompt_builder.py
"""
Prompt Builder for evidence-grounded generation
"""
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)

def build_multimodal_prompt(retrieved_documents: List[Dict[str, Any]],
                            query: Optional[str] = None) -> str:
    """
    Build a prompt for evidence-grounded generation with strict refusal enforcement

    Args:
        retrieved_documents: List of retrieved document dictionaries
        query: Optional query to include

    Returns:
        Formatted prompt string with evidence or explicit refusal instruction
    """
    system_prompt = """ðŸ”’ SYSTEM PROMPT â€” EVIDENCE-GATED RAG ENFORCEMENT
You are an Evidence-Grounded Reasoning Agent in a Multimodal RAG system.

CRITICAL RULES (MUST FOLLOW):
1. You are ONLY allowed to answer using the provided evidence.
2. You MUST NOT use your general or prior knowledge.
3. If the provided evidence is empty, insufficient, or irrelevant:
   - You MUST refuse to answer.
   - You MUST NOT attempt to explain the topic.
4. Refusal is REQUIRED when:
   - No documents are provided
   - Evidence does not directly support the query
   - Confidence cannot be established from evidence

ANSWER SUFFICIENCY RULES (MANDATORY):
5. Single-word, label-only, or entity-only responses are NOT valid answers.
6. For "who is / what is / explain" queries, you MUST provide a complete explanatory sentence.
7. If evidence only confirms existence (e.g., name mentioned once) without details:
   - You MUST explicitly state that limitation in your answer.
   - Example: "The evidence mentions [entity] but does not provide enough information to fully answer the question."
8. NEVER output just an entity name, label, or identifier as the answer.
9. Answers must be substantive, containing at least 8 words in a complete sentence.

REFUSAL FORMAT (MANDATORY):
If you refuse, respond ONLY with valid JSON in this exact structure:

{
  "refusal": true,
  "reason": "Insufficient or missing supporting evidence in the knowledge base.",
  "answer": null,
  "confidence": 0.0,
  "cited_sources": []
}

ANSWER FORMAT (ONLY IF EVIDENCE EXISTS):
If and only if evidence is sufficient, respond in valid JSON:

{
  "refusal": false,
  "answer": "<complete explanatory sentence strictly derived from evidence>",
  "confidence": <float between 0 and 1>,
  "cited_sources": [
    {
      "source_id": "<source identifier>",
      "modality": "<text | image | audio>"
    }
  ]
}

IMPORTANT:
- You must NEVER answer from memory.
- You must NEVER hallucinate.
- You must NEVER partially answer.
- You must NEVER give single-word or label-only responses.
- Silence or incomplete JSON is NOT allowed.
- Every answer must be a complete, informative sentence.
"""

    if not retrieved_documents or len(retrieved_documents) == 0:
        return f"""{system_prompt}

Evidence:
[No evidence available]

Question: {query if query else 'N/A'}

You MUST respond with the refusal JSON format since no evidence is provided."""

    context_parts = []
    source_map = {}

    for i, doc in enumerate(retrieved_documents[:10], 1):
        content = doc.get('content', doc.get('text', ''))
        metadata = doc.get('metadata', {})
        modality = metadata.get('modality', 'unknown')
        source = metadata.get('source_file', 'unknown')

        source_id = f"evidence_{i}"
        source_map[source_id] = {
            "source": source,
            "modality": modality
        }

        context_parts.append(
            f"[{source_id}] (Source: {source}, Type: {modality})\n{content}\n"
        )

    context = "\n".join(context_parts)

    if query:
        prompt = f"""{system_prompt}

Evidence:
{context}

Question: {query}

IMPORTANT INSTRUCTIONS:
1. Respond with ONLY ONE valid JSON object - nothing else.
2. Do NOT include the template examples in your response.
3. Do NOT add conversational text like "Please let me know" or "I have a follow-up question".
4. Do NOT output multiple JSON objects.
5. Your ENTIRE response must be a single valid JSON object.

Respond NOW with your JSON (ANSWER format if evidence sufficient, REFUSAL format if not):"""
    else:
        prompt = f"""{system_prompt}

Evidence:
{context}

Provide a summary in valid JSON format based on the evidence above."""

    return prompt

def build_evaluation_prompt(query: str, evidence: List[str]) -> str:
    """
    Build a prompt for evidence evaluation

    Args:
        query: User query
        evidence: List of evidence strings

    Returns:
        Evaluation prompt
    """
    evidence_text = "\n\n".join([f"Evidence {i+1}: {e}" for i, e in enumerate(evidence)])

    prompt = f"""Evaluate whether the provided evidence is sufficient to answer this question:

Question: {query}

{evidence_text}

Evaluation:
1. Sufficiency: (Sufficient/Partial/Insufficient)
2. Confidence Level: (High/Medium/Low)
3. Missing Information: (if any)
4. Recommendation: (Answer/Request More Info/Refuse)

Provide your evaluation:"""

    return prompt

def build_conflict_detection_prompt(evidence_pieces: List[str]) -> str:
    """
    Build a prompt for detecting conflicts in evidence

    Args:
        evidence_pieces: List of evidence strings

    Returns:
        Conflict detection prompt
    """
    evidence_text = "\n\n".join([f"Statement {i+1}: {e}" for i, e in enumerate(evidence_pieces)])

    prompt = f"""Analyze these statements for contradictions or conflicts:

{evidence_text}

Analysis:
1. Are there any contradictions? (Yes/No)
2. If yes, describe the conflicts:
3. Which statements are most reliable?

Provide your analysis:"""

    return prompt
FILE: app/reasoning/llm/prompt_templates.py
[EMPTY FILE]
FILE: app/reasoning/uncertainty/__init__.py
[EMPTY FILE]
FILE: app/reasoning/uncertainty/communicator.py
[EMPTY FILE]
FILE: app/reasoning/uncertainty/quantifier.py
[EMPTY FILE]
FILE: app/retrieval/__init__.py
[EMPTY FILE]
FILE: app/retrieval/alignment/__init__.py
[EMPTY FILE]
FILE: app/retrieval/alignment/cross_modal_aligner.py
[EMPTY FILE]
FILE: app/retrieval/orchestrator.py
"""
Retrieval orchestrator for multimodal search
"""
import logging
from typing import List, Dict, Any, Optional
from app.storage.vector_store import VectorStore

logger = logging.getLogger(__name__)

class RetrievalOrchestrator:
    """Orchestrates multimodal retrieval across different strategies"""

    def __init__(self):
        self.vector_store = VectorStore()

    def retrieve(self, query: str, top_k: Optional[int] = None, allowed_sources: Optional[List[str]] = None) -> Dict[str, Any]:
        """Retrieve relevant documents for the query with optional source filtering"""
        try:
            if top_k is None:
                top_k = 10

            logger.info(f"Retrieving documents for query: {query[:50]}...")

            raw_results = self.vector_store.query(
                query_text=query,
                modality="text",
                n_results=top_k,
                skip_gate=True
            )

            if raw_results.get("status") == "refused":
                return {
                    "query": query,
                    "results": [],
                    "total_found": 0,
                    "refused": True,
                    "reason": raw_results.get("reason", "Query refused by topic-concept gate"),
                    "query_topic": raw_results.get("query_topic"),
                    "query_concepts": raw_results.get("query_concepts"),
                    "knowledge_base_topics": raw_results.get("knowledge_base_topics", []),
                    "knowledge_base_concepts": raw_results.get("knowledge_base_concepts", [])
                }

            formatted_results = self._format_results(raw_results)

            logger.info(f"Retrieved {len(formatted_results)} documents")

            return {
                "query": query,
                "results": formatted_results,
                "total_found": len(formatted_results),
                "query_topic": raw_results.get("query_topic"),
                "query_concepts": raw_results.get("query_concepts"),
                "match_reason": raw_results.get("match_reason")
            }

        except Exception as e:
            logger.error(f"Retrieval failed: {e}")
            return {
                "query": query,
                "results": [],
                "total_found": 0,
                "error": str(e)
            }

    def _format_results(self, raw_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Format raw retrieval results from VectorStore"""
        formatted = []

        if raw_results.get("status") == "refused":
            return formatted

        if raw_results.get("status") != "success":
            logger.warning(f"Unexpected query status: {raw_results.get('status')}")
            return formatted

        ids = raw_results.get("ids", [])
        distances = raw_results.get("distances", [])
        metadatas = raw_results.get("metadatas", [])
        documents = raw_results.get("documents", [])

        min_length = min(len(ids), len(distances), len(metadatas), len(documents))
        if min_length == 0:
            return formatted

        for i in range(min_length):
            metadata = metadatas[i] if i < len(metadatas) else {}
            formatted.append({
                "id": ids[i] if i < len(ids) else f"unknown_{i}",
                "content": documents[i] if i < len(documents) else "",
                "metadata": metadata,
                "score": 1.0 - distances[i] if i < len(distances) else 0.0,
                "rank": i + 1,
                "source": metadata.get("source", "unknown"),
                "modality": metadata.get("modality", "text")
            })

        return formatted

    def _should_use_lexical_fallback(self, query: str) -> bool:
        """Determine if lexical fallback should be used for this query"""
        query_stripped = query.strip()
        words = query_stripped.split()

        if len(words) <= 3:
            return True

        if len(words) > 1:
            capitalized_count = sum(1 for word in words[1:] if word and word[0].isupper())
            if capitalized_count > 0:
                return True

        query_lower = query_stripped.lower()
        definition_patterns = [
            "what is", "who is", "what are", "who are",
            "explain", "define", "describe",
            "tell me about", "information about"
        ]
        if any(pattern in query_lower for pattern in definition_patterns):
            return True

        return False

    def _calculate_token_overlap(self, query: str, text: str) -> float:
        """Calculate token overlap ratio between query and text"""
        query_tokens = set(query.lower().split())
        text_tokens = set(text.lower().split())

        stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                     'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
                     'should', 'may', 'might', 'must', 'can', 'about', 'of', 'for', 'in',
                     'on', 'at', 'to', 'from', 'with', 'by'}

        query_tokens = query_tokens - stopwords
        text_tokens = text_tokens - stopwords

        if not query_tokens:
            return 0.0

        overlap = len(query_tokens.intersection(text_tokens))
        return overlap / len(query_tokens)

    def _lexical_fallback(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """Intelligent lexical fallback with token overlap matching"""
        try:
            logger.info(f"Running intelligent lexical search for: {query}")

            all_docs = self.vector_store.get_documents()

            if not all_docs.get("documents"):
                return []

            query_lower = query.lower().strip()
            query_stripped = query_lower.strip('?.,!"\'')
            matches = []

            documents = all_docs["documents"]
            metadatas = all_docs.get("metadatas", [])
            ids = all_docs.get("ids", [])

            for i, doc in enumerate(documents):
                doc_lower = doc.lower()
                match_score = 0.0

                if query_stripped in doc_lower:
                    match_score = 0.7
                else:
                    overlap = self._calculate_token_overlap(query_stripped, doc_lower)
                    if overlap >= 0.4:
                        match_score = 0.5 + (overlap * 0.2)

                if match_score > 0.0:
                    metadata = metadatas[i] if i < len(metadatas) else {}
                    doc_id = ids[i] if i < len(ids) else f"lexical_{i}"

                    matches.append({
                        "id": doc_id,
                        "content": doc,
                        "metadata": metadata,
                        "score": match_score,
                        "rank": len(matches) + 1,
                        "source": metadata.get("source", "unknown"),
                        "modality": metadata.get("modality", "text")
                    })

            matches.sort(key=lambda x: x["score"], reverse=True)
            matches = matches[:top_k]

            for i, match in enumerate(matches):
                match["rank"] = i + 1

            logger.info(f"Lexical search found {len(matches)} matches (filtered and ranked)")
            return matches

        except Exception as e:
            logger.error(f"Lexical fallback failed: {e}")
            return []

        return formatted

    def add_documents(self, documents: List[Dict[str, Any]]) -> bool:
        """Add documents to the vector store"""
        try:
            embeddings = []
            metadatas = []
            ids = []

            for doc in documents:
                if doc.get("modality") == "text":
                    embedding = self.embedder.encode_text([doc["content"]])[0]
                elif doc.get("modality") == "image":
                    embedding = self.embedder.encode_text([doc.get("caption", doc["content"])])[0]
                else:
                    embedding = self.embedder.encode_text([doc["content"]])[0]

                embeddings.append(embedding.tolist())
                metadatas.append(doc.get("metadata", {}))
                ids.append(doc.get("id", f"doc_{len(ids)}"))

            self.vector_store.add_embeddings(embeddings, metadatas, ids)

            logger.info(f"Added {len(documents)} documents to vector store")
            return True

        except Exception as e:
            logger.error(f"Failed to add documents: {e}")
            return False
FILE: app/retrieval/query/__init__.py
[EMPTY FILE]
FILE: app/retrieval/query/analyzer.py
"""
Query Analyzer - Intent classification and modality detection for multimodal RAG
"""
import logging
from enum import Enum
from typing import List

logger = logging.getLogger(__name__)

class QueryIntent(Enum):
    """Query intent types for modality-aware retrieval"""
    VISUAL_ATTRIBUTE = "visual_attribute"
    VISUAL_DESCRIPTION = "visual_description"
    VISUAL_IDENTITY = "visual_identity"
    AUDIO_CONTENT = "audio_content"
    TEXT_SEARCH = "text_search"

class QueryAnalyzer:
    """Analyzes queries to determine intent and required modalities"""

    VISUAL_ATTRIBUTE_KEYWORDS = [
        'color', 'colour', 'shape', 'size', 'appearance', 'look', 'looks',
        'wear', 'wearing', 'clothing', 'shirt', 'dress', 'outfit', 'pants',
        'shoes', 'hat', 'jacket', 'clothes',
        'red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown',
        'black', 'white', 'gray', 'grey', 'violet', 'indigo', 'cyan', 'magenta',
        'round', 'square', 'rectangular', 'circular', 'oval', 'triangular',
        'big', 'small', 'large', 'tiny', 'huge', 'tall', 'short', 'wide', 'narrow'
    ]

    VISUAL_DESCRIPTION_KEYWORDS = [
        'image', 'picture', 'photo', 'see', 'show', 'visible', 'shown',
        'in the image', 'in the picture', 'in the photo',
        'what is in', 'what\'s in', 'what do you see', 'describe the'
    ]

    AUDIO_CONTENT_KEYWORDS = [
        'said', 'mentioned', 'audio', 'recording', 'voice', 'transcript',
        'spoken', 'discussed', 'talked', 'conversation', 'interview',
        'what did', 'what was said', 'listen', 'hear', 'heard'
    ]

    VISUAL_IDENTITY_KEYWORDS = [
        'who is', 'what is this', 'who\'s this', 'identify', 'person in',
        'who are', 'name of', 'which person'
    ]

    def classify_intent(self, query: str) -> QueryIntent:
        """
        Classify query intent based on keyword matching.
        Priority order: visual_attribute > visual_description > audio_content > visual_identity > text_search

        Args:
            query: User query string

        Returns:
            QueryIntent enum value
        """
        query_lower = query.lower()

        if any(keyword in query_lower for keyword in self.VISUAL_ATTRIBUTE_KEYWORDS):
            logger.info(f"[ANALYZER] Classified as VISUAL_ATTRIBUTE: '{query}'")
            return QueryIntent.VISUAL_ATTRIBUTE

        if any(keyword in query_lower for keyword in self.VISUAL_DESCRIPTION_KEYWORDS):
            logger.info(f"[ANALYZER] Classified as VISUAL_DESCRIPTION: '{query}'")
            return QueryIntent.VISUAL_DESCRIPTION

        if any(keyword in query_lower for keyword in self.AUDIO_CONTENT_KEYWORDS):
            logger.info(f"[ANALYZER] Classified as AUDIO_CONTENT: '{query}'")
            return QueryIntent.AUDIO_CONTENT

        if any(keyword in query_lower for keyword in self.VISUAL_IDENTITY_KEYWORDS):
            logger.info(f"[ANALYZER] Classified as VISUAL_IDENTITY: '{query}'")
            return QueryIntent.VISUAL_IDENTITY

        logger.info(f"[ANALYZER] Classified as TEXT_SEARCH (default): '{query}'")
        return QueryIntent.TEXT_SEARCH

    def get_required_modalities(self, intent: QueryIntent) -> List[str]:
        """
        Map query intent to required modalities for retrieval filtering.

        Args:
            intent: QueryIntent enum value

        Returns:
            List of required modality strings
        """
        modality_map = {
            QueryIntent.VISUAL_ATTRIBUTE: ["image"],
            QueryIntent.VISUAL_DESCRIPTION: ["image"],
            QueryIntent.VISUAL_IDENTITY: ["image", "text"],
            QueryIntent.AUDIO_CONTENT: ["audio", "text"],
            QueryIntent.TEXT_SEARCH: ["text"]
        }

        modalities = modality_map.get(intent, ["text"])
        logger.debug(f"[ANALYZER] Intent {intent.value} requires modalities: {modalities}")
        return modalities

    def is_visual_intent(self, intent: QueryIntent) -> bool:
        """Check if the intent requires visual evidence"""
        return intent in [
            QueryIntent.VISUAL_ATTRIBUTE,
            QueryIntent.VISUAL_DESCRIPTION,
            QueryIntent.VISUAL_IDENTITY
        ]
FILE: app/retrieval/query/expander.py
[EMPTY FILE]
FILE: app/retrieval/query/multi_query_generator.py
"""
Multi-Query Generator using LLM
"""
import logging
from typing import List
import asyncio

logger = logging.getLogger(__name__)

async def generate_multi_queries(query: str, llama_client) -> List[str]:
    """
    Generate multiple query variations using LLM

    Args:
        query: Original user query
        llama_client: QwenReasoner instance

    Returns:
        List of query variations including the original
    """
    try:
        from app.storage.vector_store import VectorStore
        vector_store = VectorStore()
        kb_summary = vector_store.get_knowledge_catalog()
        kb_topics = kb_summary.get('topics', [])
        kb_concepts = kb_summary.get('concepts', [])

        if not kb_topics and not kb_concepts:
            logger.info("[MULTI-QUERY] KB is empty - skipping LLM query generation")
            return [query]
    except Exception as e:
        logger.warning(f"[MULTI-QUERY] Failed to check KB status: {e} - proceeding with generation")

    try:
        prompt = f"""Generate 2 alternative phrasings of this question. Only output the questions, nothing else.

Question: {query}

1.
2."""

        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            llama_client.generate_response,
            prompt,
            100
        )

        lines = [line.strip() for line in response.strip().split('\n') if line.strip()]
        alternative_queries = []

        for line in lines:
            skip_phrases = ['here are', 'alternative', 'phrasings', 'following', 'rephrase']
            if any(phrase in line.lower() for phrase in skip_phrases):
                continue

            cleaned = line.strip()
            if cleaned and len(cleaned) > 10:
                if cleaned[0].isdigit() and '.' in cleaned[:3]:
                    cleaned = cleaned.split('.', 1)[1].strip()

                cleaned = cleaned.replace('**', '').replace('*', '')

                if cleaned and not any(phrase in cleaned.lower() for phrase in skip_phrases):
                    alternative_queries.append(cleaned)

        all_queries = [query]
        if alternative_queries:
            all_queries.extend(alternative_queries[:2])

        logger.info(f"Generated {len(all_queries)} query variations")
        return all_queries

    except Exception as e:
        logger.error(f"Failed to generate multi-queries: {e}")
        return [query]
FILE: app/retrieval/reranking/__init__.py
[EMPTY FILE]
FILE: app/retrieval/reranking/relevance_scorer.py
[EMPTY FILE]
FILE: app/retrieval/reranking/reranker.py
[EMPTY FILE]
FILE: app/retrieval/strategies/__init__.py
[EMPTY FILE]
FILE: app/retrieval/strategies/base_strategy.py
[EMPTY FILE]
FILE: app/retrieval/strategies/cross_modal_strategy.py
[EMPTY FILE]
FILE: app/retrieval/strategies/intent_aware_strategy.py
[EMPTY FILE]
FILE: app/retrieval/strategies/multimodal_strategy.py
"""
Multimodal retrieval strategy
"""
import logging
from typing import Dict, Any, List
from app.config import settings

logger = logging.getLogger(__name__)

class MultimodalRetrievalStrategy:
    """Strategy for processing multimodal retrieval results"""

    def __init__(self):
        self.reranking_enabled = settings.reranking_enabled
        self.similarity_threshold = settings.similarity_threshold

    def process_results(self, query: str, raw_results: Dict[str, Any]) -> Dict[str, Any]:
        """Process and refine retrieval results"""
        try:
            filtered_results = self._filter_by_threshold(raw_results)

            if self.reranking_enabled:
                filtered_results = self._rerank_results(query, filtered_results)

            balanced_results = self._balance_modalities(filtered_results)

            return balanced_results

        except Exception as e:
            logger.error(f"Failed to process results: {e}")
            return raw_results

    def _filter_by_threshold(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Filter results by similarity threshold"""
        if not results.get("distances"):
            return results

        similarities = [1.0 - dist for dist in results["distances"][0]]

        keep_indices = [i for i, sim in enumerate(similarities) if sim >= self.similarity_threshold]

        if not keep_indices:
            keep_indices = list(range(min(3, len(similarities))))
            logger.warning(f"No results met threshold {self.similarity_threshold}, keeping top {len(keep_indices)}")

        filtered_results = {
            "documents": [[results["documents"][0][i] for i in keep_indices]],
            "metadatas": [[results["metadatas"][0][i] for i in keep_indices]],
            "distances": [[results["distances"][0][i] for i in keep_indices]],
            "ids": [[results["ids"][0][i] for i in keep_indices]]
        }

        logger.info(f"Filtered results: {len(keep_indices)} kept from {len(similarities)}")
        return filtered_results

    def _rerank_results(self, query: str, results: Dict[str, Any]) -> Dict[str, Any]:
        """Rerank results based on query relevance"""

        if not results.get("documents") or not results["documents"][0]:
            return results

        documents = results["documents"][0]
        metadatas = results["metadatas"][0]

        rerank_scores = []
        for doc, metadata in zip(documents, metadatas):
            score = self._calculate_rerank_score(query, doc, metadata)
            rerank_scores.append(score)

        sorted_indices = sorted(range(len(rerank_scores)), key=lambda i: rerank_scores[i], reverse=True)

        reranked_results = {
            "documents": [[documents[i] for i in sorted_indices]],
            "metadatas": [[metadatas[i] for i in sorted_indices]],
            "distances": [[results["distances"][0][i] for i in sorted_indices]],
            "ids": [[results["ids"][0][i] for i in sorted_indices]]
        }

        logger.info("Results reranked")
        return reranked_results

    def _calculate_rerank_score(self, query: str, document: str, metadata: Dict[str, Any]) -> float:
        """Calculate reranking score for a document"""
        score = 0.0

        query_words = set(query.lower().split())
        doc_words = set(document.lower().split())
        overlap = len(query_words.intersection(doc_words))
        score += overlap * 0.1

        if len(document) > 100:
            score += 0.02

        query_lower = query.lower()
        modality = metadata.get("modality", "unknown")

        if any(keyword in query_lower for keyword in ['audio', 'recording', 'listen', 'sound', 'voice', 'transcription']):
            if modality == "audio":
                score += 0.15
        elif modality == "text":
            score += 0.02

        return score

    def _balance_modalities(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Ensure balanced representation across modalities"""
        if not results.get("metadatas") or not results["metadatas"][0]:
            return results

        metadatas = results["metadatas"][0]
        modality_counts = {}

        for metadata in metadatas:
            modality = metadata.get("modality", "unknown")
            modality_counts[modality] = modality_counts.get(modality, 0) + 1

        if len(modality_counts) <= 1 or max(modality_counts.values()) <= len(metadatas) * 0.7:
            return results

        logger.info(f"Modalities found: {modality_counts}")
        return results

async def multimodal_retrieve(query: str, orchestrator) -> List[Dict[str, Any]]:
    """
    Perform multimodal retrieval for a query

    Args:
        query: Query string
        orchestrator: RetrievalOrchestrator instance

    Returns:
        List of retrieved documents
    """
    import asyncio

    try:
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            orchestrator.retrieve,
            query
        )

        return result.get('results', [])

    except Exception as e:
        logger.error(f"Multimodal retrieve failed: {e}")
        return []
FILE: app/services/__init__.py
[EMPTY FILE]
FILE: app/services/ingestion_service.py
[EMPTY FILE]
FILE: app/services/reasoning_service.py
[EMPTY FILE]
FILE: app/services/retrieval_service.py
[EMPTY FILE]
FILE: app/storage/__init__.py
[EMPTY FILE]
FILE: app/storage/chat_store/__init__.py
"""Chat history storage module."""
from .history_manager import ChatHistoryManager

__all__ = ['ChatHistoryManager']
FILE: app/storage/chat_store/history_manager.py
"""
Chat History Manager - Persistent conversation storage per session.

Features:
- JSON-based storage per session ID
- Thread-safe operations
- Automatic cleanup of old conversations
- Support for multi-turn dialogue
"""
import json
import logging
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import asyncio
from threading import Lock

logger = logging.getLogger(__name__)

class ChatHistoryManager:
    """Manages conversation history for multi-turn dialogue."""

    def __init__(self, storage_dir: str = "F:/Pluto/data/chat_history"):
        """
        Initialize chat history manager.

        Args:
            storage_dir: Directory to store chat history JSON files
        """
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self._locks = {}
        self._global_lock = Lock()

        logger.info(f"ChatHistoryManager initialized with storage: {self.storage_dir}")

    def _get_session_lock(self, session_id: str) -> Lock:
        """Get or create a lock for the given session."""
        with self._global_lock:
            if session_id not in self._locks:
                self._locks[session_id] = Lock()
            return self._locks[session_id]

    def _get_history_path(self, session_id: str) -> Path:
        """Get the file path for a session's history."""
        return self.storage_dir / f"{session_id}.json"

    async def save_turn(
        self,
        session_id: str,
        user_query: str,
        system_response: str,
        cited_sources: Optional[List[str]] = None,
        confidence_score: Optional[float] = None,
        is_conflicting: bool = False,
        conflicts: Optional[List[str]] = None
    ) -> None:
        """
        Save a conversation turn to history.

        Args:
            session_id: Session identifier
            user_query: User's question
            system_response: System's answer
            cited_sources: List of source citations
            confidence_score: Confidence score (0-1)
            is_conflicting: Whether conflicting evidence was detected
            conflicts: List of conflict descriptions
        """
        lock = self._get_session_lock(session_id)

        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            None,
            self._save_turn_sync,
            lock,
            session_id,
            user_query,
            system_response,
            cited_sources,
            confidence_score,
            is_conflicting,
            conflicts
        )

    def _save_turn_sync(
        self,
        lock: Lock,
        session_id: str,
        user_query: str,
        system_response: str,
        cited_sources: Optional[List[str]],
        confidence_score: Optional[float],
        is_conflicting: bool,
        conflicts: Optional[List[str]]
    ) -> None:
        """Synchronous implementation of save_turn."""
        with lock:
            history_path = self._get_history_path(session_id)

            if history_path.exists():
                try:
                    with open(history_path, 'r', encoding='utf-8') as f:
                        history = json.load(f)
                except json.JSONDecodeError:
                    logger.warning(f"Corrupted history file for session {session_id}, creating new")
                    history = {"session_id": session_id, "turns": []}
            else:
                history = {"session_id": session_id, "turns": []}

            turn = {
                "turn_id": len(history["turns"]) + 1,
                "timestamp": datetime.utcnow().isoformat(),
                "user_query": user_query,
                "system_response": system_response,
                "cited_sources": cited_sources or [],
                "confidence_score": confidence_score,
                "is_conflicting": is_conflicting,
                "conflicts": conflicts or []
            }

            history["turns"].append(turn)

            if len(history["turns"]) > 50:
                history["turns"] = history["turns"][-50:]
                logger.info(f"Trimmed history for session {session_id} to last 50 turns")

            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(history, f, indent=2, ensure_ascii=False)

            logger.info(f"Saved turn {turn['turn_id']} for session {session_id}")

    async def get_history(
        self,
        session_id: str,
        max_turns: int = 10
    ) -> List[Dict]:
        """
        Get conversation history for a session.

        Args:
            session_id: Session identifier
            max_turns: Maximum number of recent turns to return

        Returns:
            List of conversation turns (most recent last)
        """
        lock = self._get_session_lock(session_id)

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self._get_history_sync,
            lock,
            session_id,
            max_turns
        )

    def _get_history_sync(
        self,
        lock: Lock,
        session_id: str,
        max_turns: int
    ) -> List[Dict]:
        """Synchronous implementation of get_history."""
        with lock:
            history_path = self._get_history_path(session_id)

            if not history_path.exists():
                logger.debug(f"No history found for session {session_id}")
                return []

            try:
                with open(history_path, 'r', encoding='utf-8') as f:
                    history = json.load(f)

                turns = history.get("turns", [])

                if len(turns) > max_turns:
                    turns = turns[-max_turns:]

                logger.info(f"Loaded {len(turns)} turns for session {session_id}")
                return turns

            except json.JSONDecodeError:
                logger.error(f"Failed to parse history for session {session_id}")
                return []
            except Exception as e:
                logger.error(f"Error loading history for session {session_id}: {e}")
                return []

    async def clear_session(self, session_id: str) -> bool:
        """
        Clear all history for a session.

        Args:
            session_id: Session identifier

        Returns:
            True if cleared successfully, False otherwise
        """
        lock = self._get_session_lock(session_id)

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self._clear_session_sync,
            lock,
            session_id
        )

    def _clear_session_sync(self, lock: Lock, session_id: str) -> bool:
        """Synchronous implementation of clear_session."""
        with lock:
            history_path = self._get_history_path(session_id)

            if history_path.exists():
                try:
                    history_path.unlink()
                    logger.info(f"Cleared history for session {session_id}")
                    return True
                except Exception as e:
                    logger.error(f"Failed to clear history for session {session_id}: {e}")
                    return False
            else:
                logger.debug(f"No history to clear for session {session_id}")
                return True

    async def get_session_info(self, session_id: str) -> Dict:
        """
        Get metadata about a session.

        Args:
            session_id: Session identifier

        Returns:
            Dict with session metadata (turn_count, first_query, last_query, etc.)
        """
        history = await self.get_history(session_id, max_turns=1000)

        if not history:
            return {
                "session_id": session_id,
                "exists": False,
                "turn_count": 0
            }

        return {
            "session_id": session_id,
            "exists": True,
            "turn_count": len(history),
            "first_query": history[0]["user_query"] if history else None,
            "last_query": history[-1]["user_query"] if history else None,
            "last_timestamp": history[-1]["timestamp"] if history else None
        }

_history_manager = None

def get_history_manager() -> ChatHistoryManager:
    """Get or create the singleton ChatHistoryManager instance."""
    global _history_manager
    if _history_manager is None:
        _history_manager = ChatHistoryManager()
    return _history_manager
FILE: app/storage/file_store/__init__.py
[EMPTY FILE]
FILE: app/storage/file_store/file_manager.py
[EMPTY FILE]
FILE: app/storage/file_store/local_storage.py
[EMPTY FILE]
FILE: app/storage/metadata_store/__init__.py
[EMPTY FILE]
FILE: app/storage/qdrant/client.py
"""
Qdrant client for vector storage with Named Vectors architecture
"""
import logging
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from qdrant_client.http import models
import requests

from app.config import settings

logger = logging.getLogger(__name__)

class QdrantClientWrapper:
    """Qdrant client with connection pooling"""

    _instance = None
    _client = None

    def __new__(cls):
        """Singleton pattern for connection reuse"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if self._client is None:
            self._client = QdrantClient(
                host=settings.qdrant_host,
                port=settings.qdrant_port,
                timeout=30.0,
                prefer_grpc=False
            )
            logger.info("Qdrant client initialized")

    @property
    def client(self):
        """Get the shared client instance"""
        return self._client

    def health_check(self) -> bool:
        """Check Qdrant connection health"""
        try:
            self._client.get_collections()
            return True
        except Exception as e:
            logger.error(f"Qdrant health check failed: {e}")
            return False

    def create_collection(self):
        """Create collection with Named Vectors for multimodal support"""
        try:
            vectors_config = {
                "text_vector_space": models.VectorParams(
                    size=settings.embedding_dimension,
                    distance=models.Distance.COSINE
                ),
                "image_vector_space": models.VectorParams(
                    size=settings.embedding_dimension,
                    distance=models.Distance.COSINE
                ),
                "audio_vector": models.VectorParams(
                    size=settings.embedding_dimension,
                    distance=models.Distance.COSINE
                )
            }

            self.client.recreate_collection(
                collection_name=settings.collection_name,
                vectors_config=vectors_config
            )
            logger.info(f"Created collection '{settings.collection_name}' with Named Vectors")
        except Exception as e:
            logger.error(f"Failed to create collection: {e}")
            raise

    def get_collection(self):
        """Get collection info"""
        try:
            return self.client.get_collection(collection_name=settings.collection_name)
        except Exception as e:
            logger.error(f"Failed to get collection: {e}")
            raise

    def get_points_count(self) -> int:
        """Get the total number of points in the collection"""
        try:
            collection_info = self.get_collection()
            return collection_info.points_count
        except Exception as e:
            logger.error(f"Failed to get points count: {e}")
            return 0

    def upsert_points(self, points: List[models.PointStruct]):
        """Upsert points with Named Vectors"""
        try:
            self.client.upsert(
                collection_name=settings.collection_name,
                points=points
            )
            logger.info(f"Upserted {len(points)} points to collection")
        except Exception as e:
            logger.error(f"Failed to upsert points: {e}")
            raise

    def search_vectors(self, query_vector: List[float], vector_name: str, limit: int = 10,
                      score_threshold: float = 0.4, filter_conditions: Optional[models.Filter] = None) -> List[models.ScoredPoint]:
        """Search vectors with Named Vectors and filtering"""
        try:
            search_result = self.client.query_points(
                collection_name=settings.collection_name,
                query=query_vector,
                using=vector_name,
                limit=limit,
                score_threshold=score_threshold,
                query_filter=filter_conditions
            )
            return search_result.points
        except Exception as e:
            logger.error(f"Failed to search vectors: {e}")
            raise

    def delete_points(self, point_ids: List[str]):
        """Delete points by IDs"""
        try:
            self.client.delete(
                collection_name=settings.collection_name,
                points_selector=models.PointIdsList(
                    points=point_ids
                )
            )
            logger.info(f"Deleted {len(point_ids)} points")
        except Exception as e:
            logger.error(f"Failed to delete points: {e}")
            raise

    def delete_collection(self):
        """Delete the entire collection"""
        try:
            self.client.delete_collection(collection_name=settings.collection_name)
            logger.info(f"Deleted collection '{settings.collection_name}'")
        except Exception as e:
            logger.error(f"Failed to delete collection: {e}")
            raise

    def scroll_all_points(self, limit: int = 10000) -> List[models.Record]:
        """Scroll through all points in collection"""
        try:
            scroll_result = self.client.scroll(
                collection_name=settings.collection_name,
                limit=limit,
                with_payload=True
            )
            return scroll_result[0]
        except Exception as e:
            logger.error(f"Failed to scroll all points: {e}")
            return []

    def scroll_points(self, filter_conditions: Optional[models.Filter] = None, limit: int = 1000) -> List[models.Record]:
        """Scroll through points with optional filtering"""
        try:
            scroll_result = self.client.scroll(
                collection_name=settings.collection_name,
                scroll_filter=filter_conditions,
                limit=limit,
                with_payload=True
            )
            return scroll_result[0]
        except Exception as e:
            logger.error(f"Failed to scroll points: {e}")
            return []
FILE: app/storage/qdrant/collections.py
"""
Qdrant collection management utilities
"""
import logging
from qdrant_client import QdrantClient
from qdrant_client.http import models

from app.config import settings
from app.storage.qdrant.client import QdrantClientWrapper

logger = logging.getLogger(__name__)

def get_qdrant_client() -> QdrantClientWrapper:
    """Get Qdrant client instance"""
    return QdrantClientWrapper()

def ensure_collection_exists():
    """Ensure the collection exists with proper configuration"""
    client = get_qdrant_client()
    try:
        collections = client.client.get_collections()
        collection_names = [c.name for c in collections.collections]

        if settings.collection_name not in collection_names:
            logger.info(f"Collection '{settings.collection_name}' does not exist, creating...")
            client.create_collection()
        else:
            logger.info(f"Collection '{settings.collection_name}' already exists")
    except Exception as e:
        logger.error(f"Failed to ensure collection exists: {e}")
        raise
FILE: app/storage/qdrant/indexing.py
"""
Qdrant indexing utilities for multimodal embeddings
"""
import logging
from typing import List, Dict, Any, Optional
from qdrant_client.http import models

from app.config import settings
from app.storage.qdrant.client import QdrantClientWrapper

logger = logging.getLogger(__name__)

class QdrantIndexer:
    """Handles indexing operations for Qdrant with Named Vectors"""

    def __init__(self):
        self.client = QdrantClientWrapper()

    def index_multimodal_document(self, document_id: str, text_embedding: List[float],
                                image_embedding: Optional[List[float]] = None,
                                audio_embedding: Optional[List[float]] = None,
                                metadata: Dict[str, Any] = None):
        """Index a document with multiple vector representations"""
        try:
            vectors = {
                "text_vector_space": text_embedding
            }

            if image_embedding:
                vectors["image_vector_space"] = image_embedding

            if audio_embedding:
                vectors["audio_vector"] = audio_embedding

            payload = metadata or {}
            payload.update({
                "document_id": document_id,
                "has_text": True,
                "has_image": image_embedding is not None,
                "has_audio": audio_embedding is not None
            })

            point = models.PointStruct(
                id=document_id,
                vector=vectors,
                payload=payload
            )

            self.client.upsert_points([point])
            logger.info(f"Indexed document {document_id} with {len(vectors)} vectors")

        except Exception as e:
            logger.error(f"Failed to index document {document_id}: {e}")
            raise

    def batch_index_documents(self, documents: List[Dict[str, Any]]):
        """Batch index multiple documents"""
        try:
            points = []

            for doc in documents:
                document_id = doc["id"]
                text_embedding = doc["text_embedding"]
                image_embedding = doc.get("image_embedding")
                audio_embedding = doc.get("audio_embedding")
                metadata = doc.get("payload") or doc.get("metadata") or {}

                logger.info(f"[DEBUG] Doc keys: {list(doc.keys())}")
                logger.info(f"[DEBUG] Payload from doc: {doc.get('payload')}")
                logger.info(f"[DEBUG] Metadata keys: {list(metadata.keys())}")
                logger.info(f"[DEBUG] document_topic: {metadata.get('document_topic')}")
                logger.info(f"[DEBUG] document_concepts: {metadata.get('document_concepts')}")

                vectors = {
                    "text_vector_space": text_embedding
                }

                if image_embedding:
                    vectors["image_vector_space"] = image_embedding

                if audio_embedding:
                    vectors["audio_vector"] = audio_embedding

                payload = metadata.copy()
                payload.update({
                    "document_id": document_id,
                    "has_text": True,
                    "has_image": image_embedding is not None,
                    "has_audio": audio_embedding is not None
                })

                point = models.PointStruct(
                    id=document_id,
                    vector=vectors,
                    payload=payload
                )

                points.append(point)

            if points:
                self.client.upsert_points(points)
                logger.info(f"Batch indexed {len(points)} documents")

        except Exception as e:
            logger.error(f"Failed to batch index documents: {e}")
            raise
FILE: app/storage/qdrant/query_builder.py
"""
Qdrant query building utilities with Payload Filtering
"""
import logging
from typing import List, Dict, Any, Optional
from qdrant_client.http import models

from app.config import settings

logger = logging.getLogger(__name__)

class QdrantQueryBuilder:
    """Builds Qdrant queries with proper filtering"""

    @staticmethod
    def build_topic_filter(document_topic: str) -> models.Filter:
        """Build filter for topic-concept gate matching"""
        return models.Filter(
            must=[
                models.FieldCondition(
                    key="document_topic",
                    match=models.MatchValue(value=document_topic)
                )
            ]
        )

    @staticmethod
    def build_concept_filter(document_concepts: List[str]) -> models.Filter:
        """Build filter for concept matching"""
        return models.Filter(
            should=[
                models.FieldCondition(
                    key="document_concepts",
                    match=models.MatchAny(any=document_concepts)
                )
            ]
        )

    @staticmethod
    def build_modality_filter(modality: str) -> models.Filter:
        """Build filter for modality availability"""
        modality_field = f"has_{modality.lower()}"
        return models.Filter(
            must=[
                models.FieldCondition(
                    key=modality_field,
                    match=models.MatchValue(value=True)
                )
            ]
        )

    @staticmethod
    def build_source_filter(source_file: str) -> models.Filter:
        """Build filter for single source file matching"""
        return models.Filter(
            must=[
                models.FieldCondition(
                    key="source_file",
                    match=models.MatchValue(value=source_file)
                )
            ]
        )

    @staticmethod
    def build_source_filter_multiple(source_files: List[str]) -> models.Filter:
        """Build filter for source file matching"""
        return models.Filter(
            must=[
                models.FieldCondition(
                    key="source_file",
                    match=models.MatchAny(any=source_files)
                )
            ]
        )

    @staticmethod
    def combine_filters(*filters: models.Filter) -> models.Filter:
        """Combine multiple filters with AND logic"""
        combined_conditions = []
        for filter_obj in filters:
            if filter_obj.must:
                combined_conditions.extend(filter_obj.must)
            if filter_obj.should:
                combined_conditions.append(
                    models.FieldCondition(
                        key="combined_should",
                        match=models.MatchAny(any=filter_obj.should)
                    )
                )

        return models.Filter(must=combined_conditions)

    @staticmethod
    def get_vector_name_for_modality(modality: str) -> str:
        """Get the appropriate vector name for a modality"""
        modality_map = {
            "text": "text_vector_space",
            "image": "image_vector_space",
            "audio": "audio_vector"
        }
        return modality_map.get(modality.lower(), "text_vector_space")
FILE: app/storage/vector_store.py
"""
Vector Store - High-level interface for Qdrant vector storage operations with Named Vectors
Guaranteed data consistency and prevention of ghost documents or stale embeddings.
"""
import logging
from typing import List, Dict, Any, Optional, Set, Tuple
import uuid
import os
from pathlib import Path

from app.storage.qdrant.client import QdrantClientWrapper
from app.storage.qdrant.indexing import QdrantIndexer
from app.storage.qdrant.query_builder import QdrantQueryBuilder
from app.embeddings.manager import EmbeddingsManager
from app.config import settings

logger = logging.getLogger(__name__)

class VectorStore:
    """High-level interface for Qdrant vector storage and retrieval with Named Vectors"""

    def __init__(self):
        self.qdrant_client = QdrantClientWrapper()
        self.indexer = QdrantIndexer()
        self.query_builder = QdrantQueryBuilder()
        self.embeddings_manager = EmbeddingsManager()
        logger.info("VectorStore initialized with Qdrant")

    def _normalize_source_file(self, source_file: str) -> str:
        """Normalize source file path to lowercase filename only"""
        if not source_file:
            return "unknown"
        return Path(source_file).name.lower()

    def _validate_chunk_metadata(self, chunk: Dict[str, Any]) -> Dict[str, Any]:
        """Ensure chunk has all required metadata for deterministic identity"""
        if 'chunk_id' not in chunk:
            content_hash = hash(chunk.get('content', chunk.get('text', '')))
            source_hash = hash(self._normalize_source_file(chunk.get('source_file', '')))
            chunk['chunk_id'] = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{content_hash}_{source_hash}"))

        chunk['source_file'] = self._normalize_source_file(chunk.get('source_file', ''))

        if 'document_topic' not in chunk:
            metadata_topic = chunk.get('metadata', {}).get('document_topic', '')
            chunk['document_topic'] = metadata_topic if metadata_topic else 'unknown'

        if 'document_concepts' not in chunk:
            metadata_concepts = chunk.get('metadata', {}).get('document_concepts', [])
            if isinstance(metadata_concepts, str):
                chunk['document_concepts'] = [c.strip() for c in metadata_concepts.split(',') if c.strip()]
            elif isinstance(metadata_concepts, list):
                chunk['document_concepts'] = metadata_concepts
            else:
                chunk['document_concepts'] = []

        return chunk

    def add_documents(self, chunks: List[Dict[str, Any]], session_id: str = "default") -> Dict[str, Any]:
        """
        Add documents with multimodal embeddings to Qdrant vector store

        Args:
            chunks: List of chunk dictionaries
            session_id: Session identifier (default: "default")

        Returns:
            Summary of addition operation
        """
        try:
            if not chunks:
                logger.warning("No chunks provided to add_documents")
                return {"status": "skipped", "count": 0}

            documents_data = []

            for chunk in chunks:
                chunk = self._validate_chunk_metadata(chunk)

                text_content = chunk.get('content', chunk.get('text', ''))
                if not text_content:
                    logger.warning(f"Skipping chunk with no content: {chunk.get('chunk_id')}")
                    continue

                text_embedding = self.embeddings_manager.embed_text(text_content)

                image_embedding = None
                if chunk.get('image_data'):
                    image_embedding = self.embeddings_manager.embed_image(chunk['image_data'])

                audio_embedding = None
                if chunk.get('audio_data'):
                    audio_embedding = self.embeddings_manager.embed_audio(chunk['audio_data'])

                payload = {
                    'chunk_id': chunk['chunk_id'],
                    'source_file': chunk['source_file'],
                    'modality': chunk.get('modality', 'text'),
                    'source_type': chunk.get('source_type', 'text'),
                    'chunk_index': chunk.get('chunk_index', 0),
                    'total_chunks': chunk.get('total_chunks', 1),
                    'document_topic': chunk['document_topic'],
                    'document_concepts': chunk['document_concepts'],
                    'content': text_content
                }

                if chunk.get('prev_chunk_id'):
                    payload['prev_chunk_id'] = str(chunk['prev_chunk_id'])
                if chunk.get('next_chunk_id'):
                    payload['next_chunk_id'] = str(chunk['next_chunk_id'])

                if 'metadata' in chunk:
                    payload.update(chunk['metadata'])

                payload['session_id'] = session_id

                documents_data.append({
                    "id": chunk['chunk_id'],
                    "text_embedding": text_embedding,
                    "image_embedding": image_embedding,
                    "audio_embedding": audio_embedding,
                    "payload": payload
                })

            logger.info(f"[DEBUG] Sample document keys: {list(documents_data[0].keys()) if documents_data else 'None'}")
            logger.info(f"[DEBUG] Sample payload keys: {list(documents_data[0]['payload'].keys()) if documents_data and 'payload' in documents_data[0] else 'None'}")

            self.indexer.batch_index_documents(documents_data)

            validation_result = self._validate_ingestion(documents_data)

            logger.info(f"Added {len(documents_data)} documents to Qdrant vector store")
            logger.info(f"Post-ingestion validation: {validation_result}")

            return {
                "status": "success",
                "count": len(documents_data),
                "collection": settings.collection_name,
                "validation": validation_result
            }

        except Exception as e:
            logger.error(f"Failed to add documents to vector store: {e}")
            raise

    def _validate_ingestion(self, documents_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Post-ingestion validation to ensure data consistency"""
        try:
            current_count = self.qdrant_client.get_points_count()

            unique_topics = set()
            unique_concepts = set()

            for doc in documents_data:
                payload = doc.get('payload', {})
                topic = payload.get('document_topic', '')
                concepts = payload.get('document_concepts', [])

                if topic:
                    unique_topics.add(topic)
                for concept in concepts:
                    unique_concepts.add(concept)

            all_points = self.qdrant_client.scroll_all_points(limit=10000)
            old_topics = set()
            old_concepts = set()

            for point in all_points:
                payload = point.payload or {}
                topic = payload.get('document_topic', '')
                concepts = payload.get('document_concepts', [])

                if topic and topic not in unique_topics:
                    old_topics.add(topic)
                for concept in concepts:
                    if concept not in unique_concepts:
                        old_concepts.add(concept)

            validation_result = {
                "total_points": current_count,
                "new_topics": list(unique_topics),
                "new_concepts": list(unique_concepts),
                "old_topics_found": list(old_topics),
                "old_concepts_found": list(old_concepts),
                "is_consistent": len(old_topics) == 0 and len(old_concepts) == 0
            }

            if not validation_result["is_consistent"]:
                logger.error(f"[VALIDATION] Inconsistency detected! Old topics: {old_topics}, Old concepts: {old_concepts}")

            return validation_result

        except Exception as e:
            logger.error(f"Failed to validate ingestion: {e}")
            return {"error": str(e)}

    def query(self, query_text: str, session_id: str = "default", modality: str = "text", n_results: int = 10,
              query_topic: Optional[str] = None, query_concepts: Optional[List[str]] = None,
              skip_gate: bool = False) -> Dict[str, Any]:
        """
        Query vector store with strict topic-concept compatibility gate.

        Args:
            query_text: Query string
            session_id: Session identifier (default: "default")
            modality: Modality to search in ("text", "image", "audio")
            n_results: Number of results to return
            query_topic: Extracted topic from query
            query_concepts: Extracted concepts from query

        Returns:
            Query results or refusal
        """
        try:
            kb_summary = self.get_knowledge_catalog()

            kb_topics = kb_summary.get('topics', [])
            kb_concepts = kb_summary.get('concepts', [])
            if not kb_topics and not kb_concepts:
                logger.warning("[GATE] Query refused: Knowledge base is empty - no documents uploaded")
                return {
                    "status": "refused",
                    "reason": "no_match: Knowledge base is empty - no documents uploaded yet. Please upload documents before asking questions.",
                    "query_topic": None,
                    "query_concepts": [],
                    "knowledge_base_topics": [],
                    "knowledge_base_concepts": []
                }

            if skip_gate:
                logger.info("[QUERY] Skipping gate check (already validated by gate_node)")
                match_reason = "gate_already_validated"
            else:
                if query_topic is None or query_concepts is None:
                    query_topic, query_concepts = self._extract_query_knowledge(query_text)
                    logger.info(f"[EXTRACT] Extracted topic: '{query_topic}', concepts: {query_concepts}")

                is_compatible, match_reason = self._check_topic_concept_compatibility(
                    query_topic, query_concepts, kb_summary
                )

                logger.info(f"[GATE] Query topic: '{query_topic}'")
                logger.info(f"[GATE] Query concepts: {query_concepts}")
                logger.info(f"[GATE] KB topics: {kb_topics}")
                logger.info(f"[GATE] KB concepts: {kb_concepts}")
                logger.info(f"[GATE] Match decision: {match_reason}")

                if not is_compatible:
                    logger.warning(f"[GATE] Query refused: {match_reason}")
                    return {
                        "status": "refused",
                        "reason": match_reason,
                        "query_topic": query_topic,
                        "query_concepts": query_concepts,
                        "knowledge_base_topics": kb_topics,
                        "knowledge_base_concepts": kb_concepts
                    }

            if modality == "text":
                query_embedding = self.embeddings_manager.embed_text(query_text)
                vector_name = "text_vector_space"
            elif modality == "image":
                query_embedding = self.embeddings_manager.embed_text(query_text)
                vector_name = "text_vector_space"
            else:
                query_embedding = self.embeddings_manager.embed_text(query_text)
                vector_name = "text_vector_space"

            from qdrant_client.http import models

            session_filter = models.Filter(
                must=[
                    models.FieldCondition(
                        key="session_id",
                        match=models.MatchValue(value=session_id)
                    )
                ]
            )

            results = self.qdrant_client.search_vectors(
                query_vector=query_embedding,
                vector_name=vector_name,
                limit=n_results,
                score_threshold=0.2,
                filter_conditions=session_filter
            )

            logger.info(f"[QUERY] Retrieved {len(results)} points")

            formatted_results = {
                "status": "success",
                "ids": [point.id for point in results],
                "distances": [1.0 - point.score for point in results],
                "metadatas": [point.payload for point in results],
                "documents": [point.payload.get("content", "") for point in results],
                "query_topic": query_topic,
                "query_concepts": query_concepts,
                "match_reason": match_reason
            }

            return formatted_results

        except Exception as e:
            logger.error(f"Failed to query vector store: {e}")
            return {
                "status": "refused",
                "reason": f"Query processing failed: {str(e)}",
                "query_topic": query_topic or "unknown",
                "query_concepts": query_concepts or []
            }

    def get_knowledge_catalog(self) -> Dict[str, Any]:
        """Build knowledge catalog by querying Qdrant payloads using scroll(with_payload=True)"""
        try:
            points, _ = self.qdrant_client.client.scroll(
                collection_name=settings.collection_name,
                limit=10000,
                with_payload=True
            )

            topics = set()
            concepts = set()

            for p in points:
                payload = p.payload
                if "document_topic" in payload:
                    topics.add(payload["document_topic"])
                if "document_concepts" in payload:
                    concepts.update(payload["document_concepts"])

            return {"topics": list(topics), "concepts": list(concepts)}

        except Exception as e:
            logger.error(f"Failed to get knowledge catalog: {e}")
            return {"topics": [], "concepts": []}

    def _extract_query_knowledge(self, query_text: str) -> Tuple[str, List[str]]:
        """Extract topic and concepts from query text using Llama with strict JSON output"""
        try:
            import json
            from app.reasoning.llm.llama_reasoner import LlamaReasoner
            reasoner = LlamaReasoner()

            knowledge_prompt = f"""You are a classifier.
Extract the main topic and key concepts from the user query.

Rules:
* Output ONLY valid JSON
* Do not add explanations
* Do not include examples
* Topic must be 1-3 words
* Concepts must be single words from the query

Format exactly:
{{
  "topic": "<topic>",
  "concepts": ["<concept1>", "<concept2>"]
}}

User query: {query_text}"""

            response = reasoner.generate(
                prompt=knowledge_prompt,
                max_tokens=100,
                temperature=0.0,
                stop_sequences=["\n\n", "```", "User query:", "Rules:"]
            )

            try:
                parsed = json.loads(response.strip())
                query_topic = parsed.get('topic', '').strip()
                query_concepts = [c.strip().lower() for c in parsed.get('concepts', []) if c.strip()]

                if not query_topic or not query_concepts:
                    logger.warning(f"[EXTRACTION] Empty topic or concepts from JSON: {parsed}")
                    return "Unknown", []

                return query_topic, query_concepts[:8]

            except json.JSONDecodeError as e:
                logger.error(f"[EXTRACTION] Failed to parse JSON response: {response} - Error: {e}")
                return "Unknown", []

        except Exception as e:
            logger.error(f"Failed to extract query knowledge: {e}")
            return "Unknown", []

    def _validate_topic_against_query(self, topic: str, query_text: str) -> bool:
        """Validate that extracted topic appears in or matches query text"""
        logger.info(f"[VALIDATION] Validating topic '{topic}' against query '{query_text}'")
        if not topic or topic.lower() == "unknown":
            return False

        topic_lower = topic.lower()
        query_lower = query_text.lower()

        topic_words = set(topic_lower.split())
        query_words = set(query_lower.split())

        logger.info(f"[VALIDATION] Topic: '{topic_lower}', Topic words: {topic_words}, Query words: {query_words}")

        overlap = topic_words & query_words
        logger.info(f"[VALIDATION] Overlap: {overlap}")
        if overlap:
            return True

        if topic_lower in ["ai", "artificial intelligence"] and any(word in query_words for word in ["ai", "artificial", "intelligence"]):
            return True

        if topic_lower == "machine learning" and any(word in query_words for word in ["machine", "learning", "ml"]):
            return True

        if topic_lower == "data science" and any(word in query_words for word in ["data", "science"]):
            return True

        logger.warning(f"[VALIDATION] Topic '{topic}' not found in query: '{query_text}'")
        return False

    def _check_topic_concept_compatibility(self, query_topic: str, query_concepts: List[str],
                                          kb_summary: Dict[str, Any]) -> Tuple[bool, str]:
        """Check if query is compatible with knowledge base using topic-concept gate"""
        try:
            if query_topic.lower() == "unknown" or not query_topic or not query_concepts:
                return False, "extraction_failed: Could not extract valid topic and concepts from query"

            kb_topics = set(kb_summary.get('topics', []))
            kb_concepts = set(kb_summary.get('concepts', []))

            if query_topic and query_topic.lower() in [t.lower() for t in kb_topics]:
                return True, f"topic_match: '{query_topic}' found in knowledge base"

            if query_concepts and kb_concepts:
                query_concept_set = set(query_concepts)
                overlap = len(query_concept_set & kb_concepts)
                overlap_ratio = overlap / len(query_concept_set) if query_concept_set else 0.0

                if overlap_ratio >= 0.3:
                    return True, f"concept_match: {overlap}/{len(query_concept_set)} concepts overlap ({overlap_ratio:.1%})"

            return False, f"no_match: query topic '{query_topic}' and concepts {query_concepts} not found in knowledge base topics {list(kb_topics)} or concepts {list(kb_concepts)}"

        except Exception as e:
            logger.error(f"Failed to check topic-concept compatibility: {e}")
            return False, f"compatibility_check_failed: {str(e)}"

    def delete_by_source(self, source_file: str) -> int:
        """
        Delete all documents from a specific source file using payload filters

        Args:
            source_file: Source file path (will be normalized)

        Returns:
            Number of documents deleted
        """
        try:
            normalized_source = self._normalize_source_file(source_file)

            filter_conditions = self.query_builder.build_source_filter(normalized_source)
            matching_points = self.qdrant_client.scroll_points(
                filter_conditions=filter_conditions,
                limit=10000
            )

            if not matching_points:
                logger.info(f"No documents found for source: {normalized_source}")
                return 0

            point_ids = [point.id for point in matching_points]

            self.qdrant_client.delete_points(point_ids)

            logger.info(f"Deleted {len(point_ids)} documents from source: {normalized_source}")
            return len(point_ids)

        except Exception as e:
            logger.error(f"Failed to delete documents by source: {e}")
            raise

    def delete_by_session(self, session_id: str) -> int:
        """
        Delete all documents for a specific session

        Args:
            session_id: Session identifier

        Returns:
            Number of documents deleted
        """
        try:
            from qdrant_client.http import models

            logger.info(f"[DELETE] Removing all documents for session: {session_id}")

            self.qdrant_client.client.delete(
                collection_name=settings.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="session_id",
                                match=models.MatchValue(value=session_id)
                            )
                        ]
                    )
                )
            )

            logger.info(f"[DELETE] Successfully cleared session: {session_id}")
            return 1

        except Exception as e:
            logger.error(f"Failed to delete session documents: {e}")
            return 0

    def get_stats(self) -> Dict[str, Any]:
        """Get vector store statistics"""
        try:
            points_count = self.qdrant_client.get_points_count()

            return {
                "total_documents": points_count,
                "collection_name": settings.collection_name,
                "embedding_dimension": self.embeddings_manager.get_embedding_dimension(),
                "vector_names": ["text_vector_space", "image_vector_space", "audio_vector"]
            }

        except Exception as e:
            logger.error(f"Failed to get stats: {e}")
            return {"error": str(e)}

    def reset(self):
        """Reset the vector store (delete all data) and recreate collection"""
        try:
            self.qdrant_client.delete_collection()
            self.qdrant_client.create_collection()
            logger.info("Qdrant vector store reset and collection recreated successfully")
        except Exception as e:
            logger.error(f"Failed to reset vector store: {e}")
            raise
FILE: app/utils/__init__.py
[EMPTY FILE]
FILE: app/utils/async_utils.py
[EMPTY FILE]
FILE: app/utils/file_utils.py
[EMPTY FILE]
FILE: app/utils/gpu_check.py
"""
GPU Availability Check - Validates hardware acceleration for C++ engines
"""
import logging
import sys

logger = logging.getLogger(__name__)

def validate_gpu_availability():
    """
    Validate that hardware acceleration is available for C++ engines.

    This system uses C++-powered engines that support both GPU and CPU:
    1. fastembed (ONNX-based CLIP embeddings) - GPU preferred, CPU fallback
    2. llama-cpp-python (GGUF models) - GPU preferred, CPU fallback

    GPU acceleration provides significant performance benefits but is not mandatory.

    Logs warnings if GPU is not available but allows CPU operation.
    """
    logger.info("=" * 80)
    logger.info("HARDWARE ACCELERATION CHECK")
    logger.info("=" * 80)

    try:
        from fastembed import TextEmbedding
        test_model = TextEmbedding("Qdrant/clip-ViT-B-32-text")
        logger.info("[OK] fastembed (CLIP) initialized successfully")
        logger.info("[INFO] fastembed uses ONNX runtime - GPU acceleration if available")
    except Exception as e:
        logger.warning(f"[WARNING] fastembed initialization failed: {e}")
        logger.warning("[WARNING] CLIP embeddings will use CPU fallback")

    try:
        import llama_cpp
        logger.info("[OK] llama-cpp-python available")
        logger.info("[INFO] llama-cpp-python will use GPU if CUDA build is available")
    except ImportError:
        logger.error("[ERROR] llama-cpp-python not installed")
        raise RuntimeError("llama-cpp-python is required but not installed")

    logger.info("[INFO] System supports both GPU and CPU operation")
    logger.info("[INFO] GPU acceleration preferred for performance")
    logger.info("[INFO] CPU fallback available for compatibility")

    logger.info("=" * 80)
    logger.info("HARDWARE CHECK COMPLETED")
    logger.info("=" * 80)

def get_gpu_info() -> dict:
    """
    Get information about available hardware acceleration.

    Returns:
        dict: Hardware information for C++ engines
    """
    info = {
        "available": True,
        "device": "cpu",
        "engines": []
    }

    try:
        from fastembed import TextEmbedding
        info["engines"].append("fastembed (CLIP via ONNX)")
        info["fastembed_gpu"] = True
    except:
        info["engines"].append("fastembed (unavailable)")

    try:
        import llama_cpp
        info["engines"].append("llama-cpp-python")
        info["llama_gpu"] = True
    except:
        info["engines"].append("llama-cpp-python (unavailable)")

    if "fastembed (CLIP via ONNX)" in info["engines"]:
        info["device"] = "gpu_preferred"
    if "llama-cpp-python" in info["engines"]:
        info["device"] = "gpu_preferred"

    return info

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    try:
        validate_gpu_availability()
        print("\nOK GPU validation passed!")
        print("\nGPU Info:")
        info = get_gpu_info()
        for key, value in info.items():
            print(f"  {key}: {value}")
    except RuntimeError as e:
        print(f"\nFAIL GPU validation failed: {e}")
        sys.exit(1)
FILE: app/utils/image_utils.py
[EMPTY FILE]
FILE: app/utils/logging_utils.py
"""
Logging utilities for safe text output on Windows
"""
import re

def safe_text(text: str, max_length: int = 80) -> str:
    """
    Sanitize text for safe logging on Windows (avoiding Unicode errors)

    Args:
        text: The text to sanitize
        max_length: Maximum length after sanitization (for truncation)

    Returns:
        ASCII-safe text suitable for logging
    """
    if not text:
        return ""

    try:
        safe = text.encode('ascii', errors='replace').decode('ascii')

        replacements = {
            '\u2502': '|',
            '\u251c': '|-',
            '\u2514': '`-',
            '\u2500': '-',
            '\uf041': '',
            '\u2022': '*',
            '\u2013': '-',
            '\u2014': '--',
            '\u201c': '"',
            '\u201d': '"',
            '\u2018': "'",
            '\u2019': "'",
        }

        for char, replacement in replacements.items():
            safe = safe.replace(char, replacement)

        safe = re.sub(r'[^\x20-\x7E\n\t]', '', safe)

        safe = ' '.join(safe.split())

        if len(safe) > max_length:
            safe = safe[:max_length]

        return safe

    except Exception as e:
        return f"<text encoding error: {str(e)[:30]}>"
FILE: app/utils/retry_utils.py
[EMPTY FILE]
FILE: app/utils/text_utils.py
[EMPTY FILE]
FILE: app/utils/topic_catalog_logger.py
"""
Topic catalog logging for persistence and observability
"""
import json
import logging
from pathlib import Path
from typing import List, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

def log_topic_catalog(topics: List[str], log_dir: Optional[Path] = None) -> bool:
    """
    Log the current topic catalog to a JSON file for persistence.

    Args:
        topics: List of available topics in the knowledge base
        log_dir: Directory to save the catalog (defaults to data/logs/retrieval/)

    Returns:
        True if successful, False otherwise
    """
    try:
        if log_dir is None:
            log_dir = Path("f:/Pluto/data/logs/retrieval")

        log_dir.mkdir(parents=True, exist_ok=True)

        catalog = {
            "timestamp": datetime.now().isoformat(),
            "topic_count": len(topics),
            "topics": sorted(topics)
        }

        catalog_path = log_dir / "current_topic_catalog.json"
        with open(catalog_path, 'w', encoding='utf-8') as f:
            json.dump(catalog, f, indent=2, ensure_ascii=False)

        logger.info(f"Topic catalog logged to {catalog_path}")
        logger.info(f"Current knowledge base covers: {', '.join(topics)}")
        return True

    except Exception as e:
        logger.error(f"Failed to log topic catalog: {e}")
        return False

def load_topic_catalog(log_dir: Optional[Path] = None) -> List[str]:
    """
    Load the persisted topic catalog from JSON file.

    Args:
        log_dir: Directory containing the catalog

    Returns:
        List of topics, empty list if file not found
    """
    try:
        if log_dir is None:
            log_dir = Path("f:/Pluto/data/logs/retrieval")

        catalog_path = log_dir / "current_topic_catalog.json"

        if not catalog_path.exists():
            logger.warning(f"Topic catalog not found at {catalog_path}")
            return []

        with open(catalog_path, 'r', encoding='utf-8') as f:
            catalog = json.load(f)

        topics = catalog.get('topics', [])
        logger.info(f"Loaded {len(topics)} topics from catalog")
        return topics

    except Exception as e:
        logger.error(f"Failed to load topic catalog: {e}")
        return []
FILE: app/utils/topic_utils.py
"""
Topic normalization utilities for Topic Gate Architecture
"""
import logging
from typing import List, Set
import re

logger = logging.getLogger(__name__)

def normalize_topic(topic: str) -> str:
    """
    Normalize a topic string for fuzzy matching.

    Args:
        topic: Raw topic string

    Returns:
        Normalized topic string (lowercase, stripped, single spaces)
    """
    if not topic:
        return ""

    normalized = topic.lower().strip()

    fillers = ["key concepts:", "handout", "introduction to", "chapter", "overview"]
    for word in fillers:
        normalized = normalized.replace(word, "")

    normalized = re.sub(r'\s+', ' ', normalized)

    stop_words = ['the', 'a', 'an', 'of', 'in', 'on', 'at', 'to', 'for']
    words = normalized.split()
    filtered_words = [w for w in words if w not in stop_words]

    return ' '.join(filtered_words) if filtered_words else normalized

def clean_llm_topic_response(raw_response: str) -> str:
    """
    Clean LLM topic extraction response by removing verbosity.

    Handles common LLM patterns like:
    - "The main topic is: Biology"
    - "Topic: Computer Science"
    - "This document is about \"Photosynthesis\"."
    - "Photosynthesis Concepts: -" (hallucination)

    Args:
        raw_response: Raw LLM output

    Returns:
        Clean topic (1-3 words)
    """
    if not raw_response:
        return ""

    cleaned = raw_response.strip()

    logger.debug(f"[TOPIC CLEAN] Input: '{cleaned}'")

    prefixes = [
        r'^the\s+(main\s+)?topic\s+(is|of\s+this\s+document\s+is)\s*:?\s*',
        r'^topic\s*:?\s*',
        r'^this\s+document\s+(is\s+about|discusses|covers)\s*:?\s*',
        r'^main\s+subject\s*:?\s*',
        r'^subject\s*:?\s*',
    ]

    for prefix in prefixes:
        before = cleaned
        cleaned = re.sub(prefix, '', cleaned, flags=re.IGNORECASE)
        if cleaned != before:
            logger.debug(f"[TOPIC CLEAN] After prefix removal: '{cleaned}'")

    cleaned = cleaned.strip('"\'\'\'""`')

    cleaned = cleaned.rstrip('.!?,;:')

    cleaned = re.sub(r'[\[\]\(\){}]', '', cleaned)

    cleaned = re.sub(r'\s+', ' ', cleaned).strip()

    before_concept_fix = cleaned
    cleaned = re.sub(r'\s+[Cc]oncepts?\s*:.*$', '', cleaned)
    if cleaned != before_concept_fix:
        logger.debug(f"[TOPIC CLEAN] After concept removal: '{before_concept_fix}' -> '{cleaned}'")

    words = cleaned.split()
    if len(words) > 3:
        cleaned = ' '.join(words[:3])
        logger.debug(f"[TOPIC CLEAN] Truncated to 3 words: '{cleaned}'")

    cleaned = cleaned.title() if cleaned else ""

    logger.debug(f"[TOPIC CLEAN] Final: '{cleaned}'")

    return cleaned

def topics_match(query_topic: str, document_topics: List[str], threshold: float = 0.6) -> bool:
    """
    Check if query topic matches any document topic using fuzzy matching.

    Args:
        query_topic: Normalized topic from user query
        document_topics: List of normalized topics from documents
        threshold: Similarity threshold (0.0 to 1.0)

    Returns:
        True if a match is found, False otherwise
    """
    if not query_topic or not document_topics:
        return False

    query_norm = normalize_topic(query_topic)
    query_words = set(query_norm.split())

    for doc_topic in document_topics:
        doc_norm = normalize_topic(doc_topic)
        doc_words = set(doc_norm.split())

        if query_norm == doc_norm:
            logger.info(f"[TOPIC MATCH] Exact: '{query_topic}' == '{doc_topic}'")
            return True

        if query_norm in doc_norm or doc_norm in query_norm:
            logger.info(f"[TOPIC MATCH] Substring: '{query_topic}' <-> '{doc_topic}'")
            return True

        if query_words and doc_words:
            intersection = query_words & doc_words
            union = query_words | doc_words
            similarity = len(intersection) / len(union)

            if similarity >= threshold:
                logger.info(f"[TOPIC MATCH] Fuzzy ({similarity:.2f}): '{query_topic}' ~ '{doc_topic}'")
                return True

    logger.warning(f"[TOPIC MISMATCH] Query '{query_topic}' not in {document_topics}")
    return False

def extract_unique_topics(metadatas: List[dict]) -> Set[str]:
    """
    Extract unique topic values from metadata list.

    Args:
        metadatas: List of metadata dictionaries

    Returns:
        Set of unique topics
    """
    topics = set()
    for metadata in metadatas:
        if metadata and 'document_topic' in metadata:
            topic = metadata['document_topic']
            if topic and isinstance(topic, str):
                topics.add(normalize_topic(topic))
    return topics

def extract_concepts_from_text(text: str, max_concepts: int = 5) -> List[str]:
    """
    Extract key concepts (nouns/entities) from text using simple heuristics.

    Args:
        text: Input text
        max_concepts: Maximum number of concepts to return

    Returns:
        List of concept strings
    """
    if not text:
        return []

    question_words = ['what', 'how', 'why', 'when', 'where', 'who', 'which', 'is', 'are', 'does', 'do', 'can', 'will', 'would', 'should']

    words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())

    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between', 'under', 'over'}

    concepts = []
    for word in words:
        if word not in stop_words and word not in question_words:
            concepts.append(word)

    unique_concepts = []
    seen = set()
    for concept in concepts:
        if concept not in seen:
            unique_concepts.append(concept)
            seen.add(concept)
            if len(unique_concepts) >= max_concepts:
                break

    return unique_concepts

def normalize_concept(concept: str) -> str:
    """
    Normalize a concept for matching (handles CO2 vs carbon dioxide, etc.)

    Args:
        concept: Raw concept string

    Returns:
        Normalized concept
    """
    if not concept:
        return ""

    normalized = concept.lower().strip()

    abbreviations = {
        'co2': 'carbon dioxide',
        'o2': 'oxygen',
        'h2o': 'water',
        'ai': 'artificial intelligence',
        'ml': 'machine learning',
        'rag': 'retrieval augmented generation',
        'llm': 'large language model',
        'gpu': 'graphics processing unit',
    }

    if normalized in abbreviations:
        return abbreviations[normalized]

    return normalized

def concepts_match(query_concepts: List[str], knowledge_concepts: List[str], threshold: float = 0.3) -> bool:
    """
    Check if any query concepts match knowledge base concepts.

    Args:
        query_concepts: Concepts from user query
        knowledge_concepts: Concepts from knowledge base
        threshold: Match threshold (fraction of query concepts that must match)

    Returns:
        True if sufficient concept overlap exists
    """
    if not query_concepts or not knowledge_concepts:
        return False

    query_norm = [normalize_concept(c) for c in query_concepts]
    kb_norm = [normalize_concept(c) for c in knowledge_concepts]

    matches = 0
    for q_concept in query_norm:
        for kb_concept in kb_norm:
            if q_concept == kb_concept:
                logger.info(f"[CONCEPT MATCH] Exact: '{q_concept}' == '{kb_concept}'")
                matches += 1
                break
            elif q_concept in kb_concept or kb_concept in q_concept:
                logger.info(f"[CONCEPT MATCH] Substring: '{q_concept}' <-> '{kb_concept}'")
                matches += 1
                break

    match_ratio = matches / len(query_norm)
    if match_ratio >= threshold:
        logger.info(f"[CONCEPT MATCH] {matches}/{len(query_norm)} concepts matched ({match_ratio:.1%} >= {threshold:.1%})")
        return True

    logger.warning(f"[CONCEPT MISMATCH] Only {matches}/{len(query_norm)} concepts matched ({match_ratio:.1%} < {threshold:.1%})")
    return False
FILE: app/utils/validation_utils.py
[EMPTY FILE]
FILE: scripts/__init__.py
[EMPTY FILE]
FILE: scripts/acid_test.py
"""
Acid Test: Multimodal RAG Alignment (Fully Automated)
1. Generates a sample image and text file if not present
2. Uploads both to the /ingest endpoint
3. Queries Qdrant for semantic similarity
4. Prints a clear success rate
"""
import requests
import time
import os
from PIL import Image, ImageDraw, ImageFont

image_found = False
text_found = False

API_URL = "http://localhost:8000/api/v1/ingest/file"
QUERY_URL = "http://localhost:8000/api/v1/query/"
ASSET_DIR = os.path.abspath("test_assets")
os.makedirs(ASSET_DIR, exist_ok=True)
IMAGE_PATH = os.path.join(ASSET_DIR, "red_circuit_board.jpg")
TEXT_PATH = os.path.join(ASSET_DIR, "ruby_substrate.txt")

if not os.path.exists(IMAGE_PATH):
    img = Image.new("RGB", (256, 256), color=(180, 0, 0))
    draw = ImageDraw.Draw(img)
    for i in range(20, 236, 40):
        draw.line((i, 20, i, 236), fill=(255, 200, 0), width=4)
        draw.line((20, i, 236, i), fill=(255, 200, 0), width=4)
    draw.text((30, 120), "CIRCUIT", fill=(255,255,255))
    img.save(IMAGE_PATH, "JPEG")
    print(f"Generated sample image at {IMAGE_PATH}")

if not os.path.exists(TEXT_PATH):
    with open(TEXT_PATH, "w", encoding="utf-8") as f:
        f.write("The hardware utilizes a ruby-colored substrate for the wiring.")
    print(f"Generated sample text at {TEXT_PATH}")

with open(IMAGE_PATH, "rb") as img_file:
    files = {"file": (os.path.basename(IMAGE_PATH), img_file, "image/jpeg")}
    resp_img = requests.post(API_URL, files=files)
    print("Image upload status:", resp_img.status_code, resp_img.text)

with open(TEXT_PATH, "rb") as txt_file:
    files = {"file": (os.path.basename(TEXT_PATH), txt_file, "text/plain")}
    resp_txt = requests.post(API_URL, files=files)
    print("Text upload status:", resp_txt.status_code, resp_txt.text)

print("Waiting for ingestion...")
time.sleep(5)

success_count = int(image_found) + int(text_found)
success_count = int(image_found) + int(text_found)
query = {"query": "crimson electronics", "top_k": 3, "collection": "pluto_main"}
resp_query = requests.post(QUERY_URL, json=query)
print("Query status:", resp_query.status_code)
results = resp_query.json()

import json as _json
print("\n--- FULL QDRANT RAW RESULTS ---")
print(_json.dumps(results, indent=2))
print("--- END RAW RESULTS ---\n")

SIMILARITY_THRESHOLD = 0.4
hits = results.get("results", [])
print("Top 3 results:")
image_found = False
text_found = False
for hit in hits:
    print(f"ID: {hit.get('id')}, Score: {hit.get('score')}, Modality: {hit.get('metadata', {}).get('modality')}")
    if 'metadata' in hit and 'file_name' in hit['metadata']:
        print(f"DEBUG: Found {hit['metadata']['file_name']} with score {hit['score']}")
    if hit.get('metadata', {}).get('modality', '').startswith('image') and hit.get('score', 0) > SIMILARITY_THRESHOLD:
        image_found = True
    if hit.get('metadata', {}).get('modality', '') == 'text' and hit.get('score', 0) > SIMILARITY_THRESHOLD:
        text_found = True

success_count = int(image_found) + int(text_found)
print(f"\nSuccess Rate: {success_count}/2 modalities matched with high similarity (>{SIMILARITY_THRESHOLD})")
if image_found and text_found:
    print("ACID TEST PASSED: Both image and text file are highly similar to the query.")
else:
    print("ACID TEST FAILED: One or both modalities missing or below similarity threshold.")
FILE: scripts/benchmark.py
[EMPTY FILE]
FILE: scripts/benchmark_performance.py
"""
Performance benchmark script
"""
import asyncio
import aiohttp
import time
from statistics import mean, stdev

BASE_URL = "http://localhost:8000/api/v1"

async def test_query_latency(session, query, session_id, iterations=10):
    """Measure query latency"""
    latencies = []

    for i in range(iterations):
        start = time.time()

        async with session.post(
            f"{BASE_URL}/query/",
            headers={"X-Session-ID": session_id},
            json={"query": query}
        ) as response:
            await response.json()

        latencies.append(time.time() - start)

    return {
        "mean": mean(latencies),
        "stdev": stdev(latencies) if len(latencies) > 1 else 0,
        "min": min(latencies),
        "max": max(latencies)
    }

async def test_concurrent_users(num_users=10):
    """Simulate concurrent users"""
    async with aiohttp.ClientSession() as session:
        tasks = [
            test_query_latency(session, f"Query from user {i}", f"session-{i}")
            for i in range(num_users)
        ]

        results = await asyncio.gather(*tasks)

        print(f"\n{'='*60}")
        print(f"CONCURRENT USERS TEST ({num_users} users)")
        print(f"{'='*60}")

        all_means = [r['mean'] for r in results]
        print(f"Average latency: {mean(all_means):.2f}s")
        print(f"Worst latency: {max(r['max'] for r in results):.2f}s")
        print(f"Best latency: {min(r['min'] for r in results):.2f}s")

if __name__ == "__main__":
    asyncio.run(test_concurrent_users(num_users=10))
FILE: scripts/cleanup_vectorstore.py
"""
Qdrant Cleanup Utility - Remove Orphaned Documents

This script helps clean up the Qdrant vector store by:
1. Identifying documents whose source files no longer exist
2. Removing duplicate documents
3. Providing detailed cleanup report

Usage:
    python scripts/cleanup_vectorstore.py [--dry-run]
"""
import sys
import os
import logging
from pathlib import Path
from collections import defaultdict

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.storage.vector_store import VectorStore
from app.config import settings

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def analyze_vector_store():
    """Analyze vector store for issues"""
    logger.info("=" * 70)
    logger.info("QDRANT ANALYSIS UTILITY")
    logger.info("=" * 70)

    vector_store = VectorStore()

    try:
        collection_info = vector_store.qdrant_client.get_collection()
        total_docs = collection_info.points_count
        logger.info(f"\n[STATS] Total points in collection: {total_docs}")
        logger.info(f"[STATS] Collection: {settings.collection_name}")

        logger.warning("âš ï¸  Full analysis not yet implemented for Qdrant")
        logger.info("âœ“ Basic stats retrieved")

    except Exception as e:
        logger.error(f"Failed to analyze vector store: {e}")
        return

    return {
        'total_docs': total_docs,
        'orphaned_docs': [],
        'missing_source_docs': [],
        'duplicates': {}
    }

def cleanup_orphaned_documents(dry_run=True):
    """Clean up orphaned documents"""
    logger.info("\n" + "=" * 70)
    if dry_run:
        logger.info("DRY RUN MODE - No changes will be made")
    else:
        logger.info("CLEANUP MODE - Removing orphaned documents")
    logger.info("=" * 70)

    logger.warning("âš ï¸  Orphaned document cleanup not yet implemented for Qdrant")
    logger.info("âœ“ No cleanup performed")

def main():
    """Main function"""
    import argparse

    parser = argparse.ArgumentParser(description='Clean up Qdrant vector store')
    parser.add_argument('--dry-run', action='store_true',
                       help='Show what would be deleted without actually deleting')
    parser.add_argument('--clean', action='store_true',
                       help='Actually perform cleanup (removes orphaned documents)')

    args = parser.parse_args()

    analysis = analyze_vector_store()

    if args.clean or args.dry_run:
        cleanup_orphaned_documents(dry_run=args.dry_run)
    else:
        logger.info("\nðŸ’¡ Tip: Use --dry-run to see what would be deleted")
        logger.info("ðŸ’¡ Use --clean to actually remove orphaned documents")

    logger.info("\n" + "=" * 70)
    logger.info("Analysis complete!")
    logger.info("=" * 70)

if __name__ == "__main__":
    main()
FILE: scripts/convert_clip_to_gguf.py
"""
Script to convert HuggingFace CLIP model to GGUF format for clip-cpp

This script:
1. Clones the clip.cpp repository
2. Converts the existing HuggingFace CLIP model to GGUF format
3. Places the GGUF file in the correct location for the backend

Usage:
    python scripts/convert_clip_to_gguf.py
"""

import os
import sys
import subprocess
from pathlib import Path
import shutil

SCRIPT_DIR = Path(__file__).parent
BACKEND_DIR = SCRIPT_DIR.parent
DATA_DIR = BACKEND_DIR.parent / "data"
MODELS_DIR = DATA_DIR / "models"
CLIP_HF_DIR = MODELS_DIR / "clip-vit-base-patch32" / "models--openai--clip-vit-base-patch32" / "snapshots" / "3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268"
CLIP_GGUF_DIR = MODELS_DIR / "clip"
CLIP_CPP_DIR = BACKEND_DIR / "clip.cpp"

def download_preconverted_gguf():
    """Download pre-converted CLIP GGUF model from HuggingFace"""
    print("\nðŸ”„ Downloading pre-converted CLIP GGUF model from HuggingFace...")

    try:
        from huggingface_hub import hf_hub_download

        print("   Repository: mys/ggml_CLIP-ViT-B-32")
        print("   File: ggml-model-f16.gguf")

        gguf_path = hf_hub_download(
            repo_id="mys/ggml_CLIP-ViT-B-32",
            filename="ggml-model-f16.gguf",
            cache_dir=str(MODELS_DIR / "cache")
        )

        final_path = CLIP_GGUF_DIR / "clip-vit-base-patch32.gguf"
        shutil.copy(gguf_path, final_path)

        file_size = final_path.stat().st_size / (1024 * 1024)
        print(f"\nâœ… Downloaded pre-converted CLIP GGUF model")
        print(f"   Location: {final_path}")
        print(f"   Size: {file_size:.2f} MB")

        return True
    except Exception as e:
        print(f"\nâš ï¸  Download failed: {e}")
        print(f"\nðŸ“¥ Manual Download Instructions:")
        print(f"   1. Go to: https://huggingface.co/mys/ggml_CLIP-ViT-B-32")
        print(f"   2. Download: ggml-model-f16.gguf")
        print(f"   3. Rename to: clip-vit-base-patch32.gguf")
        print(f"   4. Place in: {CLIP_GGUF_DIR}")
        return False

def main():
    print("=" * 80)
    print("CLIP Model Conversion to GGUF")
    print("=" * 80)

    if download_preconverted_gguf():
        print("\n" + "=" * 80)
        print("âœ… CLIP GGUF Model Ready!")
        print("=" * 80)
        return

    if not CLIP_HF_DIR.exists():
        print(f"\nâŒ ERROR: HuggingFace CLIP model not found at {CLIP_HF_DIR}")
        print("Please download the model first using:")
        print("  python scripts/setup_models.py")
        sys.exit(1)

    print(f"\nâœ… Found HuggingFace CLIP model at {CLIP_HF_DIR}")

    if not CLIP_CPP_DIR.exists():
        print(f"\nðŸ“¥ Cloning clip.cpp repository...")
        try:
            subprocess.run(
                ["git", "clone", "https://github.com/monatis/clip.cpp.git", str(CLIP_CPP_DIR)],
                check=True,
                cwd=str(BACKEND_DIR)
            )
            print("âœ… clip.cpp repository cloned successfully")
        except subprocess.CalledProcessError as e:
            print(f"âŒ ERROR: Failed to clone clip.cpp repository: {e}")
            sys.exit(1)
    else:
        print(f"\nâœ… clip.cpp repository already exists at {CLIP_CPP_DIR}")

    CLIP_GGUF_DIR.mkdir(parents=True, exist_ok=True)
    print(f"\nâœ… Output directory created: {CLIP_GGUF_DIR}")

    print(f"\nðŸ”„ Converting CLIP model to GGUF format...")
    print(f"   Source: {CLIP_HF_DIR}")
    print(f"   Target: {CLIP_GGUF_DIR}")

    conversion_script = CLIP_CPP_DIR / "models" / "convert_hf_to_gguf.py"

    if not conversion_script.exists():
        print(f"âŒ ERROR: Conversion script not found at {conversion_script}")
        sys.exit(1)

    try:
        result = subprocess.run(
            [
                sys.executable,
                str(conversion_script),
                "-m", str(CLIP_HF_DIR),
                "-o", str(CLIP_GGUF_DIR)
            ],
            check=True,
            cwd=str(CLIP_CPP_DIR),
            capture_output=True,
            text=True
        )

        print("\nâœ… Conversion successful!")
        print(result.stdout)

        gguf_file = CLIP_GGUF_DIR / "clip-vit-base-patch32.gguf"
        if gguf_file.exists():
            file_size = gguf_file.stat().st_size / (1024 * 1024)
            print(f"\nâœ… GGUF file created: {gguf_file}")
            print(f"   Size: {file_size:.2f} MB")
        else:
            print(f"\nâš ï¸  Warning: Expected GGUF file not found at {gguf_file}")
            print(f"   Checking for alternative file names...")
            gguf_files = list(CLIP_GGUF_DIR.glob("*.gguf"))
            if gguf_files:
                print(f"   Found: {[f.name for f in gguf_files]}")
                shutil.move(str(gguf_files[0]), str(gguf_file))
                print(f"   Renamed to: {gguf_file.name}")
            else:
                print(f"   âŒ No GGUF files found in {CLIP_GGUF_DIR}")
                sys.exit(1)

    except subprocess.CalledProcessError as e:
        print(f"\nâŒ ERROR: Conversion failed")
        print(f"   Return code: {e.returncode}")
        print(f"   STDOUT: {e.stdout}")
        print(f"   STDERR: {e.stderr}")
        sys.exit(1)

    print(f"\nðŸ§¹ Cleanup options:")
    print(f"   - Keep clip.cpp directory: {CLIP_CPP_DIR}")
    print(f"   - Keep HuggingFace model: {CLIP_HF_DIR}")
    print(f"\nðŸ’¡ You can delete the clip.cpp directory if you don't need it anymore:")
    print(f"   shutil.rmtree('{CLIP_CPP_DIR}')")

    print("\n" + "=" * 80)
    print("âœ… CLIP GGUF Conversion Complete!")
    print("=" * 80)
    print(f"\nðŸ“ GGUF Model Location: {CLIP_GGUF_DIR / 'clip-vit-base-patch32.gguf'}")
    print(f"\nðŸš€ Next steps:")
    print(f"   1. Install clip-cpp: pip install clip-cpp")
    print(f"   2. Set CMAKE_ARGS for CUDA: set CMAKE_ARGS=\"-DGGML_CUDA=on\"")
    print(f"   3. Reinstall with CUDA: pip install clip-cpp --force-reinstall --no-cache-dir")
    print(f"   4. Start backend: python -m uvicorn app.main:app --host 0.0.0.0 --port 8000")

if __name__ == "__main__":
    main()
FILE: scripts/download_llama.py
#!/usr/bin/env python3
"""
Download Llama-3.2-1B-Instruct-Q4_K_M.gguf model
"""

import os
from pathlib import Path
from huggingface_hub import hf_hub_download

def main():
    model_dir = Path(__file__).parent.parent.parent / "data" / "models" / "llama"
    model_dir.mkdir(parents=True, exist_ok=True)

    print("Downloading Llama-3.2-1B-Instruct-Q4_K_M.gguf...")
    model_path = hf_hub_download(
        repo_id="bartowski/Llama-3.2-1B-Instruct-GGUF",
        filename="Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        cache_dir=model_dir
    )
    print(f"Downloaded to: {model_path}")

if __name__ == "__main__":
    main()
FILE: scripts/initialize_vectorstore.py
[EMPTY FILE]
FILE: scripts/reset_vectorstore.py
"""
Reset Qdrant Vector Store - Clean Slate Utility

Use this script to clear all previous documents from the vector store.
This prevents semantic drift where the model finds "best matches" in irrelevant old data.

Usage:
    python scripts/reset_vectorstore.py
"""
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from app.storage.vector_store import reset_vector_store
from app.config import settings
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    """Reset the vector store to start fresh"""
    logger.info("=" * 60)
    logger.info("QDRANT RESET UTILITY")
    logger.info("=" * 60)

    logger.info(f"Target Qdrant instance: {settings.qdrant_host}:{settings.qdrant_port}")
    logger.info(f"Collection: {settings.collection_name}")

    response = input("\nâš ï¸  This will DELETE all existing documents. Continue? (yes/no): ")

    if response.lower() != 'yes':
        logger.info("Reset cancelled by user.")
        return

    success = reset_vector_store()

    if success:
        logger.info("\nâœ… Vector store reset successfully!")
        logger.info("You can now ingest new documents with a clean slate.")
    else:
        logger.error("\nâŒ Reset failed. Check the error messages above.")
        sys.exit(1)

if __name__ == "__main__":
    main()
FILE: scripts/setup_models.py
"""
Model Setup Script for PLUTO
Downloads models from Hugging Face into data/models/
- LLaMA 3.1 8B (Q4_K_M): unsloth/Meta-Llama-3.1-8B-Instruct-GGUF
- CLIP: openai/clip-vit-base-patch32
- Whisper: openai/whisper-base
"""
import os
from pathlib import Path
from huggingface_hub import hf_hub_download

import sys
PROJECT_ROOT = Path(__file__).resolve().parents[2]
MODEL_DIR = PROJECT_ROOT / "data" / "models"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

def download_llama():
    llama_filename = "Llama-3.2-1B-Instruct-Q4_K_M.gguf"
    llama_file = MODEL_DIR / llama_filename
    if llama_file.exists():
        print(f"Llama model already exists at {llama_file}, skipping download.")
        return
    print("Downloading Llama 3.2 1B GGUF model (Q4_K_M)...")
    llama_path = hf_hub_download(
        repo_id="bartowski/Llama-3.2-1B-Instruct-GGUF",
        filename=llama_filename,
        cache_dir=MODEL_DIR
    )
    print(f"Downloaded Llama model to {llama_path}")

def download_clip():
    print("Downloading CLIP (FastEmbed) ONNX models from Qdrant...")
    clip_text_file = MODEL_DIR / "clip-ViT-B-32-text.onnx"
    if not clip_text_file.exists():
        clip_text = hf_hub_download(
            repo_id="Qdrant/clip-ViT-B-32-text",
            filename="model.onnx",
            cache_dir=MODEL_DIR
        )
        os.rename(clip_text, clip_text_file)
        print(f"Downloaded CLIP text encoder to {clip_text_file}")
    else:
        print(f"CLIP text encoder already exists at {clip_text_file}, skipping download.")
    clip_vision_file = MODEL_DIR / "clip-ViT-B-32-vision.onnx"
    if not clip_vision_file.exists():
        clip_vision = hf_hub_download(
            repo_id="Qdrant/clip-ViT-B-32-vision",
            filename="model.onnx",
            cache_dir=MODEL_DIR
        )
        os.rename(clip_vision, clip_vision_file)
        print(f"Downloaded CLIP vision encoder to {clip_vision_file}")
    else:
        print(f"CLIP vision encoder already exists at {clip_vision_file}, skipping download.")

def download_whisper():
    print("Downloading Faster-Whisper (CTranslate2) model files...")
    for fname in ["model.bin", "vocabulary.txt", "tokenizer.json", "config.json"]:
        whisper_file = MODEL_DIR / fname
        if whisper_file.exists():
            print(f"Whisper file already exists at {whisper_file}, skipping download.")
            continue
        file_path = hf_hub_download(
            repo_id="Systran/faster-whisper-base",
            filename=fname,
            cache_dir=MODEL_DIR
        )
        print(f"Downloaded Whisper file: {file_path}")

def main():
    download_llama()
    download_clip()
    download_whisper()
    print("All models downloaded.")

if __name__ == "__main__":
    main()
FILE: scripts/struct_extract.py
import os
from pathlib import Path

TARGET_PATH = Path(r"F:\Pluto")

OUTPUT_FILE = Path.cwd() / "folder_structure.txt"

def generate_tree(path: Path, prefix: str = "") -> list[str]:
    """
    Recursively generate folder structure as a tree.
    """
    lines = []
    entries = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))

    for index, entry in enumerate(entries):
        connector = "â””â”€â”€ " if index == len(entries) - 1 else "â”œâ”€â”€ "
        lines.append(f"{prefix}{connector}{entry.name}")

        if entry.is_dir():
            extension = "    " if index == len(entries) - 1 else "â”‚   "
            lines.extend(generate_tree(entry, prefix + extension))

    return lines

def main():
    if not TARGET_PATH.exists():
        print(f"âŒ Path does not exist: {TARGET_PATH}")
        return

    print(f"ðŸ“‚ Scanning: {TARGET_PATH}")

    tree_lines = [f"{TARGET_PATH}"]
    tree_lines.extend(generate_tree(TARGET_PATH))

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(tree_lines))

    print(f"âœ… Folder structure saved to: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
FILE: scripts/test_chat_history.py
"""
Test Chat History Feature

This script tests:
1. Multi-turn conversations with context preservation
2. Follow-up questions that reference previous answers
3. Session management (info, clear)
"""

print("""
ðŸ§ª CHAT HISTORY TEST SCRIPT
===========================

Prerequisites:
- Backend server running on http://localhost:8000
- Documents already ingested in session-alice

TEST 1: Multi-Turn Conversation
--------------------------------

curl -X POST http://localhost:8000/api/v1/query/ \
  -H "Content-Type: application/json" \
  -H "X-Session-ID: session-alice" \
  -d '{"query": "What is the nervous system?"}'

curl -X POST http://localhost:8000/api/v1/query/ \
  -H "Content-Type: application/json" \
  -H "X-Session-ID: session-alice" \
  -d '{"query": "What are its main components?"}'

curl -X POST http://localhost:8000/api/v1/query/ \
  -H "Content-Type: application/json" \
  -H "X-Session-ID: session-alice" \
  -d '{"query": "Tell me more about that"}'

TEST 2: Check Session Info
---------------------------

curl -X GET http://localhost:8000/api/v1/session/info \
  -H "X-Session-ID: session-alice"

Expected Output:
{
  "session_id": "session-alice",
  "document_count": X,
  "chat_history": {
    "turn_count": 3,
    "first_query": "What is the nervous system?",
    "last_query": "Tell me more about that",
    "last_timestamp": "2026-01-27T..."
  }
}

TEST 3: Clear Session
----------------------

curl -X DELETE http://localhost:8000/api/v1/session/clear \
  -H "X-Session-ID: session-alice"

Expected: Both documents and chat history cleared

VALIDATION CHECKLIST:
--------------------
âœ… Turn 2 should understand "its" refers to "nervous system" from Turn 1
âœ… Turn 3 should reference content from Turn 2
âœ… Session info shows correct turn count
âœ… History includes first_query, last_query, timestamps
âœ… Clear operation removes both documents and chat history

MANUAL TEST (PowerShell):
--------------------------

curl.exe -X POST -H "Content-Type: application/json" `
  -H "X-Session-ID: session-alice" `
  -d '{\"query\": \"What is the nervous system?\"}' `
  http://localhost:8000/api/v1/query/

curl.exe -X POST -H "Content-Type: application/json" `
  -H "X-Session-ID: session-alice" `
  -d '{\"query\": \"What are its main components?\"}' `
  http://localhost:8000/api/v1/query/

curl.exe -X GET -H "X-Session-ID: session-alice" `
  http://localhost:8000/api/v1/session/info

cat F:\\Pluto\\data\\chat_history\\session-alice.json

""")
FILE: scripts/test_ingestion.py
import os
from pathlib import Path
from app.ingestion.orchestrator import ingest_file
from app.storage.qdrant.collections import get_qdrant_client
from app.ingestion.validators.file_validator import FileValidator

TEST_FILES = [
    os.path.join(os.path.dirname(__file__), '../test_assets/ruby_substrate.txt'),
    os.path.join(os.path.dirname(__file__), '../test_assets/red_circuit_board.jpg'),
]

def test_ingestion():
    print("Testing ingestion pipeline...")
    validator = FileValidator()
    for file_path in TEST_FILES:
        print(f"\nValidating: {file_path}")
        assert validator.validate(file_path), f"Validation failed: {validator.errors}"
        print("Validation passed.")
        doc_ids = ingest_file(file_path)
        print(f"Ingested doc IDs: {doc_ids}")
        assert doc_ids, "No document IDs returned!"
        client = get_qdrant_client()
        collection_info = client.get_collection()
        print(f"Collection points count: {collection_info.points_count}")
        assert collection_info.points_count > 0, "No points found in collection"
        print("Points successfully stored in Qdrant")
    print("\nAll tests passed!")

if __name__ == "__main__":
    test_ingestion()
FILE: scripts/tests/__init__.py
[EMPTY FILE]
FILE: scripts/tests/integration/__init__.py
[EMPTY FILE]
FILE: scripts/tests/integration/test_api.py
[EMPTY FILE]
FILE: scripts/tests/integration/test_contradictory_sources.py
[EMPTY FILE]
FILE: scripts/tests/integration/test_end_to_end.py
[EMPTY FILE]
FILE: scripts/tests/integration/test_ingestion_pipeline.py
[EMPTY FILE]
FILE: scripts/tests/integration/test_missing_data.py
[EMPTY FILE]
FILE: scripts/tests/integration/test_partial_failures.py
[EMPTY FILE]
FILE: scripts/tests/integration/test_retrieval_pipeline.py
[EMPTY FILE]
FILE: scripts/tests/integration/test_uncertainty_handling.py
[EMPTY FILE]
FILE: scripts/tests/test_backend_modelLoading.py
[EMPTY FILE]
FILE: scripts/tests/unit/__init__.py
[EMPTY FILE]
FILE: scripts/tests/unit/test_chunking.py
import pytest
from app.ingestion.chunking.text_chunker import micro_chunk_text

def test_micro_chunking_and_token_safety():
    text = "A" * 1000
    chunks = micro_chunk_text(text)
    assert all(len(chunk['chunk']) <= 235 for chunk in chunks), "Chunk exceeds 235 characters!"
    for i, chunk in enumerate(chunks):
        assert 'prev_chunk_id' in chunk and 'next_chunk_id' in chunk
        if i == 0:
            assert chunk['prev_chunk_id'] is None
        if i == len(chunks) - 1:
            assert chunk['next_chunk_id'] is None
        if 0 < i < len(chunks) - 1:
            assert chunk['prev_chunk_id'] is not None and chunk['next_chunk_id'] is not None
    print(f"Total chunks: {len(chunks)}. All chunk sizes and context IDs are valid.")
FILE: scripts/tests/unit/test_embeddings.py
[EMPTY FILE]
FILE: scripts/tests/unit/test_graph.py
[EMPTY FILE]
FILE: scripts/tests/unit/test_processors.py
[EMPTY FILE]
FILE: scripts/tests/unit/test_reasoning.py
[EMPTY FILE]
FILE: scripts/tests/unit/test_retrieval.py
[EMPTY FILE]
FILE: scripts/verify_no_autowipe.py
"""
Verify that auto-wipe is disabled in main.py
"""
import sys
from pathlib import Path

main_py_path = Path(__file__).parent.parent / "app" / "main.py"

with open(main_py_path, 'r', encoding='utf-8') as f:
    content = f.read()

if 'force_wipe_memory(QDRANT_PATH)' in content and not '
    print("[ERROR] Auto-wipe is STILL ACTIVE!")
    print("The line 'force_wipe_memory(QDRANT_PATH)' is not commented out.")
    sys.exit(1)
elif '
    print("[OK] Auto-wipe is properly DISABLED (commented out)")
    print("\nNext steps:")
    print("1. STOP the backend server completely (Ctrl+C)")
    print("2. Verify no Python process is running")
    print("3. Restart: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000")
    print("4. The database should persist between restarts now!")
    sys.exit(0)
else:
    print("[WARN] force_wipe_memory not found in main.py")
    print("This might be OK if the function was removed entirely")
    sys.exit(0)
SUMMARY
Total files: 172
Original size: 367,407 chars
Processed size: 319,974 chars
Reduction: 12.9%

File types:
 .py: 172
